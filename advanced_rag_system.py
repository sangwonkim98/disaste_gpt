"""
ê³ ê¸‰ RAG ì‹œìŠ¤í…œ (Query Rewriting + Reranking)
 - vLLM ê¸°ë°˜ ì¿¼ë¦¬ ì¬ì‘ì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
 - Cross-Encoder ì „ì—­ ì¬ìˆœìœ„í™” (ë©€í‹° PDF ì§€ì›)
 - ë¬¸ì„œ ì„ë² ë”© ìºì‹± ë° ì¬ì‚¬ìš©
 - ìºì‹œ ë¬´íš¨í™” ì „ëµ (ëª¨ë¸/ì²­í¬ ì„¤ì • í•´ì‹œ í¬í•¨)
 - GPU ìì› ì‹±ê¸€í„´ ê´€ë¦¬
 - ë‹¤ì–‘ì„± ì œì•½ (ë¬¸ì„œë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜)

ê°œì„ ì‚¬í•­:
- Cross-Encoder ì „ì—­ ë­í‚¹ ë¬¸ì œ í•´ê²°
- ì„ë² ë”© ì¬ìˆœìœ„í™” ì‹œ embed_documents() ì‚¬ìš©
- ì¿¼ë¦¬ ì¬ì‘ì„± ê°€ì¤‘ì¹˜ë¥¼ max â†’ meanìœ¼ë¡œ ê°œì„ 
- vLLM í˜¸ì¶œ ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
- ë¡œê·¸ ë ˆë²¨ ê°œì„  (ë¯¼ê°ì •ë³´ DEBUG ì „ìš©)
"""

import os
import logging
import json
import hashlib
import time
import unicodedata
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional

# LangChain imports
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.docstore.in_memory import InMemoryDocstore

# LangChain reranker
try:
    from langchain_community.document_compressors import FlashrankRerank
    RERANKER_AVAILABLE = True
except ImportError:
    RERANKER_AVAILABLE = False

from config import (
    EMBEDDING_MODEL, EMBEDDING_CACHE_DIR, VECTORSTORE_CACHE_DIR,
    CHUNK_SIZE, CHUNK_OVERLAP, TOP_K_RESULTS,
    # vLLM ì„œë²„ ì„¤ì •
    VLLM_SERVER_URL, VLLM_API_KEY, LLM_MODEL_NAME, TEMPERATURE, TOP_P,
    # GPU ë¶„ë¦¬ ì„¤ì •
    RAG_GPU_DEVICE, FAISS_MODE
)

from ocr_processor import OCRProcessor

logger = logging.getLogger(__name__)


def _get_config_default(name: str, default_value):
    """config.pyì— ì˜µì…˜ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆì „í•œ ê¸°ë³¸ê°’ ì œê³µ"""
    try:
        import config as _cfg
        return getattr(_cfg, name, default_value)
    except Exception:
        return default_value


# Query Rewriting ê¸°ë³¸ê°’
QUERY_REWRITING_ENABLED = _get_config_default("QUERY_REWRITING_ENABLED", True)
QUERY_REWRITE_NUM = _get_config_default("QUERY_REWRITE_NUM", 3)
QUERY_REWRITE_TIMEOUT = _get_config_default("QUERY_REWRITE_TIMEOUT", 25)
QUERY_REWRITE_MAX_RETRIES = _get_config_default("QUERY_REWRITE_MAX_RETRIES", 2)

# Reranking ê¸°ë³¸ê°’
RERANKING_ENABLED = _get_config_default("RERANKING_ENABLED", True)
RERANK_TOP_N = _get_config_default("RERANK_TOP_N", 24)
CANDIDATES_PER_QUERY = _get_config_default("CANDIDATES_PER_QUERY", 12)
ORIGINAL_QUERY_WEIGHT = _get_config_default("ORIGINAL_QUERY_WEIGHT", 0.6)
REWRITE_QUERIES_WEIGHT = _get_config_default("REWRITE_QUERIES_WEIGHT", 0.4)

# Cross-Encoder ì„¤ì •
CROSS_ENCODER_RERANKING_ENABLED = _get_config_default("CROSS_ENCODER_RERANKING_ENABLED", True)
CROSS_ENCODER_MODEL = _get_config_default("CROSS_ENCODER_MODEL", "BAAI/bge-reranker-base")
CROSS_ENCODER_TOP_N = _get_config_default("CROSS_ENCODER_TOP_N", TOP_K_RESULTS)

# ë‹¤ì–‘ì„± ì œì•½
MAX_CHUNKS_PER_DOC = _get_config_default("MAX_CHUNKS_PER_DOC", 3)


# GPU ìì› ì‹±ê¸€í„´
_GPU_RESOURCES = None

def _get_gpu_resources():
    """GPU ìì›ì„ ì‹±ê¸€í„´ìœ¼ë¡œ ê´€ë¦¬"""
    global _GPU_RESOURCES
    if _GPU_RESOURCES is None:
        try:
            import faiss
            if faiss.get_num_gpus() > 0:
                _GPU_RESOURCES = faiss.StandardGpuResources()
                _GPU_RESOURCES.setTempMemory(512 * 1024 * 1024)  # 512MB
                logger.info("GPU ìì› ì‹±ê¸€í„´ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"GPU ìì› ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    return _GPU_RESOURCES


def _get_cache_hash() -> str:
    """ìºì‹œ ë¬´íš¨í™”ë¥¼ ìœ„í•œ ì„¤ì • í•´ì‹œ ìƒì„±"""
    config_str = f"{EMBEDDING_MODEL}_{CHUNK_SIZE}_{CHUNK_OVERLAP}"
    return hashlib.md5(config_str.encode()).hexdigest()[:8]


class AdvancedRAGSystem:
    """Query rewriting + reranking ì§€ì› RAG ì‹œìŠ¤í…œ (ê°œì„  ë²„ì „)"""

    def __init__(self):
        # FAISS ëª¨ë“œ ì„¤ì • (configì—ì„œ ë¡œë“œ)
        # FAISS_MODE="cpu"ë©´ GPU ì‚¬ìš© ì•ˆ í•¨ (OOM ë°©ì§€, ê¶Œì¥)
        # FAISS_MODE="gpu"ë©´ GPU ì‚¬ìš©
        self.faiss_mode = FAISS_MODE.lower()

        if self.faiss_mode == "gpu":
            self.use_gpu = self._check_gpu_availability()
            if self.use_gpu:
                logger.info("GPU FAISS ì‚¬ìš© ê°€ëŠ¥ - GPU ê°€ì† í™œì„±í™”")
            else:
                logger.info("GPU FAISS ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ í´ë°±")
        else:
            self.use_gpu = False
            logger.info(f"FAISS_MODE={FAISS_MODE} - CPU ëª¨ë“œë¡œ ì‹¤í–‰ (OOM ë°©ì§€)")

        # ë””ë°”ì´ìŠ¤ ì„¤ì • (Embeddingìš© - vLLMê³¼ ë¶„ë¦¬ëœ GPU ì‚¬ìš©)
        import torch
        if torch.cuda.is_available() and RAG_GPU_DEVICE:
            # RAG ì „ìš© GPU ì‚¬ìš© (GPU 2)
            gpu_id = int(RAG_GPU_DEVICE)
            if gpu_id < torch.cuda.device_count():
                self.device = f"cuda:{gpu_id}"
                logger.info(f"ğŸ”¥ RAG ì „ìš© GPU ì‚¬ìš©: {self.device} (vLLMê³¼ ë¶„ë¦¬)")
            else:
                self.device = "cpu"
                logger.warning(f"GPU {gpu_id} ì—†ìŒ, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        else:
            self.device = "cpu"
            logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")

        # ì„ë² ë”© ë¡œë”
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            cache_folder=str(EMBEDDING_CACHE_DIR),
            model_kwargs={"device": self.device}
        )

        # OCR
        self.ocr_processor = OCRProcessor()

        # VectorStore ìºì‹œ
        self.pdf_vectorstores: Dict[str, FAISS] = {}
        self.pdf_documents: Dict[str, List[Document]] = {}
        self.pdf_names: Dict[str, str] = {}
        
        # ë¬¸ì„œ ì„ë² ë”© ìºì‹œ (ì¬ìˆœìœ„í™” ì„±ëŠ¥ ê°œì„ )
        self.pdf_embeddings_cache: Dict[str, np.ndarray] = {}

        # ì²­í‚¹
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
            keep_separator=True
        )

        # ìºì‹œ ë””ë ‰í† ë¦¬ (í•´ì‹œ í¬í•¨)
        self.cache_hash = _get_cache_hash()
        self.vectorstore_cache_dir = Path(VECTORSTORE_CACHE_DIR) / f"v_{self.cache_hash}"
        self.vectorstore_cache_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“¦ ìºì‹œ ë””ë ‰í† ë¦¬: {self.vectorstore_cache_dir} (í•´ì‹œ: {self.cache_hash})")

        # vLLM endpoint
        self.vllm_endpoint = VLLM_SERVER_URL.rstrip("/") + "/chat/completions"
        self.vllm_headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {VLLM_API_KEY}" if VLLM_API_KEY else ""
        }

        # LangChain Reranker (FlashrankRerank)
        self.reranker = None
        if CROSS_ENCODER_RERANKING_ENABLED and RERANKER_AVAILABLE:
            try:
                logger.info(f"ğŸ”§ Reranker ì´ˆê¸°í™”: FlashrankRerank (top_n={CROSS_ENCODER_TOP_N})")
                self.reranker = FlashrankRerank(
                    top_n=CROSS_ENCODER_TOP_N,
                    model="ms-marco-MiniLM-L-12-v2"  # Flashrank ê¸°ë³¸ ëª¨ë¸
                )
                logger.info(f"âœ… Reranker ë¡œë“œ ì™„ë£Œ (top_n: {CROSS_ENCODER_TOP_N})")
            except Exception as e:
                logger.warning(f"Reranker ë¡œë“œ ì‹¤íŒ¨, ì„ë² ë”© ê¸°ë°˜ ì¬ìˆœìœ„í™”ë¡œ í´ë°±: {e}")
                self.reranker = None
        elif CROSS_ENCODER_RERANKING_ENABLED and not RERANKER_AVAILABLE:
            logger.warning("FlashrankRerank ì‚¬ìš© ë¶ˆê°€. ì„ë² ë”© ê¸°ë°˜ ì¬ìˆœìœ„í™” ì‚¬ìš©")
            self.reranker = None

        logger.info("ğŸš€ Advanced RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _check_gpu_availability(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (ì‹±ê¸€í„´ ìì› í™œìš©)"""
        try:
            import faiss
            gpu_count = faiss.get_num_gpus()
            if gpu_count > 0:
                # ì‹±ê¸€í„´ GPU ìì› ì´ˆê¸°í™” ì‹œë„
                gpu_res = _get_gpu_resources()
                return gpu_res is not None
            return False
        except Exception as e:
            logger.warning(f"GPU FAISS í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    def _create_faiss_index(self, embeddings: np.ndarray, use_gpu: bool = None) -> "faiss.Index":
        """FAISS ì¸ë±ìŠ¤ ìƒì„± (GPU ì‹±ê¸€í„´ ìì› í™œìš©)"""
        import faiss
        if use_gpu is None:
            use_gpu = self.use_gpu

        dimension = embeddings.shape[1]
        embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

        if use_gpu:
            try:
                cpu_index = faiss.IndexFlatIP(dimension)
                gpu_res = _get_gpu_resources()
                if gpu_res is not None:
                    # RAG_GPU_DEVICEëŠ” í”„ë¡œì„¸ìŠ¤ ë‚´ë¶€ ì¸ë±ìŠ¤ (CUDA_VISIBLE_DEVICES ì ìš© í›„)
                    gpu_id = int(RAG_GPU_DEVICE) if RAG_GPU_DEVICE else 0
                    gpu_index = faiss.index_cpu_to_gpu(gpu_res, gpu_id, cpu_index)
                    gpu_index.add(embeddings_norm.astype("float32"))
                    return gpu_index
                else:
                    logger.warning("GPU ìì› ì—†ìŒ, CPUë¡œ í´ë°±")
            except Exception as e:
                logger.warning(f"GPU FAISS ìƒì„± ì‹¤íŒ¨, CPUë¡œ í´ë°±: {e}")

        index = faiss.IndexFlatIP(dimension)
        index.add(embeddings_norm.astype("float32"))
        return index

    def _save_vectorstore_with_gpu_support(self, vectorstore: FAISS, cache_path: Path) -> bool:
        try:
            import faiss
            index_type = type(vectorstore.index).__name__
            is_gpu_index = "Gpu" in index_type or hasattr(vectorstore.index, "getDevice")
            if is_gpu_index:
                cpu_index = faiss.index_gpu_to_cpu(vectorstore.index)
                original_index = vectorstore.index
                vectorstore.index = cpu_index
                vectorstore.save_local(str(cache_path))
                vectorstore.index = original_index
            else:
                vectorstore.save_local(str(cache_path))
            return True
        except Exception as e:
            logger.warning(f"ë²¡í„°ìŠ¤í† ì–´ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def _load_vectorstore_with_gpu_support(self, cache_path: Path, pdf_path: str) -> Optional[FAISS]:
        try:
            import faiss
            vectorstore = FAISS.load_local(
                str(cache_path),
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            if self.use_gpu:
                try:
                    res = faiss.StandardGpuResources()
                    res.setTempMemory(512 * 1024 * 1024)
                    gpu_id = int(RAG_GPU_DEVICE) if RAG_GPU_DEVICE else 0
                    gpu_index = faiss.index_cpu_to_gpu(res, gpu_id, vectorstore.index)
                    vectorstore.index = gpu_index
                except Exception as e:
                    logger.warning(f"GPU ë³µì‚¬ ì‹¤íŒ¨, CPU ëª¨ë“œ ì‚¬ìš©: {e}")
            self.pdf_vectorstores[pdf_path] = vectorstore
            return vectorstore
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    # =====================
    # Query Rewriting (vLLM with Retry)
    # =====================
    def _rewrite_queries(self, user_query: str) -> List[str]:
        """ì¿¼ë¦¬ ì¬ì‘ì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        if not QUERY_REWRITING_ENABLED or QUERY_REWRITE_NUM <= 0:
            return []

        import requests
        from time import sleep
        
        system_prompt = (
            "ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í•œêµ­ì–´ ì§ˆë¬¸ì„ ì •ë³´ê²€ìƒ‰ì— ìµœì í™”ëœ ì§§ì€ ì§ˆì˜ë¡œ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬ì‘ì„±í•˜ì„¸ìš”. "
            "- ë„ë©”ì¸ ìš©ì–´/ë™ì˜ì–´/ì¶•ì•½/ì™„ê³¡ í‘œí˜„ í˜¼í•©\n"
            "- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°\n"
            "- 1ì¤„ë‹¹ 1ê°œì˜ ì¬ì‘ì„± ì¿¼ë¦¬\n"
            "- ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šê¸°"
        )
        user_prompt = (
            f"ì›ë³¸ ì§ˆë¬¸: {user_query}\n"
            f"ìœ„ ì§ˆë¬¸ì„ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ {QUERY_REWRITE_NUM}ê°œ ì¬ì‘ì„±í•˜ì„¸ìš”."
        )

        payload = {
            "model": LLM_MODEL_NAME,
            "temperature": TEMPERATURE,
            "top_p": TOP_P,
            "max_tokens": 512,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            # Query Rewritingì—ëŠ” reasoning ë¶ˆí•„ìš” - ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ë¹„í™œì„±í™”
            "extra_body": {
                "chat_template_kwargs": {"enable_thinking": False}
            }
        }

        # ì¬ì‹œë„ ë¡œì§ (ì§€ìˆ˜ ë°±ì˜¤í”„)
        for attempt in range(QUERY_REWRITE_MAX_RETRIES + 1):
            try:
                resp = requests.post(
                    self.vllm_endpoint, 
                    headers=self.vllm_headers, 
                    json=payload, 
                    timeout=QUERY_REWRITE_TIMEOUT
                )
                resp.raise_for_status()
                data = resp.json()
                message = data.get("choices", [{}])[0].get("message", {})
                
                # EXAONE reasoning mode: content ë˜ëŠ” reasoning_content ì‚¬ìš©
                content = message.get("content") or message.get("reasoning_content")
                
                # contentê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                if not content:
                    logger.warning(f"vLLM APIì—ì„œ ë¹ˆ ì‘ë‹µ ë°˜í™˜ (ì‹œë„ {attempt+1})")
                    if attempt < QUERY_REWRITE_MAX_RETRIES:
                        wait_time = (2 ** attempt) * 0.5
                        sleep(wait_time)
                        continue
                    else:
                        return []
                
                # DEBUG ë ˆë²¨ì—ì„œë§Œ ì¬ì‘ì„± ì¿¼ë¦¬ ì¶œë ¥ (ë¯¼ê°ì •ë³´ ë³´í˜¸)
                logger.debug(f"ì¬ì‘ì„± ì¿¼ë¦¬ ì‘ë‹µ: {content}")
                
                lines = [l.strip("- â€¢\t ") for l in content.splitlines() if l.strip()]
                rewrites: List[str] = []
                for line in lines:
                    # ë‹¨ìˆœ ë²ˆí˜¸ ì œê±°
                    cleaned = line
                    if ":" in cleaned and cleaned.split(":")[0].strip().isdigit():
                        cleaned = cleaned.split(":", 1)[1].strip()
                    elif len(cleaned) > 2 and cleaned[:2].isdigit() and cleaned[1] == ".":
                        cleaned = cleaned[2:].strip()
                    if cleaned:
                        rewrites.append(cleaned)
                    if len(rewrites) >= QUERY_REWRITE_NUM:
                        break
                
                # ì¬ì‘ì„± ì¿¼ë¦¬ ë‚´ìš© ì¶œë ¥ (ê²€ìƒ‰ í’ˆì§ˆ í™•ì¸ìš©)
                logger.info(f"âœ… ì¿¼ë¦¬ ì¬ì‘ì„± ì„±ê³µ: {len(rewrites)}ê°œ ìƒì„±")
                for i, rq in enumerate(rewrites, 1):
                    logger.info(f"   {i}. {rq}")
                return rewrites
                
            except Exception as e:
                if attempt < QUERY_REWRITE_MAX_RETRIES:
                    wait_time = (2 ** attempt) * 0.5  # ì§€ìˆ˜ ë°±ì˜¤í”„: 0.5s, 1s, 2s...
                    logger.warning(f"ì¿¼ë¦¬ ì¬ì‘ì„± ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{QUERY_REWRITE_MAX_RETRIES+1}), {wait_time}ì´ˆ í›„ ì¬ì‹œë„: {e}")
                    sleep(wait_time)
                else:
                    logger.warning(f"ì¿¼ë¦¬ ì¬ì‘ì„± ìµœì¢… ì‹¤íŒ¨, ì›ë¬¸ë§Œ ì‚¬ìš©: {e}")
                    return []
        
        return []

    # =====================
    # Reranking helpers (ê°œì„  ë²„ì „)
    # =====================
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        denom = (np.linalg.norm(a) * np.linalg.norm(b))
        if denom == 0:
            return 0.0
        return float(np.dot(a, b) / denom)

    def _rerank_with_embeddings(self, query: str, candidate_docs: List[Document], rewrite_queries: List[str]) -> List[Tuple[Document, float]]:
        """ì„ë² ë”© ê¸°ë°˜ ì¬ìˆœìœ„í™” (embed_documents() ì‚¬ìš©, mean ê°€ì¤‘ì¹˜)"""
        if not candidate_docs:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        original_query_emb = self.embeddings.embed_query(query)
        rewrite_embs = [self.embeddings.embed_query(q) for q in rewrite_queries] if rewrite_queries else []

        # ë¬¸ì„œ ì„ë² ë”© (embed_documents() ì‚¬ìš© - ì˜¬ë°”ë¥¸ ë¶„í¬)
        doc_texts = [doc.page_content for doc in candidate_docs]
        doc_embs = self.embeddings.embed_documents(doc_texts)

        scored: List[Tuple[Document, float]] = []
        for doc, doc_emb in zip(candidate_docs, doc_embs):
            # ì›ë³¸ ì¿¼ë¦¬ ì ìˆ˜
            score_main = self._cosine_similarity(original_query_emb, np.array(doc_emb))
            
            # ì¬ì‘ì„± ì¿¼ë¦¬ë“¤ì˜ í‰ê·  ì ìˆ˜ (max â†’ meanìœ¼ë¡œ ê°œì„ )
            if rewrite_embs:
                rewrite_scores = [self._cosine_similarity(np.array(re), np.array(doc_emb)) for re in rewrite_embs]
                score_rewrites = float(np.mean(rewrite_scores))
            else:
                score_rewrites = 0.0
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = ORIGINAL_QUERY_WEIGHT * score_main + REWRITE_QUERIES_WEIGHT * score_rewrites
            scored.append((doc, final_score))
        
        scored.sort(key=lambda x: x[1], reverse=True)
        return scored

    def _rerank_with_langchain(self, query: str, candidate_docs: List[Document]) -> List[Tuple[Document, float]]:
        """LangChain Rerankerë¡œ ì¬ìˆœìœ„í™” (ì ìˆ˜ í¬í•¨ ë°˜í™˜)"""
        if not self.reranker:
            return [(doc, 0.0) for doc in candidate_docs]
        try:
            # LangChain rerankerë¡œ ì¬ìˆœìœ„í™”
            compressed = self.reranker.compress_documents(candidate_docs, query)
            
            # ì ìˆ˜ ì¶”ì¶œ (LangChainì´ relevance_scoreë¥¼ metadataì— ì €ì¥)
            scored_docs: List[Tuple[Document, float]] = []
            for i, doc in enumerate(compressed):
                # FlashrankRerankëŠ” relevance_scoreë¥¼ metadataì— ì €ì¥
                score = doc.metadata.get('relevance_score', 1.0 - (i * 0.1))  # ìˆœì„œ ê¸°ë°˜ í´ë°±
                scored_docs.append((doc, float(score)))
            
            return scored_docs
        except Exception as e:
            logger.warning(f"Reranker ì¬ìˆœìœ„í™” ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€: {e}")
            return [(doc, 0.0) for doc in candidate_docs]
    
    def _apply_diversity_constraint(self, scored_docs: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:
        """ë‹¤ì–‘ì„± ì œì•½: í•œ ë¬¸ì„œ(PDF)ë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ"""
        if MAX_CHUNKS_PER_DOC <= 0:
            return scored_docs
        
        doc_chunk_count: Dict[str, int] = {}
        filtered: List[Tuple[Document, float]] = []
        
        for doc, score in scored_docs:
            source = doc.metadata.get("source", "unknown")
            count = doc_chunk_count.get(source, 0)
            
            if count < MAX_CHUNKS_PER_DOC:
                filtered.append((doc, score))
                doc_chunk_count[source] = count + 1
        
        return filtered

    # =====================
    # Document & Vectorstore
    # =====================
    def create_documents_from_pdf(self, pdf_path: str) -> List[Document]:
        import time
        start = time.time()
        text = self.ocr_processor.extract_pdf_text(pdf_path)
        if not text or not text.strip():
            logger.warning(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {Path(pdf_path).name}")
            return []
        # í•œê¸€ NFD â†’ NFC ì •ê·œí™” (ìëª¨ ë¶„í•´ ë°©ì§€ ì•ˆì „ì¥ì¹˜)
        text = unicodedata.normalize('NFC', text)
        chunks = self.text_splitter.split_text(text)
        documents: List[Document] = []
        for i, chunk in enumerate(chunks):
            if not chunk.strip():
                continue
            documents.append(Document(
                page_content=chunk,
                metadata={
                    "source": pdf_path,
                    "pdf_name": Path(pdf_path).name,
                    "chunk_id": i,
                    "total_chunks": len(chunks),
                    "chunk_length": len(chunk),
                }
            ))
        logger.info(f"ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(documents)}ê°œ, {time.time()-start:.2f}s")
        return documents

    def get_vectorstore_cache_path(self, pdf_path: str) -> Path:
        pdf_name = Path(pdf_path).stem
        return self.vectorstore_cache_dir / f"{pdf_name}_vectorstore"

    def build_vectorstore(self, pdf_path: str) -> Optional[FAISS]:
        import time
        start = time.time()
        self.pdf_names[pdf_path] = Path(pdf_path).stem
        cache_path = self.get_vectorstore_cache_path(pdf_path)

        if cache_path.exists():
            vs = self._load_vectorstore_with_gpu_support(cache_path, pdf_path)
            if vs is not None:
                logger.info(f"ìºì‹œ ë¡œë“œ ì™„ë£Œ: {Path(pdf_path).name} ({time.time()-start:.2f}s)")
                return vs

        docs = self.create_documents_from_pdf(pdf_path)
        if not docs:
            return None

        texts = [d.page_content for d in docs]
        embeddings_np = np.array(self.embeddings.embed_documents(texts))
        index = self._create_faiss_index(embeddings_np)

        vectorstore = FAISS(
            embedding_function=self.embeddings,
            index=index,
            docstore=InMemoryDocstore({i: doc for i, doc in enumerate(docs)}),
            index_to_docstore_id={i: i for i in range(len(docs))}
        )

        self._save_vectorstore_with_gpu_support(vectorstore, cache_path)
        self.pdf_vectorstores[pdf_path] = vectorstore
        self.pdf_documents[pdf_path] = docs
        logger.info(f"ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ: {Path(pdf_path).name} ({time.time()-start:.2f}s)")
        return vectorstore

    def build_index(self, pdf_paths: List[str]):
        for p in pdf_paths:
            try:
                self.build_vectorstore(p)
            except Exception as e:
                logger.warning(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨({Path(p).name}): {e}")

    # =====================
    # Search (Rewriting + Reranking)
    # =====================
    def _collect_candidates(self, vectorstore: FAISS, query: str, rewrite_queries: List[str], candidate_k: int) -> List[Document]:
        # ê¸°ë³¸ í›„ë³´ ìˆ˜ì§‘: ì›ë¬¸ + ì¬ì‘ì„± ì§ˆì˜
        search_docs = vectorstore.similarity_search(query, k=candidate_k)
        for rq in rewrite_queries:
            try:
                search_docs.extend(vectorstore.similarity_search(rq, k=min(CANDIDATES_PER_QUERY, candidate_k)))
            except Exception as e:
                logger.warning(f"ì¬ì‘ì„± ì§ˆì˜ ê²€ìƒ‰ ì‹¤íŒ¨({rq}): {e}")
        # ì¤‘ë³µ ì œê±° (source, chunk_id)
        unique = {}
        for d in search_docs:
            key = (d.metadata.get("source"), d.metadata.get("chunk_id"))
            if key not in unique:
                unique[key] = d
        return list(unique.values())

    def search(self, query: str, selected_pdf_path: str = None, top_k: int = TOP_K_RESULTS) -> List[Dict]:
        """ê²€ìƒ‰ (Cross-Encoder ì „ì—­ ë­í‚¹ ì§€ì›, ë‹¤ì–‘ì„± ì œì•½ ì ìš©)"""
        start = time.time()
        rewrite_queries = self._rewrite_queries(query)
        results: List[Dict] = []

        def _docs_to_results(scored_docs: List[Tuple[Document, float]], use_ce_score: bool = False) -> List[Dict]:
            out: List[Dict] = []
            for d, s in scored_docs:
                out.append({
                    "text": d.page_content,
                    "similarity_score": float(s),
                    "pdf_name": d.metadata.get("pdf_name", "Unknown"),
                    "source": d.metadata.get("source", ""),
                    "chunk_id": d.metadata.get("chunk_id", 0),
                    "metadata": d.metadata,
                    "reranked": use_ce_score,  # CE ì¬ìˆœìœ„í™” ì—¬ë¶€ í‘œì‹œ
                })
            return out

        if selected_pdf_path and selected_pdf_path != "all":
            # ë‹¨ì¼ PDF ê²€ìƒ‰
            matched_path = None
            for path in self.pdf_vectorstores.keys():
                if Path(path).stem == Path(selected_pdf_path).stem or Path(path).name == selected_pdf_path:
                    matched_path = path
                    break
            if not matched_path:
                logger.warning(f"ì„ íƒëœ PDF ë²¡í„°ìŠ¤í† ì–´ ì—†ìŒ: {selected_pdf_path}")
                return []

            vs = self.pdf_vectorstores[matched_path]
            candidate_k = max(RERANK_TOP_N if RERANKING_ENABLED else top_k, top_k)
            candidates = self._collect_candidates(vs, query, rewrite_queries, candidate_k)

            if self.reranker and RERANKING_ENABLED:
                scored = self._rerank_with_langchain(query, candidates)
                # ë‹¤ì–‘ì„± ì œì•½ ì ìš©
                scored = self._apply_diversity_constraint(scored)
                results = _docs_to_results(scored[:top_k], use_ce_score=True)
            elif RERANKING_ENABLED:
                scored = self._rerank_with_embeddings(query, candidates, rewrite_queries)
                # ë‹¤ì–‘ì„± ì œì•½ ì ìš©
                scored = self._apply_diversity_constraint(scored)
                results = _docs_to_results(scored[:top_k])
            else:
                scored = [(d, 0.0) for d in candidates]
                results = _docs_to_results(scored[:top_k])

            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ(ë‹¨ì¼): {len(results)}ê°œ, {time.time()-start:.3f}s")
            logger.info("ğŸ“‹ ìµœì¢… ê²€ìƒ‰ ê²°ê³¼:")
            for i, r in enumerate(results, 1):
                preview = r['text'][:100].replace('\n', ' ')
                logger.info(f"   {i}. [{r['similarity_score']:.4f}] {r['pdf_name']} (ì²­í¬ #{r['chunk_id']}) - {preview}...")
            return results

        # ì „ì²´ PDF ê²€ìƒ‰ - Cross-Encoder ì „ì—­ ë­í‚¹ ë¬¸ì œ í•´ê²°
        logger.info(f"ğŸ” ì „ì²´ PDF ê²€ìƒ‰ ì‹œì‘ (PDF ìˆ˜: {len(self.pdf_vectorstores)})")
        
        # 1ë‹¨ê³„: ëª¨ë“  PDFì—ì„œ í›„ë³´ ìˆ˜ì§‘
        all_candidates: List[Document] = []
        for pdf_path, vs in self.pdf_vectorstores.items():
            try:
                candidate_k = CANDIDATES_PER_QUERY if RERANKING_ENABLED else top_k
                candidates = self._collect_candidates(vs, query, rewrite_queries, candidate_k)
                all_candidates.extend(candidates)
                logger.debug(f"  - {Path(pdf_path).name}: {len(candidates)}ê°œ í›„ë³´")
            except Exception as e:
                logger.warning(f"ê²€ìƒ‰ ì‹¤íŒ¨({Path(pdf_path).name}): {e}")
        
        logger.info(f"ğŸ“¦ ì´ í›„ë³´ ìˆ˜: {len(all_candidates)}ê°œ")
        
        if not all_candidates:
            logger.warning("í›„ë³´ ë¬¸ì„œ ì—†ìŒ")
            return []
        
        # 2ë‹¨ê³„: ì „ì—­ ì¬ìˆœìœ„í™” (ëª¨ë“  í›„ë³´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬)
        if self.reranker and RERANKING_ENABLED:
            # LangChain Rerankerë¡œ ì „ì—­ ì¬ìˆœìœ„í™”
            logger.info(f"ğŸ”§ Reranker ì „ì—­ ì¬ìˆœìœ„í™” ì‹œì‘ ({len(all_candidates)}ê°œ â†’ {CROSS_ENCODER_TOP_N}ê°œ)")
            scored = self._rerank_with_langchain(query, all_candidates)
            # ë‹¤ì–‘ì„± ì œì•½ ì ìš©
            scored = self._apply_diversity_constraint(scored)
            results = _docs_to_results(scored[:top_k], use_ce_score=True)
            
        elif RERANKING_ENABLED:
            # ì„ë² ë”© ê¸°ë°˜ ì¬ìˆœìœ„í™”
            logger.info(f"ğŸ“Š ì„ë² ë”© ê¸°ë°˜ ì¬ìˆœìœ„í™” ì‹œì‘ ({len(all_candidates)}ê°œ)")
            scored = self._rerank_with_embeddings(query, all_candidates, rewrite_queries)
            # ë‹¤ì–‘ì„± ì œì•½ ì ìš©
            scored = self._apply_diversity_constraint(scored)
            results = _docs_to_results(scored[:top_k])
            
        else:
            # ì¬ìˆœìœ„í™” ì—†ìŒ
            scored = [(d, 0.0) for d in all_candidates]
            results = _docs_to_results(scored[:top_k])

        # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê¹… (í’ˆì§ˆ í™•ì¸ìš©)
        logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ(ì „ì²´): {len(results)}ê°œ, {time.time()-start:.3f}s")
        logger.info("ğŸ“‹ ìµœì¢… ê²€ìƒ‰ ê²°ê³¼:")
        for i, r in enumerate(results, 1):
            preview = r['text'][:100].replace('\n', ' ')
            logger.info(f"   {i}. [{r['similarity_score']:.4f}] {r['pdf_name']} (ì²­í¬ #{r['chunk_id']}) - {preview}...")
        
        return results

    # =====================
    # Utils (ê°œì„  ë²„ì „)
    # =====================
    def format_search_results(self, results: List[Dict], selected_pdf_path: str = None) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… (ì¬ìˆœìœ„í™” ì •ë³´ í¬í•¨)"""
        if not results:
            return "ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        if selected_pdf_path and selected_pdf_path != "all":
            pdf_name = Path(selected_pdf_path).stem
            formatted = f"## ğŸ“‹ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© (ê²€ìƒ‰ ëŒ€ìƒ: {pdf_name})\n\n"
        else:
            formatted = "## ğŸ“‹ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© (ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰)\n\n"
        
        for i, r in enumerate(results, 1):
            # ì¬ìˆœìœ„í™” ë±ƒì§€
            rerank_badge = "ğŸ”§ Reranked" if r.get('reranked', False) else ""
            
            formatted += f"### {i}. {r['pdf_name']} {rerank_badge}\n"
            formatted += f"**ì²­í¬ ID:** {r['chunk_id']} | **ê´€ë ¨ë„:** {r['similarity_score']:.4f}\n\n"
            formatted += f"**ë‚´ìš©:**\n```\n{r['text']}\n```\n\n"
            formatted += "---\n\n"
        
        return formatted

    def get_pdf_list(self) -> List[Tuple[str, str]]:
        """PDF ëª©ë¡ ë°˜í™˜ (íŒŒì¼ ì •ë³´ í¬í•¨)"""
        pdf_list = [("ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ (All Documents)", "all")]
        for pdf_path in self.pdf_names.keys():
            pdf_name = self.pdf_names[pdf_path]
            # ë²¡í„°ìŠ¤í† ì–´ ì •ë³´
            if pdf_path in self.pdf_vectorstores:
                vs = self.pdf_vectorstores[pdf_path]
                chunk_count = vs.index.ntotal if hasattr(vs.index, 'ntotal') else 0
                display_name = f"{pdf_name} ({chunk_count} chunks)"
            else:
                display_name = pdf_name
            pdf_list.append((display_name, pdf_path))
        return pdf_list

    def get_system_stats(self) -> Dict:
        """ì‹œìŠ¤í…œ í†µê³„ (ìºì‹œ í•´ì‹œ í¬í•¨)"""
        reranker_info = "disabled"
        if self.reranker:
            reranker_info = "flashrank_rerank"
        elif RERANKING_ENABLED:
            reranker_info = "embedding_based"
        
        return {
            "total_pdfs": len(self.pdf_vectorstores),
            "total_vectors": sum(vs.index.ntotal for vs in self.pdf_vectorstores.values()) if self.pdf_vectorstores else 0,
            "gpu_enabled": self.use_gpu,
            "device": self.device,
            "embedding_model": EMBEDDING_MODEL,
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "cache_hash": self.cache_hash,
            "reranker": reranker_info,
            "query_rewriting": QUERY_REWRITING_ENABLED,
            "max_chunks_per_doc": MAX_CHUNKS_PER_DOC,
        }


def test_advanced_rag():
    from config import PDF_FILES
    rag = AdvancedRAGSystem()
    existing = [p for p in PDF_FILES if Path(p).exists()]
    if not existing:
        print("âŒ í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    rag.build_vectorstore(existing[0])
    results = rag.search("ì‚¬ì—…ë¹„ ì§‘í–‰ ê¸°ì¤€", existing[0], top_k=3)
    print(f"ğŸ” ê²°ê³¼ {len(results)}ê°œ")
    for i, r in enumerate(results):
        print(f"{i+1}. {r['similarity_score']:.3f} | {r['pdf_name']} #{r['chunk_id']}")
    return True


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_advanced_rag()


