import logging
import json
import re
from typing import List, Dict, Generator, Tuple, Optional
from pathlib import Path
import requests
from openai import OpenAI

from config import (
    VLLM_SERVER_URL, VLLM_API_KEY, LLM_MODEL_NAME, ENABLE_REASONING,
    MAX_TOKENS, TEMPERATURE, TOP_P, TOP_K, MIN_P, SYSTEM_MESSAGE, TOP_K_RESULTS
)
from advanced_rag_system import AdvancedRAGSystem
from exaone_agent_manager import exaone_agent

logger = logging.getLogger(__name__)

class ExaoneClient:
    def __init__(self, server_url, api_key, model_name="LGAI-EXAONE/EXAONE-4.0-32B-GPTQ"):
        self.server_url = server_url
        self.api_key = api_key
        self.model_name = model_name

        self.client = OpenAI(
            api_key=api_key,
            base_url=server_url,
        )

        logger.info(f"ExaoneClient initialized {server_url} with {model_name}")

    def generate_response(self, messages: List[Dict], enable_thinking: bool = False,
                          temperature: float = TEMPERATURE, max_tokens: int = MAX_TOKENS,
                          top_p: float = TOP_P, stream: bool = True):

        try:
            response = self.client.chat.completions.create(
                model = self.model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                stream=stream,
                extra_body={
                    "chat_template_kwargs": {"enable_thinking": enable_thinking},
                },
            )

            return response

        except Exception as e:
            logger.error(f"EXAONE API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None


class ChatManager:

    def __init__(self):
        
        self.rag_system = AdvancedRAGSystem()

        self.exaone_client = ExaoneClient(
            server_url=VLLM_SERVER_URL,
            api_key=VLLM_API_KEY,
            model_name = LLM_MODEL_NAME,
        )

        self.reasoning_temperature = 0.6
        self.reasoning_top_p = 0.95
        self.non_reasoning_temperature = 0.4
        self.non_reasoning_top_p = 0.9

    def initialize_system(self, pdf_paths: List[str]):
        logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        self.rag_system.build_index(pdf_paths)
        logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _prepare_messages(self, user_message: str, history: List[List[str]], rag_context: str = None, agent_context: str = None, reasoning_mode: bool = True):
        messages = []

        system_content = SYSTEM_MESSAGE
        # ë¹„ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” <think> íƒœê·¸ ì§€ì‹œë¥¼ ì œê±°í•˜ì—¬ ë¶ˆí•„ìš”í•œ í† í° ì†Œë¹„ ë°©ì§€
        if not reasoning_mode:
            system_content = re.sub(
                r'ì‘ë‹µ í˜•ì‹:.*?í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”\.',
                'ì‘ë‹µ í˜•ì‹:\nêµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ìµœì¢… ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì„¸ìš”.',
                system_content,
                flags=re.DOTALL
            )
        if agent_context:
            system_content += f"\n\n === Agent ì‹¤í–‰ ê²°ê³¼ ===\n{agent_context}\n\nìœ„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        if rag_context:
            system_content += f"\n\n === ì°¸ê³  ë¬¸ì„œ ====\n{rag_context}\n\nìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        messages.append({
            "role": "system",
            "content": system_content
        })

        # ë©€í‹°í„´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ê°œì„ 
        # íˆìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ê¸¸ë©´ ìµœê·¼ ëŒ€í™”ë§Œ ìœ ì§€ (í† í° ì œí•œ ê³ ë ¤)
        max_history_turns = 10  # ìµœëŒ€ 10í„´ê¹Œì§€ ìœ ì§€
        if len(history) > max_history_turns:
            history = history[-max_history_turns:]
            logger.info(f"ğŸ”„ [HISTORY] íˆìŠ¤í† ë¦¬ê°€ ê¸¸ì–´ì„œ ìµœê·¼ {max_history_turns}í„´ë§Œ ìœ ì§€")

        for human_msg, ai_msg in history:
            if human_msg and human_msg.strip():
                messages.append({"role": "user", "content": human_msg.strip()})
            if ai_msg and ai_msg.strip():
                # AI ë©”ì‹œì§€ì—ì„œ UI ë§ˆí¬ë‹¤ìš´ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
                clean_ai_msg = self._clean_ai_message(ai_msg.strip())
                messages.append({"role": "assistant", "content": clean_ai_msg})

        messages.append({"role": "user", "content": user_message})

        logger.info(f"ğŸ”§ [HISTORY] ì¤€ë¹„ëœ ë©”ì‹œì§€ ìˆ˜: {len(messages)} (ì‹œìŠ¤í…œ: 1, íˆìŠ¤í† ë¦¬: {len(history)*2}, í˜„ì¬: 1)")
        return messages

    def _clean_ai_message(self, ai_msg: str) -> str:
        """AI ë©”ì‹œì§€ì—ì„œ UI ë§ˆí¬ë‹¤ìš´ê³¼ ìƒíƒœ ë©”ì‹œì§€ë¥¼ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€"""
        # UI ìƒíƒœ ë©”ì‹œì§€ íŒ¨í„´ ì œê±°
        patterns_to_remove = [
            r'ğŸ” \[PROCESS\].*?\n?',
            r'ğŸ¤– EXAONE Agent.*?\n?',
            r'ğŸ¤” \*\*\[.*?\]\*\*\n?',
            r'ğŸ’¬ \*\*\[.*?\]\*\*\n?',
            r'```\n.*?\n```',  # ì¶”ë¡  ê³¼ì • ì½”ë“œ ë¸”ë¡
            r'âœ….*?ì™„ë£Œ.*?\n?',
            r'âŒ.*?ì˜¤ë¥˜.*?\n?',
            r'ğŸ§  \*\*\[.*?\]\*\*\n?',
            r'(?m)^\s*(ğŸ¤–|ğŸ”§|âš™ï¸|ğŸ§ |ğŸ“±|âœ…|âŒ).*$\n?',
            r'ğŸ”§ .*?\n?',
            r'âš™ï¸ .*?\n?',
            r'ğŸ“± .*?\n?',
            r'ğŸ§  .*?\n?',
        ]
        
        cleaned_msg = ai_msg
        for pattern in patterns_to_remove:
            cleaned_msg = re.sub(pattern, '', cleaned_msg, flags=re.DOTALL | re.MULTILINE)
        
        # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        cleaned_msg = re.sub(r'\n\s*\n+', '\n\n', cleaned_msg)
        cleaned_msg = cleaned_msg.strip()
        
        return cleaned_msg

    def _estimate_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ì¶”ì • (í•œêµ­ì–´ ê¸°ì¤€ ëŒ€ëµ ê¸€ìë‹¹ 1.5í† í°)"""
        if not text:
            return 0
        return int(len(text) * 1.5)

    def _estimate_messages_tokens(self, messages: List[Dict]) -> int:
        """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ë¥¼ ì¶”ì •"""
        total = 0
        for msg in messages:
            total += self._estimate_tokens(msg.get("content", ""))
            total += 4  # role ë“± ë©”íƒ€ë°ì´í„° ì˜¤ë²„í—¤ë“œ
        return total

    def _truncate_rag_context(self, messages: List[Dict], max_input_tokens: int, max_tokens: int) -> List[Dict]:
        """ì…ë ¥ í† í°ì´ í•œë„ë¥¼ ì´ˆê³¼í•˜ë©´ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶•ì†Œ"""
        available_input = max_input_tokens - max_tokens - 100  # ì—¬ìœ ë¶„ 100í† í°
        estimated = self._estimate_messages_tokens(messages)

        if estimated <= available_input:
            return messages

        logger.warning(f"âš ï¸ [TRUNCATE] ì…ë ¥ í† í° ì¶”ì •ì¹˜({estimated}) > í—ˆìš© í•œë„({available_input}), RAG ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ")

        # system ë©”ì‹œì§€ì—ì„œ RAG ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ì„ ì¶•ì†Œ
        truncated = []
        for msg in messages:
            if msg["role"] == "system" and "=== ì°¸ê³  ë¬¸ì„œ ====" in msg["content"]:
                parts = msg["content"].split("=== ì°¸ê³  ë¬¸ì„œ ====")
                if len(parts) == 2:
                    base_content = parts[0]
                    rag_part = parts[1]

                    # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° ìˆ˜ ê³„ì‚°
                    other_tokens = self._estimate_messages_tokens([m for m in messages if m["role"] != "system"])
                    base_tokens = self._estimate_tokens(base_content)
                    available_for_rag = available_input - other_tokens - base_tokens - 50

                    if available_for_rag > 200:
                        # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í—ˆìš© ë²”ìœ„ ë‚´ë¡œ ì˜ë¼ëƒ„
                        max_chars = int(available_for_rag / 1.5)
                        truncated_rag = rag_part[:max_chars]
                        logger.info(f"ğŸ“ [TRUNCATE] RAG ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ: {len(rag_part)} -> {len(truncated_rag)} ë¬¸ì")
                        msg = {"role": "system", "content": base_content + "=== ì°¸ê³  ë¬¸ì„œ ====" + truncated_rag + "\n\n(ì¼ë¶€ ë‚´ìš©ì´ ê¸¸ì´ ì œí•œìœ¼ë¡œ ìƒëµë¨)"}
                    else:
                        # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì•„ì˜ˆ ì œê±°
                        logger.warning("âš ï¸ [TRUNCATE] RAG ì»¨í…ìŠ¤íŠ¸ ì™„ì „ ì œê±° (ê³µê°„ ë¶€ì¡±)")
                        msg = {"role": "system", "content": base_content.rstrip() + "\n\n(ì°¸ê³  ë¬¸ì„œê°€ ê¸¸ì´ ì œí•œìœ¼ë¡œ ìƒëµë¨)"}
            truncated.append(msg)

        return truncated

    def _call_exaone_api(self, messages: List[Dict], stream, reasoning_mode):
        if reasoning_mode:
            temperature = self.reasoning_temperature
            top_p = self.reasoning_top_p
        else:
            temperature = self.non_reasoning_temperature
            top_p = self.non_reasoning_top_p

        enable_thinking = reasoning_mode

        # vLLM ì„œë²„ì˜ max_model_len (configê³¼ ë™ê¸°í™” í•„ìš”)
        max_model_len = 16384
        max_tokens = MAX_TOKENS if MAX_TOKENS <= max_model_len else 2048

        # ì…ë ¥ í† í° ì´ˆê³¼ ì‹œ RAG ì»¨í…ìŠ¤íŠ¸ ìë™ ì¶•ì†Œ
        messages = self._truncate_rag_context(messages, max_model_len, max_tokens)

        # max_tokens ë‹¨ê³„ì  ì‹œë„
        max_tokens_candidates = [max_tokens, max_tokens // 2, 1024]
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        max_tokens_candidates = sorted(set(max_tokens_candidates), reverse=True)

        for mt in max_tokens_candidates:
            logger.info(f"ğŸ”§ [TOKENS] max_tokens: {mt} ì‹œë„")
            response = self.exaone_client.generate_response(
                messages=messages,
                enable_thinking=enable_thinking,
                temperature=temperature,
                top_p=top_p,
                max_tokens=mt,
                stream=stream,
            )
            if response is not None:
                return response
            logger.warning(f"âš ï¸ [TOKENS] max_tokens={mt} ì‹¤íŒ¨, ë” ë‚®ì€ ê°’ìœ¼ë¡œ ì¬ì‹œë„")

        logger.error("âŒ ëª¨ë“  max_tokens ì‹œë„ ì‹¤íŒ¨")
        return None
    
    def _extract_exaone_response(self, response) -> str:
        if not response.strip():
            return "", ""
        
        # ì¼€ì´ìŠ¤ 1: <think>...</think> í˜•ì‹
        think_pattern = r"<think>(.*?)</think>"
        think_match = re.search(think_pattern, response, re.DOTALL)

        if think_match:
            thinking_part = think_match.group(1).strip()
            final_answer = re.sub(think_pattern, "", response, flags=re.DOTALL).strip()
            return thinking_part, final_answer
        
        # ì¼€ì´ìŠ¤ 2: </think>ë§Œ ìˆëŠ” í˜•ì‹ (ì¶”ë¡  ê³¼ì • -> </think> -> ìµœì¢… ë‹µë³€)
        think_end_pattern = r"(.*?)</think>(.*)"
        think_end_match = re.search(think_end_pattern, response, re.DOTALL)
        
        if think_end_match:
            thinking_part = think_end_match.group(1).strip()
            final_answer = think_end_match.group(2).strip()
            return thinking_part, final_answer
        
        # ì¼€ì´ìŠ¤ 3: íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°
        return "", response.strip()
    
    def _print_final_response(self, reasoning_content, content):

        logger.info("\n" + "="*80)
        logger.info("ğŸ§  EXAONE 4.0 REASONING & RESPONSE")
        logger.info("="*80)

        thinking_part = ""
        final_answer = ""

        extracted_thinking, extracted_final_answer = self._extract_exaone_response(reasoning_content)
        thinking_part = extracted_thinking if extracted_thinking else reasoning_content.strip()
        final_answer = content.strip() if content.strip() else extracted_final_answer

        logger.info("\nğŸ’¡ ì¶”ë¡  ê³¼ì •:")
        logger.info(thinking_part)
        logger.info("\nğŸ” ìµœì¢… ë‹µë³€:")
        logger.info(final_answer)
        logger.info("="*80 + "\n")

    def _stream_response_with_reasoning(self, response) -> Generator[dict, None, None]:
        """EXAONE reasoningì„ ì§€ì›í•˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
        thinking_started = False
        content_started = False
        reasoning_content_complete = ""  # ì „ì²´ reasoning_content ìˆ˜ì§‘
        content_buffer = ""  # contentì—ì„œ </think> íƒœê·¸ ê°ì§€ìš© ë²„í¼
        reasoning_seen = False  # reasoning_contentê°€ í•œ ë²ˆì´ë¼ë„ ë„ì°©í–ˆëŠ”ì§€ ì¶”ì 
        
        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
            for chunk in response:
                if hasattr(chunk, 'choices') and chunk.choices:
                    choice = chunk.choices[0]
                    delta = choice.delta if hasattr(choice, 'delta') else {}
                    
                    # EXAONE: reasoning_content ìˆ˜ì§‘ (ì‹¤ì‹œê°„ í‘œì‹œ + ìµœì¢… ë‹µë³€ìš©)
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        reasoning_chunk = delta.reasoning_content
                        reasoning_content_complete += reasoning_chunk  # ì „ì²´ ìˆ˜ì§‘
                        reasoning_seen = True
                        
                        if not thinking_started:
                            # ì²« ë²ˆì§¸ thinking chunkì¼ ë•Œ ì‹œì‘ ì‹ í˜¸
                            logger.info(f"ğŸ¤” [THINKING START] EXAONE Reasoning ì‹œì‘")
                            yield {'type': 'thinking_start'}
                            thinking_started = True
                        yield {'type': 'thinking_chunk', 'content': reasoning_chunk}
                    
                    # content í•„ë“œ ì²˜ë¦¬ (</think> íƒœê·¸ ê°ì§€)
                    if hasattr(delta, 'content') and delta.content:
                        content_chunk = delta.content
                        content_buffer += content_chunk
                        
                        # reasoning_contentë¥¼ ì´ë¯¸ ë°›ì•˜ë‹¤ë©´ contentëŠ” ê³§ë°”ë¡œ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬
                        if reasoning_seen and not content_started and thinking_started:
                            yield {'type': 'thinking_end'}
                            content_started = True
                            content_buffer = ""  # ë‹µë³€ ì²˜ë¦¬ë¡œ ì „í™˜í–ˆìœ¼ë¯€ë¡œ ë²„í¼ ì´ˆê¸°í™”
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        elif reasoning_seen and content_started:
                            # ì´ë¯¸ ë‹µë³€ êµ¬ê°„ì´ë¼ë©´ ë°”ë¡œ ì¶œë ¥
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        
                        # ì´ë¯¸ content ëª¨ë“œë©´ ë°”ë¡œ ì¶œë ¥
                        if content_started:
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        
                        # </think> íƒœê·¸ ê°ì§€
                        if '</think>' in content_buffer:
                            logger.info(f"ğŸ” [CONTENT] content í•„ë“œì—ì„œ </think> íƒœê·¸ ê°ì§€")
                            
                            # </think> ì „ê¹Œì§€ëŠ” thinkingìœ¼ë¡œ, ì´í›„ëŠ” contentë¡œ ì²˜ë¦¬
                            parts = content_buffer.split('</think>', 1)
                            thinking_part = parts[0]
                            answer_part = parts[1] if len(parts) > 1 else ""
                            
                            # thinkingì´ ì‹œì‘ ì•ˆëìœ¼ë©´ ì‹œì‘ ì‹ í˜¸ (ë²„í¼ì— ìŒ“ì¸ ë‚´ìš© ì¶œë ¥)
                            if not thinking_started:
                                logger.info(f"ğŸ¤” [THINKING START] contentì—ì„œ ì¶”ë¡  ê³¼ì • ì‹œì‘ (ë²„í¼ì—ì„œ)")
                                yield {'type': 'thinking_start'}
                                thinking_started = True
                                # ë²„í¼ì— ìŒ“ì¸ thinking ë¶€ë¶„ í•œ ë²ˆì— ì¶œë ¥
                                if thinking_part:
                                    yield {'type': 'thinking_chunk', 'content': thinking_part}
                            # ì´ë¯¸ thinkingì´ ì‹œì‘ëìœ¼ë©´ ë²„í¼ ë‚´ìš©ì„ ë‹¤ì‹œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°©ì§€)
                            
                            yield {'type': 'thinking_end'}
                            content_started = True
                            
                            # </think> ì´í›„ ë¶€ë¶„ ì¶œë ¥
                            if answer_part:
                                yield {'type': 'content', 'content': answer_part}
                            
                            content_buffer = ""  # ë²„í¼ ì´ˆê¸°í™”
                        
                        else:
                            # </think> íƒœê·¸ê°€ ì•„ì§ ì•ˆë‚˜ì™”ìœ¼ë©´ thinkingìœ¼ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥
                            if not thinking_started:
                                logger.info(f"ğŸ¤” [THINKING START] content í•„ë“œì—ì„œ ì‹œì‘")
                                yield {'type': 'thinking_start'}
                                thinking_started = True
                            
                            yield {'type': 'thinking_chunk', 'content': content_chunk}
                    
                    elif hasattr(delta, 'content') and not delta.content:
                        # ë¹ˆ ë¬¸ìì—´ì´ ì˜¤ëŠ” ê²½ìš°ëŠ” ë¬´ì‹œ
                        pass
            
            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì²˜ë¦¬
            # finish_reason í™•ì¸ (ë§ˆì§€ë§‰ chunkì—ì„œ)
            if 'chunk' in dir() and hasattr(chunk, 'choices') and chunk.choices:
                finish_reason = getattr(chunk.choices[0], 'finish_reason', None)
                if finish_reason:
                    logger.info(f"ğŸ [FINISH] finish_reason: {finish_reason}")
                    if finish_reason == 'length':
                        logger.warning("âš ï¸ [TRUNCATED] í† í° í•œë„ë¡œ ì¸í•´ ì‘ë‹µì´ ì˜ë ¸ìŠµë‹ˆë‹¤!")
                        yield {'type': 'warning', 'content': '\n\nâš ï¸ (ì‘ë‹µì´ í† í° í•œë„ë¡œ ì¸í•´ ì˜ë ¸ìŠµë‹ˆë‹¤)'}

            if thinking_started and not content_started:
                logger.info(f"[DEBUG] Stream ended, sending thinking_end")
                yield {'type': 'thinking_end'}
                
                # contentê°€ ì •ë§ë¡œ ì—†ëŠ” ê²½ìš°ì—ë§Œ ëŒ€ì²´ content ì¶œë ¥
                # (content í•„ë“œì—ì„œ </think>ë¡œ êµ¬ë¶„ëœ ê²½ìš°ëŠ” ì´ë¯¸ content_started=Trueì´ë¯€ë¡œ ì—¬ê¸° ë„ë‹¬ ì•ˆí•¨)
                logger.warning(f"[DEBUG] content ì—†ì´ ì¢…ë£Œë¨ - reasoning_content: {len(reasoning_content_complete)}ì, buffer: {len(content_buffer)}ì")
                
                # reasoning_contentë¥¼ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‚¬ìš© (EXAONE non-reasoning ëª¨ë“œì—ì„œ ë°œìƒ)
                if reasoning_content_complete.strip():
                    # </think> íƒœê·¸ ì œê±° í›„ ì¶œë ¥
                    cleaned_content = reasoning_content_complete
                    if '</think>' in cleaned_content:
                        parts = cleaned_content.split('</think>', 1)
                        cleaned_content = parts[1] if len(parts) > 1 else parts[0]
                    
                    if cleaned_content.strip():
                        yield {'type': 'content', 'content': cleaned_content.strip()}
                        content_started = True
                elif content_buffer.strip():
                    # ë²„í¼ì—ë§Œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë²„í¼ ë‚´ìš©ì„ ë‹µë³€ìœ¼ë¡œ
                    yield {'type': 'content', 'content': content_buffer}
                    content_started = True
                            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {'type': 'error', 'content': f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

    def _stream_response_simple(self, response) -> Generator[dict, None, None]:
        """EXAONE Non-reasoning ëª¨ë“œìš© ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬

        ë¹„ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” <think> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
        ì§§ì€ ì´ˆê¸° ë²„í¼ë¡œ </think> ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸ í›„ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë°.
        """
        content_buffer = ""
        buffer_phase = True  # ì´ˆê¸° ë²„í¼ë§ ë‹¨ê³„
        BUFFER_LIMIT = 50    # ì²« 50ìê¹Œì§€ë§Œ </think> ì²´í¬

        try:
            for chunk in response:
                if hasattr(chunk, 'choices') and chunk.choices:
                    choice = chunk.choices[0]
                    delta = choice.delta if hasattr(choice, 'delta') else {}

                    # reasoning_content ë˜ëŠ” contentì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = ""
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        text = delta.reasoning_content
                    elif hasattr(delta, 'content') and delta.content:
                        text = delta.content

                    if not text:
                        continue

                    if buffer_phase:
                        content_buffer += text
                        if '</think>' in content_buffer:
                            # </think> ë°œê²¬ â†’ ì´í›„ ë‚´ìš©ë§Œ ì¶œë ¥
                            parts = content_buffer.split('</think>', 1)
                            buffer_phase = False
                            content_buffer = ""
                            if len(parts) > 1 and parts[1].strip():
                                yield {'type': 'content', 'content': parts[1]}
                        elif len(content_buffer) >= BUFFER_LIMIT:
                            # </think> ì—†ìŒ â†’ ë²„í¼ í”ŒëŸ¬ì‹œ í›„ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° ì „í™˜
                            buffer_phase = False
                            yield {'type': 'content', 'content': content_buffer}
                            content_buffer = ""
                    else:
                        # ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë°
                        yield {'type': 'content', 'content': text}

            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ - ì”ì—¬ ë²„í¼ í”ŒëŸ¬ì‹œ
            if content_buffer:
                yield {'type': 'content', 'content': content_buffer}

        except Exception as e:
            logger.error(f"Non-reasoning ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {'type': 'error', 'content': f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

    # def _process_agent(self, user_message: str, history: List[List[str]], use_exaone_native: bool = True, reasoning_mode: bool = True) -> Dict:
    #     """Agent ì²˜ë¦¬ë¥¼ ê°„ì†Œí™”ëœ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰"""
    #     # EXAONE ë„¤ì´í‹°ë¸Œ tool use ìš°ì„  ì‚¬ìš©
    #     if use_exaone_native and exaone_agent.is_available():
    #         try:
    #             logger.info("ğŸ¤– EXAONE ë„¤ì´í‹°ë¸Œ Agent ì‚¬ìš©")
    #             agent_result = exaone_agent.run_agent(user_message, history, reasoning_mode)
                
    #             if agent_result.get("success"):
    #                 return {
    #                     "success": True,
    #                     "content": agent_result.get("content", ""),
    #                     "tool_calls": agent_result.get("tool_calls", []),
    #                     "agent_type": "exaone_native"
    #                 }
    #             else:
    #                 error_msg = agent_result.get("error", "EXAONE Agent ì‹¤í–‰ ì‹¤íŒ¨")
    #                 logger.warning(f"âš ï¸ EXAONE Agent ì‹¤íŒ¨: {error_msg}")
    #                 return {"success": False, "error": "Agentë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                    
    #         except Exception as e:
    #             logger.error(f"âŒ EXAONE Agent ì˜ˆì™¸: {e}")
    #             return {"success": False, "error": "Agentë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    #     else:
    #         return {"success": False, "error": "EXAONE Agentë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    def _reset_processing_state(self):
        """ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™” - ì—ëŸ¬ ë³µêµ¬ë‚˜ ìƒˆ ì„¸ì…˜ ì‹œì‘ ì‹œì—ë§Œ ì‚¬ìš©"""
        logger.info("ğŸ”„ [RESET] ChatManager ìƒíƒœ ì´ˆê¸°í™” ì¤‘...")
        
        # ExaoneClient ìƒíƒœ ì´ˆê¸°í™” (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±)
        try:
            self.exaone_client = ExaoneClient(
                server_url=VLLM_SERVER_URL,
                api_key=VLLM_API_KEY,
                model_name=LLM_MODEL_NAME,
            )
            logger.info("ğŸ”„ ExaoneClient ì¬ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ExaoneClient ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ExaoneAgentManager ìƒíƒœë„ ì´ˆê¸°í™”
        try:
            if hasattr(exaone_agent, 'client'):
                # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ì´ˆê¸°í™”
                from openai import OpenAI
                exaone_agent.client = OpenAI(
                    api_key=VLLM_API_KEY or "EMPTY",
                    base_url=VLLM_SERVER_URL
                )
                logger.info("ğŸ”„ ExaoneAgentManager í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ExaoneAgentManager ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def start_new_session(self):
        """ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ ì‹œì‘ - ëª…ì‹œì ìœ¼ë¡œ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•  ë•Œ ì‚¬ìš©"""
        logger.info("ğŸ†• [SESSION] ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ ì‹œì‘")
        self._reset_processing_state()
        logger.info("âœ… [SESSION] ìƒˆë¡œìš´ ì„¸ì…˜ ì¤€ë¹„ ì™„ë£Œ")

    def process_message(self, user_message: str, history: List[List[str]], agent_mode: bool = False, 
                    reasoning_mode: bool = True, enable_reasoning: bool = True, enable_rag: bool = True,
                    selected_pdf_path: Optional[str] = None, reset_state: bool = False) -> Generator[Tuple[List[List[str]], str], None, None]:
        
        # í•„ìš”í•œ ê²½ìš°ì—ë§Œ ìƒíƒœ ì´ˆê¸°í™” (ìƒˆ ì„¸ì…˜ ì‹œì‘, ì—ëŸ¬ ë³µêµ¬ ë“±)
        if reset_state:
            logger.info("ğŸ”„ [RESET] ëª…ì‹œì  ìƒíƒœ ì´ˆê¸°í™” ìš”ì²­")
            self._reset_processing_state()
        
        # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´ - ë§¤ë²ˆ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
        
        # ë””ë²„ê¹…: íŒŒë¼ë¯¸í„° ìƒíƒœ ë¡œê¹…
        
        logger.info(f"ğŸ”§ [DEBUG] ChatManager íŒŒë¼ë¯¸í„° - Reasoning Mode: {reasoning_mode}, Enable Reasoning: {enable_reasoning}, Agent Mode: {agent_mode}, Enable RAG: {enable_rag}")
        
        if not user_message or not user_message.strip():
            yield (history, "âŒ ë¹ˆ ë©”ì‹œì§€ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        current_api_status = {"api": "", "message": ""}
        called_apis = []  

        def status_callback(api_name: str, message: str):
            current_api_status["api"] = api_name
            current_api_status["message"] = message
            
            # í˜¸ì¶œëœ API ì¶”ì  (ë” ì •í™•í•œ íŒ¨í„´ ë§¤ì¹­)
            if api_name == "serpapi":
                if any(keyword in message for keyword in ["ì›¹ ê²€ìƒ‰ ì™„ë£Œ", "ê²€ìƒ‰ ì™„ë£Œ", "ê²°ê³¼ ë°œê²¬"]):
                    if "SerpAPI" not in called_apis:
                        called_apis.append("SerpAPI")
                        logger.info(f"ğŸ“± [API-TRACK] SerpAPI í˜¸ì¶œ í™•ì¸: {message}")
            elif api_name == "kma":
                if any(keyword in message for keyword in ["ê¸°ìƒì²­ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ", "ê¸°ìƒ ì •ë³´", "ë‚ ì”¨ ì •ë³´"]):
                    if "ê¸°ìƒì²­ API" not in called_apis:
                        called_apis.append("ê¸°ìƒì²­ API")
                        logger.info(f"ğŸ“± [API-TRACK] ê¸°ìƒì²­ API í˜¸ì¶œ í™•ì¸: {message}")
            
            # ì¤‘ìš”í•œ ìƒíƒœ ë³€í™”ë§Œ ì¶œë ¥ (reasoningê³¼ progressëŠ” ì¶œë ¥ ì•ˆí•¨)
            if api_name not in ["reasoning", "progress"]:
                logger.info(f"ğŸ”„ [UI-STATUS] {api_name.upper()}: {message}")


        # selected_pdf_pathëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì€ ê°’ ì‚¬ìš© (Noneì´ë©´ ëª¨ë“  PDFì—ì„œ ê²€ìƒ‰)
        rag_context = ""
        search_results = []
        doc_info = ""

        try:
            logger.info(f"ğŸ” [PROCESS] ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘: {user_message}")
            new_history = history + [[user_message, ""]]

            yield (new_history, "ğŸ” [PROCESS] ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")

            agent_results = {}
            agent_context = None

            if agent_mode and exaone_agent.is_available():
                logger.info("ğŸ¤– [PROCESS] Agent ëª¨ë“œ í™œì„±í™”")
                
                # Agent ëª¨ë“œì—ì„œëŠ” í•­ìƒ ì¶”ë¡  ëª¨ë“œ ê°•ì œ í™œì„±í™”
                agent_reasoning_mode = True
                logger.info(f"ğŸ§  [AGENT] Agent ëª¨ë“œì—ì„œ ì¶”ë¡  ëª¨ë“œ ê°•ì œ í™œì„±í™”: {reasoning_mode} -> {agent_reasoning_mode}")
                
                # UIì— Agent ëª¨ë“œ ì‹œì‘ ìƒíƒœ í‘œì‹œ
                new_history[-1][1] = "ğŸ¤– EXAONE Agent ì‹¤í–‰ ì‹œì‘...\n\n"
                yield (new_history, doc_info)

                exaone_agent.set_status_callback(status_callback)

                agent_results = {"success": False}
                agent_status_message = ""
                agent_thinking_content = ""
                agent_thinking_in_progress = False
                
                try:
                    # Agentì— ì „ë‹¬í•  íˆìŠ¤í† ë¦¬ í´ë¦°ì—… (UI ë§ˆì»¤, ìƒíƒœ ë©”ì‹œì§€ ì œê±°)
                    cleaned_history_for_agent = []
                    try:
                        for h_msg, a_msg in (history or []):
                            cleaned_agent_ai_msg = self._clean_ai_message(a_msg or "") if a_msg else ""
                            cleaned_history_for_agent.append([h_msg, cleaned_agent_ai_msg])
                    except Exception:
                        cleaned_history_for_agent = history or []

                    for result_chunk in exaone_agent.run_agent_stream(user_message, cleaned_history_for_agent, agent_reasoning_mode):
                        chunk_type = result_chunk.get("type")
                        chunk_message = result_chunk.get("message", "")

                        # Agent ì¶”ë¡  ê³¼ì • ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (Agent ëª¨ë“œì—ì„œëŠ” í•­ìƒ í‘œì‹œ)
                        if chunk_type == "agent_thinking_start":
                            if enable_reasoning:  # Agent ëª¨ë“œì—ì„œëŠ” ì‚¬ìš©ì ì¶”ë¡  ì„¤ì •ë§Œ ì²´í¬
                                agent_status_message = "ğŸ¤– EXAONE Agent ì‹¤í–‰ ì‹œì‘...\n\nğŸ¤” **[Agent ì¶”ë¡  ê³¼ì •]**\n```\n"
                                agent_thinking_in_progress = True
                                new_history[-1][1] = agent_status_message
                                yield (new_history, doc_info)
                        
                        elif chunk_type == "agent_thinking_chunk":
                            if enable_reasoning and agent_thinking_in_progress:
                                thinking_chunk = result_chunk.get("content", "")
                                agent_thinking_content += thinking_chunk
                                agent_status_message += thinking_chunk
                                new_history[-1][1] = agent_status_message
                                yield (new_history, doc_info)
                        
                        elif chunk_type == "agent_thinking_end":
                            if enable_reasoning and agent_thinking_in_progress:
                                agent_status_message += "\n```\n\n"
                                agent_thinking_in_progress = False
                                new_history[-1][1] = agent_status_message
                                yield (new_history, doc_info)
                        
                        # ìµœì¢… ë‹µë³€ ì¶”ë¡  ê³¼ì • ì²˜ë¦¬ (Agent ëª¨ë“œì—ì„œëŠ” í•­ìƒ í‘œì‹œ)
                        elif chunk_type == "agent_final_thinking_start":
                            if enable_reasoning:  # Agent ëª¨ë“œì—ì„œëŠ” ì‚¬ìš©ì ì¶”ë¡  ì„¤ì •ë§Œ ì²´í¬
                                agent_status_message += "\n\nğŸ§  **[Agent ìµœì¢… ë‹µë³€ ì¶”ë¡ ]**\n```\n"
                                agent_thinking_in_progress = True
                                new_history[-1][1] = agent_status_message
                                yield (new_history, doc_info)
                        
                        elif chunk_type == "agent_final_thinking_chunk":
                            if enable_reasoning and agent_thinking_in_progress:
                                thinking_chunk = result_chunk.get("content", "")
                                agent_status_message += thinking_chunk
                                new_history[-1][1] = agent_status_message
                                yield (new_history, doc_info)
                        
                        elif chunk_type == "agent_final_thinking_end":
                            if enable_reasoning and agent_thinking_in_progress:
                                agent_status_message += "\n```\n\nğŸ’¬ **[Agent ìµœì¢… ë‹µë³€]**\n"
                                agent_thinking_in_progress = False
                                new_history[-1][1] = agent_status_message
                                yield (new_history, doc_info)
                        
                        # Agent content ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                        elif chunk_type == "content":
                            content_chunk = result_chunk.get("content", "")
                            agent_status_message += content_chunk
                            new_history[-1][1] = agent_status_message
                            yield (new_history, doc_info)
                        
                        # Agent ë„êµ¬ í˜¸ì¶œ ìƒíƒœ ì²˜ë¦¬
                        elif chunk_type == "tool_calls_start":
                            tool_count = result_chunk.get("tool_count", 0)
                            tool_msg = f"\n\nğŸ”§ **[ë„êµ¬ í˜¸ì¶œ ì‹œì‘]** ({tool_count}ê°œ ë„êµ¬)\n"
                            agent_status_message += tool_msg
                            new_history[-1][1] = agent_status_message
                            yield (new_history, doc_info)
                        
                        elif chunk_type == "tool_executing":
                            tool_name = result_chunk.get("tool_name", "unknown")
                            tool_msg = f"âš™ï¸ {tool_name} ì‹¤í–‰ ì¤‘...\n"
                            agent_status_message += tool_msg
                            new_history[-1][1] = agent_status_message
                            yield (new_history, doc_info)
                        
                        elif chunk_type == "tool_complete":
                            tool_name = result_chunk.get("tool_name", "unknown")
                            tool_msg = f"âœ… {tool_name} ì™„ë£Œ\n"
                            agent_status_message += tool_msg
                            new_history[-1][1] = agent_status_message
                            yield (new_history, doc_info)

                        elif chunk_type == "final_response":
                            current_message = "\n\nğŸ§  Agent ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...\n"
                            if not enable_reasoning:  # Agent ëª¨ë“œì—ì„œëŠ” ì¶”ë¡  í•­ìƒ í™œì„±í™”ë˜ë¯€ë¡œ enable_reasoningë§Œ ì²´í¬
                                agent_status_message = current_message
                            else:
                                agent_status_message += current_message
                            new_history[-1][1] = agent_status_message
                            yield (new_history, doc_info)
                            
                        elif chunk_type == "agent_complete":
                            agent_results = {
                                "success": True,
                                "content": result_chunk.get("content", ""),
                                "tool_calls": result_chunk.get("tool_calls", []),
                                "agent_type": "exaone_native"
                            }
                            # Agent ì™„ë£Œ ë©”ì‹œì§€ í‘œì‹œ
                            tool_count = len(agent_results.get("tool_calls", []))
                            completion_msg = f"\n\nâœ… EXAONE Agent ì™„ë£Œ! (ë„êµ¬ í˜¸ì¶œ: {tool_count}ê°œ)\n\n"
                            if not enable_reasoning:  # Agent ëª¨ë“œì—ì„œëŠ” ì¶”ë¡  í•­ìƒ í™œì„±í™”ë˜ë¯€ë¡œ enable_reasoningë§Œ ì²´í¬
                                agent_status_message = completion_msg
                            else:
                                agent_status_message += completion_msg
                            break
                            
                        elif chunk_type == "agent_error":
                            agent_results = {"success": False, "error": result_chunk.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")}
                            error_msg = f"âŒ EXAONE Agent ì˜¤ë¥˜: {agent_results['error']}\n\n"
                            if not enable_reasoning:  # Agent ëª¨ë“œì—ì„œëŠ” ì¶”ë¡  í•­ìƒ í™œì„±í™”ë˜ë¯€ë¡œ enable_reasoningë§Œ ì²´í¬
                                agent_status_message = error_msg
                            else:
                                agent_status_message += error_msg
                            break
                            
                except Exception as e:
                    logger.error(f"EXAONE Agent streaming ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    agent_results = {"success": False, "error": str(e)}
                    agent_status_message = f"âŒ EXAONE Agent ì˜ˆì™¸: {str(e)}\n\n"
                
            if agent_results.get("success"):
                logger.info("\nğŸ”— [UI] Agent ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ ì¤‘...")
                agent_context = f"\n\n=== Agent ì‹¤í–‰ ê²°ê³¼ ===\n"

                tool_calls = agent_results.get("tool_calls", [])
                if tool_calls:
                    logger.info(f"ğŸ” [UI] Agent ë„êµ¬ í˜¸ì¶œ í™•ì¸: {tool_calls}")
                    for tool in tool_calls:
                        tool_name = tool.get("name", "")
                        tool_result = tool.get("result", "")
                        agent_context += f"[{tool_name}] {tool_result}\n"
                        logger.info(f"ğŸ” [UI] Agent ë„êµ¬ í˜¸ì¶œ ê²°ê³¼: {tool_name} - {tool_result}")
                else:
                    logger.info("ğŸ” [UI] Agent ë„êµ¬ í˜¸ì¶œ ì—†ìŒ")

                
                agent_content = agent_results.get("content", "")
                if agent_content.strip():
                    agent_context += f"Agent ì‘ë‹µ: {agent_content}\n"
                    logger.info(f"ğŸ” [UI] Agent ì‘ë‹µ: {agent_content}")
                else:
                    logger.info("ğŸ” [UI] Agent ì‘ë‹µ ì—†ìŒ")


            if enable_rag and user_message.strip():
                logger.info("ğŸ” [PROCESS] RAG ê²€ìƒ‰ ëª¨ë“œ í™œì„±í™”")
                rag_status_message = "\nğŸ” [PROCESS] RAG ê²€ìƒ‰ ëª¨ë“œ í™œì„±í™”"
                existing_msg = new_history[-1][1] or ""
                new_history[-1][1] = f"{existing_msg}\n{rag_status_message}" if existing_msg else rag_status_message
                yield (new_history, doc_info)
                
                search_results = self.rag_system.search(
                    user_message,
                    selected_pdf_path,
                    top_k=TOP_K_RESULTS
                )

                if search_results:
                    rag_status_message = f"âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼"
                    existing_msg = new_history[-1][1] or ""
                    new_history[-1][1] = f"{existing_msg}\n{rag_status_message}" if existing_msg else rag_status_message
                    yield (new_history, doc_info)
                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    contexts = []
                    for i, result in enumerate(search_results, 1):
                        context = f"[ë¬¸ì„œ {i}] {result['pdf_name']} (ê´€ë ¨ë„: {result['similarity_score']:.3f})\n{result['text']}"
                        contexts.append(context)
                    
                    rag_context = "\\n\\n".join(contexts)
                    doc_info = self.rag_system.format_search_results(search_results, selected_pdf_path)
                    logger.info(f"âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")

                else:
                    doc_info = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    print(f"âŒ [UI] RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    logger.warning("RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    rag_status_message = "âŒ RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
                    existing_msg = new_history[-1][1] or ""
                    new_history[-1][1] = f"{existing_msg}\n{rag_status_message}" if existing_msg else rag_status_message
                    yield (new_history, doc_info)
            else:
                if not enable_rag:
                    print(f"\nğŸ“š [UI] RAG ê²€ìƒ‰ ë¹„í™œì„±í™”ë¨")
                    doc_info = "ğŸ“š ë¬¸ì„œ ë‚´ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤."
                else:
                    doc_info = "ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤."
            

            messages = self._prepare_messages(user_message, history, rag_context, agent_context, reasoning_mode=reasoning_mode)

                        # ìµœì¢… ë‹µë³€ ìƒì„± ì‹œì‘ì„ UIì— í‘œì‹œ
            if agent_mode or enable_rag:
                final_status_message = "ğŸ§  ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...\n\n"
            else:
                final_status_message = "\n\nğŸ§  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ì¤‘...\n\n"
            
            new_history[-1][1] = final_status_message
            yield (new_history, doc_info)
            
            response = self._call_exaone_api(messages, stream=True, reasoning_mode=reasoning_mode)

            logger.info("\nğŸ’¡ response~~~:")
            logger.info(response)



            accumulated_response = ""
            thinking_in_progress = False
            
            # í„°ë¯¸ë„ ì¶œë ¥ì„ ìœ„í•œ ë³„ë„ ìˆ˜ì§‘ (í•­ìƒ ìˆ˜ì§‘)
            collected_reasoning = ""
            collected_content = ""
            
            # UI í‘œì‹œìš© ë³€ìˆ˜ (reasoning í‘œì‹œ ì—¬ë¶€ì— ë”°ë¼)
            ui_accumulated_response = ""
            ui_content_started = False       

            print(f"\nğŸ”§ [DEBUG-STREAM] ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ê²°ì •:")
            print(f"  reasoning_mode: {reasoning_mode}")
            print(f"  enable_reasoning: {enable_reasoning}")
            print(f"  ì„ íƒëœ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹: {'_stream_response_with_reasoning' if reasoning_mode else '_stream_response_simple'}")
            
            if reasoning_mode:

                for chunk in self._stream_response_with_reasoning(response):
                    chunk_type = chunk.get('type')

                    if chunk_type == "thinking_start":
                        thinking_in_progress = True
                        if enable_reasoning:
                            if agent_mode:
                                existing_message = new_history[-1][1]
                                accumulated_response = existing_message + "\n\nğŸ¤” **[ì¶”ë¡  ê³¼ì •]**\n```\n"
                            else:
                                accumulated_response = "\n\nğŸ¤” **[ì¶”ë¡  ê³¼ì •]**\n```\n"
                            new_history[-1][1] = accumulated_response
                            logger.info(f"ğŸ¤” [UI] ì¶”ë¡  ì‹œì‘: {accumulated_response}")
                            yield (new_history, doc_info)

                    elif chunk_type == "thinking_chunk":
                        thinking_chunk = chunk.get("content", "")
                        collected_reasoning += thinking_chunk

                        if enable_reasoning and thinking_in_progress:
                            accumulated_response += thinking_chunk
                            new_history[-1][1] = accumulated_response
                            yield (new_history, doc_info)
                        elif not enable_reasoning:
                            # enable_reasoningì´ falseì—¬ë„ ìˆ˜ì§‘ì€ í•˜ë˜ UIì—ëŠ” í‘œì‹œ ì•ˆí•¨
                            pass
                    
                    elif chunk_type == "thinking_end":
                        if enable_reasoning and thinking_in_progress:
                            accumulated_response += "\n```\n\nğŸ’¬ **[ìµœì¢… ë‹µë³€]**\n"
                            new_history[-1][1] = accumulated_response
                            yield (new_history, doc_info)
                        thinking_in_progress = False
                    
                    elif chunk_type == "content":
                        content_chunk = chunk.get("content", "")
                        collected_content += content_chunk

                        if not enable_reasoning and not ui_content_started:
                            if agent_mode:
                                existing_message = new_history[-1][1]
                                accumulated_response = existing_message + "\n\nğŸ’¬ **[ìµœì¢… ë‹µë³€]**\n"
                            else:
                                accumulated_response = ""
                            ui_content_started = True
                        
                        accumulated_response += content_chunk
                        new_history[-1][1] = accumulated_response
                        yield (new_history, doc_info)
            else:
                for chunk in self._stream_response_simple(response):
                    chunk_type = chunk.get('type')
                    if chunk_type == "content":
                        content_chunk = chunk.get("content", "")
                        collected_content += content_chunk

                        if not ui_content_started:
                            if agent_mode:
                                existing_message = new_history[-1][1]
                                accumulated_response = existing_message + content_chunk
                            else:
                                accumulated_response = content_chunk
                            ui_content_started = True
                        else:
                            accumulated_response += content_chunk
                        new_history[-1][1] = accumulated_response
                        yield (new_history, doc_info)

            self._print_final_response(collected_reasoning, collected_content)

            if agent_mode:
                agent_formatted = exaone_agent.format_agent_result(agent_results)
                if agent_formatted:
                    existing_msg = new_history[-1][1] or ""
                    new_history[-1][1] = f"{existing_msg}{agent_formatted}"
                yield (new_history, doc_info)

            if new_history and len(new_history) > 0:
                final_message = new_history[-1][1]
                if final_message and final_message.strip():
                    # ì •ìƒì ì¸ ìµœì¢… ì‘ë‹µì´ ìˆëŠ” ê²½ìš°
                    logger.info(f"âœ… ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ - ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(final_message)} ë¬¸ì")
                    # ë§ˆì§€ë§‰ì— í•œ ë²ˆ ë” yieldí•˜ì—¬ UI ì—…ë°ì´íŠ¸ ë³´ì¥
                    yield (new_history, doc_info)
                else:
                    # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì˜¤ë¥˜ ì²˜ë¦¬
                    error_msg = "âŒ ì‘ë‹µ ìƒì„±ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    new_history[-1][1] = error_msg
                    logger.warning("âš ï¸ ì‘ë‹µì´ ë¹„ì–´ìˆì–´ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¡œ ëŒ€ì²´")
                    yield (new_history, doc_info)
            else:
                # new_historyê°€ ì—†ëŠ” ê²½ìš°
                error_history = history + [[user_message, "âŒ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]]
                logger.error("âŒ new_historyê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                yield (error_history, doc_info)
            
                logger.info("âœ… ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë‹¤ìŒ ìš”ì²­ì„ ìœ„í•´ ìƒíƒœ ì¬ì„¤ì •
            try:
                logger.info("ğŸ”„ [ERROR-RECOVERY] ì—ëŸ¬ ë³µêµ¬ë¥¼ ìœ„í•œ ìƒíƒœ ì¬ì„¤ì • ì¤‘...")
                self._reset_processing_state()
                logger.info("âœ… [ERROR-RECOVERY] ìƒíƒœ ì¬ì„¤ì • ì™„ë£Œ")
            except Exception as reset_error:
                logger.warning(f"âš ï¸ [ERROR-RECOVERY] ìƒíƒœ ì¬ì„¤ì • ì‹¤íŒ¨: {reset_error}")
            
            # ì‚¬ìš©ìì—ê²Œ ì¹œí™”ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
            if "connection" in str(e).lower() or "timeout" in str(e).lower():
                error_msg = "âŒ ì„œë²„ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            elif "token" in str(e).lower() or "api" in str(e).lower():
                error_msg = "âŒ API í˜¸ì¶œì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                error_msg = f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {str(e)}"
            
            error_history = history + [[user_message, error_msg]]
            yield (error_history, "âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒíƒœê°€ ì¬ì„¤ì •ë˜ì—ˆìœ¼ë‹ˆ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    def get_pdf_list(self) -> List[Tuple[str, str]]:
        """PDF ëª©ë¡ ë°˜í™˜"""
        return self.rag_system.get_pdf_list()

if __name__ == "__main__":
    import logging
    from pathlib import Path
    
    logging.basicConfig(level=logging.INFO)








