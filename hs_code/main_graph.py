"""
[LangGraph ê¸°ë°˜ Main Entry Point]
Gradio ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜: LangGraph ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ì§„ì…ì 

ê¸°ì¡´ main.pyì˜ UI êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ChatManager.process_message() ëŒ€ì‹ 
LangGraphì˜ graph.invoke() / graph.stream()ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import logging
import uuid
import re
import gradio as gr
from pathlib import Path
from typing import Generator, Tuple, List, Dict, Any

# ì„¤ì • ë° ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
from config import (
    PROJECT_NAME, VERSION, GRADIO_HOST, GRADIO_PORT, GRADIO_THEME,
    PDF_FILES, ensure_directories, VLLM_SERVER_URL, VLLM_API_KEY, LLM_MODEL_NAME
)
from graph import build_graph, GraphState
from graph.query_builder import build_query_graph
from graph.query_state import QueryState
from graph.nodes import _get_rag_system
from graph.query_nodes import parse_analyzer_response
from core.llm_client import ExaoneClient
from utils.pdf_handler import PDFUtils, TempFileManager
from services.ocr_service import OCRProcessor
from langchain_core.messages import HumanMessage, AIMessage

logger = logging.getLogger(__name__)


class LangGraphGradioApp:
    """LangGraph ê¸°ë°˜ Gradio ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤"""

    def __init__(self):
        # 1. ì´ˆê¸°í™”: ì‹œìŠ¤í…œ êµ¬ë™ì— í•„ìš”í•œ í•µì‹¬ ëª¨ë“ˆë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        ensure_directories()

        # [LangGraph] ê¸°ì¡´ ê·¸ë˜í”„ ë¹Œë“œ
        logger.info("ğŸ”¨ LangGraph ê·¸ë˜í”„ ì´ˆê¸°í™” ì¤‘...")
        self.graph = build_graph()

        # [LangGraph v2] Query Pipeline ê·¸ë˜í”„ ë¹Œë“œ
        logger.info("ğŸ”¨ Query Pipeline ê·¸ë˜í”„ ì´ˆê¸°í™” ì¤‘...")
        self.query_graph = build_query_graph()

        # [LLM Client] ë©”ì¸ ë£¨í”„ ì§ì ‘ í˜¸ì¶œìš©
        self.llm_client = ExaoneClient(
            server_url=VLLM_SERVER_URL,
            api_key=VLLM_API_KEY,
            model_name=LLM_MODEL_NAME
        )

        # [Eyes] PDF/OCR ì²˜ë¦¬ê¸°
        self.pdf_utils = PDFUtils()
        self.ocr_processor = OCRProcessor()
        self.temp_file_manager = TempFileManager()

        # PDF íŒŒì¼ ëª©ë¡ ì¤€ë¹„
        self.existing_pdfs = [pdf for pdf in PDF_FILES if Path(pdf).exists()]

        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ)
        if self.existing_pdfs:
            logger.info(f"ğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {len(self.existing_pdfs)}ê°œ PDF íŒŒì¼")
            rag_system = _get_rag_system()
            rag_system.build_index(self.existing_pdfs)
        else:
            logger.warning("PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. data/ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”.")

        logger.info("âœ… LangGraph Gradio ì•± ì´ˆê¸°í™” ì™„ë£Œ")

    def get_pdf_choices(self) -> List[str]:
        """PDF íŒŒì¼ ëª©ë¡ì„ ì„ íƒ ì˜µì…˜ìœ¼ë¡œ ë³€í™˜"""
        choices = []
        for i, pdf_path in enumerate(self.existing_pdfs, 1):
            filename = Path(pdf_path).stem
            choices.append(f"{i}. {filename}")

        return choices if choices else ["PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"]

    def get_pdf_list_for_dropdown(self) -> List[Tuple[str, str]]:
        """PDF íŒŒì¼ ëª©ë¡ì„ ë“œë¡­ë‹¤ìš´ìš©ìœ¼ë¡œ ë³€í™˜"""
        choices = [("ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ (All Documents)", "all")]
        for pdf_path in self.existing_pdfs:
            pdf_name = Path(pdf_path).stem
            choices.append((pdf_name, pdf_path))
        return choices

    def get_pdf_page(self, pdf_index: int, page_num: int) -> Tuple[Any, str]:
        """PDF í˜ì´ì§€ ì´ë¯¸ì§€ ë°˜í™˜"""
        try:
            if pdf_index >= len(self.existing_pdfs) or page_num < 0:
                return None, "ì˜ëª»ëœ í˜ì´ì§€ ë²ˆí˜¸ì…ë‹ˆë‹¤."

            pdf_path = self.existing_pdfs[pdf_index]

            if not Path(pdf_path).exists():
                return None, "PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            total_pages = self.pdf_utils.get_pdf_total_pages(pdf_path)

            if page_num >= total_pages:
                return None, f"í˜ì´ì§€ ë²ˆí˜¸ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. (ìµœëŒ€: {total_pages})"

            image_path = self.pdf_utils.get_pdf_page_image(pdf_path, page_num)

            if image_path:
                self.temp_file_manager.add_temp_file(image_path)

            return image_path, f"Page {page_num + 1} of {total_pages}"

        except Exception as e:
            logger.error(f"PDF í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None, f"PDF ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

    def process_with_graph(
        self,
        user_message: str,
        history: List[Dict[str, str]],
        reasoning_mode: bool,
        agent_mode: bool,
        enable_rag: bool,
        selected_pdf: str,
        uploaded_context: str = ""
    ) -> Generator[Tuple[List[Dict[str, str]], str, str], None, None]:
        """
        LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ì²˜ë¦¬ (Messages í¬ë§· ì§€ì›)

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (Gradio chatbot messages í˜•ì‹ [{'role': 'user', 'content': ...}, ...])
            reasoning_mode: ì¶”ë¡  ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
            agent_mode: Agent ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
            enable_rag: RAG í™œì„±í™” ì—¬ë¶€
            selected_pdf: ì„ íƒëœ PDF ê²½ë¡œ
            uploaded_context: ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©

        Yields:
            (updated_history, doc_info, thinking_content) íŠœí”Œ
        """
        # íˆìŠ¤í† ë¦¬ë¥¼ LangGraph ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        messages = []
        if history:
            for item in history:
                role = item.get('role')
                content = item.get('content')
                if role == 'user':
                    messages.append(HumanMessage(content=content))
                elif role == 'assistant':
                    messages.append(AIMessage(content=content))

        # ì´ˆê¸° ìƒíƒœ êµ¬ì„±
        initial_state: GraphState = {
            "messages": messages,
            "user_input": user_message,
            "uploaded_context": uploaded_context,
            "agent_mode": agent_mode,
            "reasoning_mode": reasoning_mode,
            "enable_rag": enable_rag,
            "selected_pdf": None if selected_pdf == "all" else selected_pdf,
            "next_action": "",
            "tool_results": None,
            "rag_results": None,
            "report_output": None,
            "tool_calls": None,
            "thinking_content": None,
            "final_response": "",
            "reference_docs": None,
        }

        # ì§„í–‰ ì¤‘ í‘œì‹œ (Messages í˜•ì‹)
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        history_with_input = (history or []) + [{"role": "user", "content": user_message}]
        # ì‘ë‹µ í”Œë ˆì´ìŠ¤í™€ë” ì¶”ê°€
        new_history = history_with_input + [{"role": "assistant", "content": "ğŸ”„ ì²˜ë¦¬ ì¤‘..."}]
        yield new_history, "", ""

        try:
            # LangGraph ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
            logger.info(f"ğŸš€ LangGraph ì‹¤í–‰ ì‹œì‘: {user_message[:50]}...")

            # graph.stream()ì„ ì‚¬ìš©í•˜ì—¬ ê° ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°›ìŒ
            doc_info = ""
            thinking_content = ""
            final_response = ""

            for event in self.graph.stream(initial_state):
                # eventëŠ” {ë…¸ë“œì´ë¦„: ë…¸ë“œì¶œë ¥} í˜•ì‹
                for node_name, node_output in event.items():
                    logger.info(f"ğŸ“ ë…¸ë“œ ì‹¤í–‰: {node_name}")

                    # node_outputì´ Noneì¸ ê²½ìš° ìŠ¤í‚µ
                    if not node_output:
                        continue

                    # ì°¸ì¡° ë¬¸ì„œ ì •ë³´ ì—…ë°ì´íŠ¸
                    if node_output.get("reference_docs"):
                        doc_info = node_output["reference_docs"]

                    # ì¶”ë¡  ê³¼ì • ì—…ë°ì´íŠ¸
                    if node_output.get("thinking_content"):
                        thinking_content = node_output["thinking_content"]

                    # ìµœì¢… ì‘ë‹µ ì—…ë°ì´íŠ¸
                    if node_output.get("final_response"):
                        final_response = node_output["final_response"]

                        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (Messages í˜•ì‹)
                        new_history[-1]['content'] = final_response
                        yield new_history, doc_info, thinking_content

            # ìµœì¢… ê²°ê³¼ ë°˜í™˜
            if not final_response:
                final_response = "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                new_history[-1]['content'] = final_response

            yield new_history, doc_info, thinking_content

        except Exception as e:
            logger.error(f"âŒ LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            error_response = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            new_history[-1]['content'] = error_response
            yield new_history, "", ""

    def process_with_query_pipeline(
        self,
        user_message: str,
        history: List[Dict[str, str]],
        reasoning_mode: bool,
        selected_pdf: str,
        uploaded_context: str = "",
        debug_mode: bool = False
    ) -> Generator[Tuple[List[Dict[str, str]], str, str], None, None]:
        """
        Query Pipelineì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ì²˜ë¦¬ (ê°œì„ ëœ íŒŒì´í”„ë¼ì¸)

        ìŠ¤íŠ¸ë¦¬ë° í¬ì¸íŠ¸:
        1. Analyzer: ì¶”ë¡  ê³¼ì • ìŠ¤íŠ¸ë¦¬ë° â†’ thinking_contentì— í‘œì‹œ
        2. Final Response: ìµœì¢… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° â†’ chatbotì— í‘œì‹œ

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            reasoning_mode: ì¶”ë¡  ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
            selected_pdf: ì„ íƒëœ PDF ê²½ë¡œ
            uploaded_context: ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©
            debug_mode: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€

        Yields:
            (updated_history, doc_info, thinking_content) íŠœí”Œ
        """
        # íˆìŠ¤í† ë¦¬ë¥¼ LangGraph ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        messages = []
        if history:
            for item in history:
                role = item.get('role')
                content = item.get('content')
                if role == 'user':
                    messages.append(HumanMessage(content=content))
                elif role == 'assistant':
                    messages.append(AIMessage(content=content))

        # QueryState ì´ˆê¸° ìƒíƒœ êµ¬ì„±
        current_state: QueryState = {
            "messages": messages,
            "user_input": user_message,
            "uploaded_pdf_content": uploaded_context if uploaded_context else None,
            "selected_manual": None if selected_pdf == "all" else selected_pdf,
            "debug_mode": debug_mode,
            "reasoning_mode": reasoning_mode,
            "execution_plan": None,
            "execution_result": None,
            "final_response": "",
            "ready_to_analyze": False,
            "analyzer_messages": [],
            "ready_to_generate": False,
            "final_messages": [],
            "next_node": "",
        }

        # ì§„í–‰ ì¤‘ í‘œì‹œ
        history_with_input = (history or []) + [{"role": "user", "content": user_message}]
        new_history = history_with_input + [{"role": "assistant", "content": "ğŸ”„ ë¶„ì„ ì¤‘..."}]
        yield new_history, "", ""

        try:
            logger.info(f"ğŸš€ Query Pipeline ì‹¤í–‰ ì‹œì‘: {user_message[:50]}...")

            doc_info = ""
            thinking_content = ""
            final_response = ""
            has_pdf = bool(uploaded_context)

            # Query Pipeline ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            for event in self.query_graph.stream(current_state):
                for node_name, node_output in event.items():
                    logger.info(f"ğŸ“ [Query Pipeline] ë…¸ë“œ ì‹¤í–‰: {node_name}")

                    if not node_output:
                        continue

                    # ========================================
                    # 1. ANALYZER ìŠ¤íŠ¸ë¦¬ë° (ì¶”ë¡  ê³¼ì • í‘œì‹œ)
                    # ========================================
                    if node_output.get("ready_to_analyze"):
                        analyzer_messages = node_output.get("analyzer_messages", [])
                        logger.info("ğŸ§  [Analyzer] ì¶”ë¡  ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")

                        new_history[-1]['content'] = "ğŸ§  ì§ˆë¬¸ ë¶„ì„ ì¤‘..."
                        yield new_history, doc_info, "**ğŸ” ë¶„ì„ ì¤‘...**"

                        try:
                            # Analyzer LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (ì¶”ë¡  ëª¨ë“œ ON)
                            stream = self.llm_client.generate_response(
                                messages=analyzer_messages,
                                enable_thinking=True,  # AnalyzerëŠ” í•­ìƒ ì¶”ë¡ 
                                temperature=0.1,
                                stream=True
                            )

                            if stream is None:
                                raise Exception("LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")

                            analyzer_response = ""
                            current_thinking = ""
                            in_thinking = False

                            for chunk in stream:
                                if chunk.choices and chunk.choices[0].delta.content:
                                    content = chunk.choices[0].delta.content
                                    analyzer_response += content

                                    # <think> íƒœê·¸ ë‚´ìš© ì¶”ì¶œí•˜ì—¬ ì‹¤ì‹œê°„ í‘œì‹œ
                                    if "<think>" in analyzer_response and not in_thinking:
                                        in_thinking = True

                                    if in_thinking:
                                        # thinking ë‚´ìš© ì¶”ì¶œ
                                        think_match = re.search(r'<think>(.*?)(?:</think>|$)', analyzer_response, re.DOTALL)
                                        if think_match:
                                            current_thinking = think_match.group(1).strip()
                                            thinking_content = f"**ğŸ§  ë¶„ì„ ì¤‘:**\n{current_thinking}"
                                            yield new_history, doc_info, thinking_content

                                    if "</think>" in content:
                                        in_thinking = False

                            # Analyzer ì‘ë‹µ íŒŒì‹± â†’ ExecutionPlan ìƒì„±
                            plan = parse_analyzer_response(analyzer_response, user_message, has_pdf)

                            # ì‹¤í–‰ ê³„íš í‘œì‹œ
                            plan_info = []
                            if plan.get("need_tools"):
                                plan_info.append(f"ğŸ› ï¸ ë„êµ¬: {plan.get('tool_list', [])}")
                            if plan.get("need_rag"):
                                plan_info.append("ğŸ“š ë§¤ë‰´ì–¼ ê²€ìƒ‰")
                            if plan.get("need_pdf"):
                                plan_info.append("ğŸ“„ PDF ë¶„ì„")

                            if plan_info:
                                thinking_content = f"**ğŸ§  ë¶„ì„ ì™„ë£Œ:**\n{current_thinking}\n\n**ğŸ“‹ ì‹¤í–‰ ê³„íš:** {', '.join(plan_info)}\n**ğŸ’¡ ì´ìœ :** {plan.get('tool_reasoning', '')}"
                            else:
                                thinking_content = f"**ğŸ§  ë¶„ì„ ì™„ë£Œ:**\n{current_thinking}\n\n**ğŸ“‹ ì¼ë°˜ ëŒ€í™”ë¡œ ì‘ë‹µ**"

                            yield new_history, doc_info, thinking_content

                            # current_state ì—…ë°ì´íŠ¸
                            current_state["execution_plan"] = plan

                            # ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ë° ì‹¤í–‰
                            if plan.get("need_tools") or plan.get("need_rag") or plan.get("need_pdf"):
                                new_history[-1]['content'] = "ğŸ”§ ì •ë³´ ìˆ˜ì§‘ ì¤‘..."
                                yield new_history, doc_info, thinking_content

                                # Executor ë…¸ë“œ ì§ì ‘ í˜¸ì¶œ
                                from graph.query_nodes import tool_executor_node, rag_executor_node, pdf_executor_node

                                current_state["execution_result"] = {}

                                if plan.get("need_tools"):
                                    logger.info("ğŸ› ï¸ [Main] ë„êµ¬ ì‹¤í–‰ ì¤‘...")
                                    tool_result = tool_executor_node(current_state)
                                    if tool_result.get("execution_result"):
                                        current_state["execution_result"].update(tool_result["execution_result"])

                                if plan.get("need_rag"):
                                    logger.info("ğŸ“š [Main] RAG ê²€ìƒ‰ ì¤‘...")
                                    rag_result = rag_executor_node(current_state)
                                    if rag_result.get("execution_result"):
                                        current_state["execution_result"].update(rag_result["execution_result"])

                                        # RAG ê²°ê³¼ í‘œì‹œ
                                        rag_docs = current_state["execution_result"].get("rag_results", [])
                                        if rag_docs:
                                            doc_info = "**ğŸ“š ê²€ìƒ‰ëœ ë§¤ë‰´ì–¼:**\n"
                                            for doc in rag_docs[:3]:
                                                if isinstance(doc, dict) and "content" in doc:
                                                    doc_info += f"- {doc.get('source', 'unknown')}: {doc['content'][:200]}...\n"

                                if plan.get("need_pdf"):
                                    logger.info("ğŸ“„ [Main] PDF ë¶„ì„ ì¤‘...")
                                    pdf_result = pdf_executor_node(current_state)
                                    if pdf_result.get("execution_result"):
                                        current_state["execution_result"].update(pdf_result["execution_result"])

                                # Synthesizer í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë©”ì‹œì§€ ì¤€ë¹„
                                from graph.query_nodes import synthesizer_node
                                synth_result = synthesizer_node(current_state)
                                final_messages = synth_result.get("final_messages", [])

                            else:
                                # Direct response
                                from graph.query_nodes import direct_response_node
                                direct_result = direct_response_node(current_state)
                                final_messages = direct_result.get("final_messages", [])

                            # ========================================
                            # 2. ìµœì¢… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                            # ========================================
                            logger.info("ğŸš€ [Main] ìµœì¢… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
                            new_history[-1]['content'] = ""
                            yield new_history, doc_info, thinking_content

                            stream = self.llm_client.generate_response(
                                messages=final_messages,
                                enable_thinking=reasoning_mode,
                                stream=True
                            )

                            partial_response = ""
                            for chunk in stream:
                                if chunk.choices and chunk.choices[0].delta.content:
                                    content = chunk.choices[0].delta.content
                                    partial_response += content

                                    # thinking íƒœê·¸ ì œê±°í•˜ê³  í‘œì‹œ
                                    display_response = re.sub(r'<think>.*?</think>', '', partial_response, flags=re.DOTALL).strip()
                                    new_history[-1]['content'] = display_response
                                    yield new_history, doc_info, thinking_content

                            # ìµœì¢… ì‘ë‹µ ì •ë¦¬ (thinking íƒœê·¸ ì œê±°)
                            final_response = re.sub(r'<think>.*?</think>', '', partial_response, flags=re.DOTALL).strip()
                            new_history[-1]['content'] = final_response
                            logger.info("âœ… [Main] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")

                        except Exception as e:
                            logger.error(f"âŒ Analyzer/ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
                            import traceback
                            traceback.print_exc()
                            final_response = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                            new_history[-1]['content'] = final_response

                        yield new_history, doc_info, thinking_content
                        return  # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì¢…ë£Œ

                    # ========================================
                    # Fallback: ê¸°ì¡´ ë°©ì‹ (ì§ì ‘ ì‘ë‹µ)
                    # ========================================
                    if node_output.get("final_response"):
                        final_response = node_output["final_response"]
                        # thinking íƒœê·¸ ì œê±°
                        final_response = re.sub(r'<think>.*?</think>', '', final_response, flags=re.DOTALL).strip()
                        new_history[-1]['content'] = final_response
                        yield new_history, doc_info, thinking_content

            if not final_response:
                final_response = "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                new_history[-1]['content'] = final_response

            yield new_history, doc_info, thinking_content

        except Exception as e:
            logger.error(f"âŒ Query Pipeline ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            error_response = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            new_history[-1]['content'] = error_response
            yield new_history, "", ""

    def create_interface(self):
        """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        with gr.Blocks(title=f"{PROJECT_NAME} {VERSION} (LangGraph)") as demo:
            gr.Markdown(f"# ğŸ¢ {PROJECT_NAME} (LangGraph Edition)")

            with gr.Row():
                # ì¢Œì¸¡: ì±„íŒ… ì˜ì—­
                with gr.Column(scale=5):
                    chatbot = gr.Chatbot(
                        label="ğŸ’¬ ëŒ€í™” (LangGraph + EXAONE 4.0)",
                        height=500,
                        show_label=True,
                    )

                    with gr.Row():
                        upload_btn = gr.UploadButton("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ (PDF/TXT)", file_types=[ ".pdf", ".txt"])

                        msg = gr.Textbox(
                            label="ë©”ì‹œì§€ ì…ë ¥",
                            placeholder="ì˜ˆ: ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ? / í•œíŒŒ ì£¼ì˜ë³´ í–‰ë™ìš”ë ¹ ì•Œë ¤ì¤˜",
                            lines=2,
                            show_label=True,
                            scale=4
                        )
                        with gr.Row():
                            submit = gr.Button("ì „ì†¡", variant="primary", scale=1)
                            clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”", variant="secondary")

                    # ì¶”ì²œ í”„ë¡¬í”„íŠ¸
                    gr.Examples(
                        examples=[
                            ["ğŸŒ¤ï¸ ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ì™€ ë¯¸ì„¸ë¨¼ì§€ ì•Œë ¤ì¤˜."],
                            ["âš ï¸ í˜„ì¬ ë°œíš¨ ì¤‘ì¸ ê¸°ìƒ íŠ¹ë³´ê°€ ìˆì–´?"],
                            ["ğŸ“š í•œíŒŒ ì£¼ì˜ë³´ ë°œë ¹ ì‹œ ì¡°ì¹˜ ì‚¬í•­ì€ ë­ì•¼?"],
                            ["ğŸ” ìµœê·¼ 3ì¼ê°„ ë°œìƒí•œ ì§€ì§„ ì •ë³´ ì•Œë ¤ì¤˜."],
                            ["ğŸ“ [íŒŒì¼ ì—…ë¡œë“œ í›„] ì´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±í•´ì¤˜."],
                        ],
                        inputs=msg,
                        label="ğŸ’¡ ì¶”ì²œ ì§ˆë¬¸ (í´ë¦­í•´ì„œ ì…ë ¥)"
                    )

                    with gr.Row():
                        reasoning_mode = gr.Checkbox(label="ğŸ§  Reasoning ëª¨ë“œ", value=True)
                        agent_mode = gr.Checkbox(label="ğŸ¤– Agent ëª¨ë“œ (ë„êµ¬ ì‚¬ìš©)", value=True)
                        enable_rag = gr.Checkbox(label="ğŸ“š ë¬¸ì„œ ê²€ìƒ‰ (RAG)", value=True)
                        query_pipeline_mode = gr.Checkbox(label="ğŸ”¬ Query Pipeline v2", value=True, info="ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")

                    with gr.Row():
                        pdf_selector = gr.Dropdown(
                            choices=[c[0] for c in self.get_pdf_list_for_dropdown()],
                            value="ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ (All Documents)",
                            label="ğŸ“š ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ì„ íƒ",
                            info="ë‹µë³€ì— ì°¸ê³ í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”"
                        )

                # ìš°ì¸¡: ë¬¸ì„œ ë·°ì–´ ë° ì •ë³´ í‘œì‹œ
                with gr.Column(scale=5):
                    gr.Markdown("### ğŸ“„ ì›ë³¸ ë¬¸ì„œ ë·°ì–´")

                    pdf_image = gr.Image(label="PDF í˜ì´ì§€", height=400, show_label=False)

                    with gr.Row():
                        pdf_choices = self.get_pdf_choices()
                        pdf_viewer_selector = gr.Dropdown(
                            choices=pdf_choices,
                            value=pdf_choices[0] if pdf_choices else None,
                            label="PDF ì„ íƒ",
                            scale=3
                        )

                    with gr.Row():
                        prev_btn = gr.Button("â—€ ì´ì „", scale=1)
                        page_info = gr.Textbox(
                            value="Page 1 / 1",
                            label="í˜ì´ì§€ ì •ë³´",
                            interactive=False,
                            scale=2
                        )
                        next_btn = gr.Button("ë‹¤ìŒ â–¶", scale=1)

                    gr.Markdown("### ğŸ” ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©")
                    doc_info = gr.Markdown("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.", height=200)

                    gr.Markdown("### ğŸ¤” ì¶”ë¡  ê³¼ì •")
                    thinking_display = gr.Markdown("Reasoning ëª¨ë“œê°€ í™œì„±í™”ë˜ë©´ AIì˜ ì‚¬ê³  ê³¼ì •ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.", height=100)

            # ìƒíƒœ ë³€ìˆ˜ë“¤
            current_pdf_index = gr.State(0)
            current_page = gr.State(0)
            uploaded_file_state = gr.State(None)
            uploaded_context_state = gr.State("")

            # --- ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì •ì˜ ---
            def user_input(user_message, history, reasoning_mode, agent_mode, enable_rag, selected_pdf, uploaded_context, use_query_pipeline):
                """ë©”ì‹œì§€ ì²˜ë¦¬ í•¸ë“¤ëŸ¬"""
                if not user_message:
                    yield history, "", "", "", gr.Button(interactive=True), ""
                    return

                # PDF ì„ íƒê°’ì—ì„œ ì‹¤ì œ ê²½ë¡œ ì¶”ì¶œ
                pdf_list = self.get_pdf_list_for_dropdown()
                selected_pdf_path = "all"
                for name, path in pdf_list:
                    if name == selected_pdf:
                        selected_pdf_path = path
                        break

                # Query Pipeline v2 ì‚¬ìš©
                if use_query_pipeline:
                    logger.info("ğŸ”¬ Query Pipeline v2 ëª¨ë“œë¡œ ì²˜ë¦¬")
                    for new_history, doc_info_content, thinking_content in self.process_with_query_pipeline(
                        user_message,
                        history or [],
                        reasoning_mode,
                        selected_pdf_path,
                        uploaded_context,
                        debug_mode=False
                    ):
                        yield new_history, thinking_content, doc_info_content, "", gr.Button(interactive=False), ""
                else:
                    # ê¸°ì¡´ LangGraphë¡œ ì²˜ë¦¬
                    logger.info("ğŸ”§ ê¸°ì¡´ LangGraph ëª¨ë“œë¡œ ì²˜ë¦¬")
                    for new_history, doc_info_content, thinking_content in self.process_with_graph(
                        user_message,
                        history or [],
                        reasoning_mode,
                        agent_mode,
                        enable_rag,
                        selected_pdf_path,
                        uploaded_context
                    ):
                        yield new_history, thinking_content, doc_info_content, "", gr.Button(interactive=False), ""

                yield new_history, thinking_content, doc_info_content, "", gr.Button(interactive=True), ""

            def handle_upload(file):
                """íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬"""
                if file is None:
                    return None, ""

                try:
                    file_path = file.name
                    if file_path.lower().endswith(".pdf"):
                        logger.info(f"ğŸ“‚ PDF íŒŒì¼ ê°ì§€ (OCR ì²˜ë¦¬): {file_path}")
                        context = self.ocr_processor.extract_pdf_text(file_path)
                    else:
                        with open(file_path, "r", encoding="utf-8") as f:
                            context = f.read()

                    return file, context
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                    return file, f"(íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)})"

            def clear_history():
                """ëŒ€í™” ì´ˆê¸°í™”"""
                self.temp_file_manager.cleanup_temp_files()
                return [], "", "", None, ""

            # --- PDF Viewer Handlers ---
            def change_pdf(pdf_choice, current_pdf_idx, current_pg):
                try:
                    new_pdf_idx = int(pdf_choice.split(".")[0]) - 1
                    if new_pdf_idx < 0 or new_pdf_idx >= len(self.existing_pdfs):
                        new_pdf_idx = 0
                except:
                    new_pdf_idx = 0

                image, page_info_text = self.get_pdf_page(new_pdf_idx, 0)
                return image, page_info_text, new_pdf_idx, 0

            def prev_page(pdf_idx, page):
                if page > 0:
                    new_page = page - 1
                    image, page_info_text = self.get_pdf_page(pdf_idx, new_page)
                    return image, page_info_text, new_page
                return None, f"Page {page + 1} / ?", page

            def next_page(pdf_idx, page):
                if pdf_idx < len(self.existing_pdfs):
                    total_pages = self.pdf_utils.get_pdf_total_pages(self.existing_pdfs[pdf_idx])
                    if page < total_pages - 1:
                        new_page = page + 1
                        image, page_info_text = self.get_pdf_page(pdf_idx, new_page)
                        return image, page_info_text, new_page
                return None, f"Page {page + 1} / ?", page

            def load_initial_pdf():
                if self.existing_pdfs and Path(self.existing_pdfs[0]).exists():
                    image, page_info_text = self.get_pdf_page(0, 0)
                    return image, page_info_text
                return None, "PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

            # --- Event Binding ---
            upload_btn.upload(
                handle_upload,
                inputs=[upload_btn],
                outputs=[uploaded_file_state, uploaded_context_state]
            )

            submit.click(
                user_input,
                inputs=[msg, chatbot, reasoning_mode, agent_mode, enable_rag, pdf_selector, uploaded_context_state, query_pipeline_mode],
                outputs=[chatbot, thinking_display, doc_info, msg, submit, uploaded_context_state],
                api_name=False
            )

            clear.click(
                clear_history,
                outputs=[chatbot, doc_info, thinking_display, uploaded_file_state, uploaded_context_state],
                api_name=False
            )

            msg.submit(
                user_input,
                inputs=[msg, chatbot, reasoning_mode, agent_mode, enable_rag, pdf_selector, uploaded_context_state, query_pipeline_mode],
                outputs=[chatbot, thinking_display, doc_info, msg, submit, uploaded_context_state],
                api_name=False
            )

            pdf_viewer_selector.change(
                change_pdf,
                inputs=[pdf_viewer_selector, current_pdf_index, current_page],
                outputs=[pdf_image, page_info, current_pdf_index, current_page]
            )

            prev_btn.click(prev_page, inputs=[current_pdf_index, current_page], outputs=[pdf_image, page_info, current_page])
            next_btn.click(next_page, inputs=[current_pdf_index, current_page], outputs=[pdf_image, page_info, current_page])

            demo.load(load_initial_pdf, outputs=[pdf_image, page_info])

        demo.queue(default_concurrency_limit=1, max_size=10)
        return demo

    def launch(self, host: str = GRADIO_HOST, port: int = GRADIO_PORT, share: bool = True):
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰"""
        demo = self.create_interface()
        logger.info(f"ğŸš€ LangGraph Gradio ì•± ì‹¤í–‰: {host}:{port}")
        if share:
            print("ğŸŒ ì™¸ë¶€ ê³µê°œ URL ìƒì„± ì¤‘... (í„°ë¯¸ë„ì— ë§í¬ê°€ í‘œì‹œë©ë‹ˆë‹¤)")
        demo.launch(server_name=host, server_port=port, share=share)


def print_startup_info():
    """ì‹œì‘ ì •ë³´ ì¶œë ¥"""
    print(f"\n{'='*80}")
    print(f"ğŸ¢ {PROJECT_NAME} {VERSION} (LangGraph Edition)")
    print(f"{'='*80}")
    print("âœ… ì£¼ìš” ê¸°ëŠ¥:")
    print("  1. âœ… LangGraph ê¸°ë°˜ ì—ì´ì „íŠ¸ ê·¸ë˜í”„")
    print("  2. âœ… Query Pipeline v2 (ê°œì„ ëœ ë„êµ¬ ì„ íƒ)")
    print("  3. âœ… vLLM ì„œë²„ ê¸°ë°˜ ê³ ì† ì¶”ë¡ ")
    print("  4. âœ… LangChain ê¸°ë°˜ RAG ì‹œìŠ¤í…œ")
    print("  5. âœ… FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤")
    print("  6. âœ… Mistral OCR + ìë™ ìºì‹±")
    print("  7. âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ")
    print(f"{'='*80}")
    print("ğŸ”§ Query Pipeline v2 êµ¬ì¡°:")
    print("  START â†’ Analyzer â”€â”¬â”€â†’ Executor â†’ Synthesizer â†’ END")
    print("                    â””â”€â†’ Direct Response â†’ END")
    print(f"{'='*80}")


if __name__ == "__main__":
    import argparse
    import signal
    import sys

    def signal_handler(sig, frame):
        """Ctrl+C ì‹œ ê°•ì œ ì¢…ë£Œ"""
        print("\nğŸ›‘ ì¢…ë£Œ ì¤‘...")
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    parser = argparse.ArgumentParser(description=f'{PROJECT_NAME} {VERSION} (LangGraph)')
    parser.add_argument('--host', type=str, default=GRADIO_HOST, help='Host IP address')
    parser.add_argument('--port', type=int, default=GRADIO_PORT, help='Port number')
    parser.add_argument('--share', action='store_true', default=True, help='Create a public URL')
    parser.add_argument('--no-share', dest='share', action='store_false', help='Disable public URL')
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    print_startup_info()
    print(f"ğŸ“ ì ‘ì† URL: http://localhost:{args.port}")
    print(f"ğŸ¤– vLLM ì„œë²„: {VLLM_SERVER_URL}")

    try:
        app = LangGraphGradioApp()
        app.launch(host=args.host, port=args.port, share=args.share)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("  1. vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print("  2. langgraph íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: pip install langgraph")
        print("  3. PDF íŒŒì¼ì´ data/ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    finally:
        try:
            app.temp_file_manager.cleanup_temp_files()
            logger.info("ğŸ—‘ï¸ ì •ë¦¬ ì‘ì—… ì™„ë£Œ")
        except:
            pass