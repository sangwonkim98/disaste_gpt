"""
[Chat Manager]
ëŒ€í™” íë¦„ ì œì–´ ë° í†µí•© ê´€ë¦¬ (Broker)
ì‚¬ìš©ì ë©”ì‹œì§€ -> [Agent/RAG/Report] ë¶„ê¸° ì²˜ë¦¬ -> LLM í˜¸ì¶œ -> ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
"""

import logging
import json
import re
from typing import List, Dict, Generator, Tuple, Optional
from pathlib import Path
import requests
from openai import OpenAI

from config import (
    VLLM_SERVER_URL, VLLM_API_KEY, LLM_MODEL_NAME, ENABLE_REASONING,
    MAX_TOKENS, TEMPERATURE, TOP_P, TOP_K, MIN_P, SYSTEM_MESSAGE, TOP_K_RESULTS
)
from services.rag_engine import AdvancedRAGSystem
from core.agent_manager import exaone_agent

logger = logging.getLogger(__name__)

class ExaoneClient:
    """EXAONE ëª¨ë¸ (VLLM ì„œë²„) í†µì‹  í´ë¼ì´ì–¸íŠ¸"""
    def __init__(self, server_url, api_key, model_name="LGAI-EXAONE/EXAONE-4.0-32B-AWQ"):
        self.server_url = server_url
        self.api_key = api_key
        self.model_name = model_name
        
        # OpenAI SDK í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        self.client = OpenAI(api_key=api_key, base_url=server_url, timeout=180.0)

    def generate_response(self, messages: List[Dict], enable_thinking: bool = False,
                          temperature: float = TEMPERATURE, max_tokens: int = MAX_TOKENS,
                          top_p: float = TOP_P, stream: bool = True):
        """
        LLM ìƒì„± ìš”ì²­ ì „ì†¡
        - extra_bodyë¥¼ í†µí•´ 'enable_thinking' íŒŒë¼ë¯¸í„° ì „ë‹¬ (Reasoning ëª¨ë“œ ì œì–´)
        """
        try:
            response = self.client.chat.completions.create(
                model = self.model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                stream=stream,
                extra_body={
                    "chat_template_kwargs": {"enable_thinking": enable_thinking},
                },
            )
            return response
        except Exception as e:
            logger.error(f"EXAONE API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None


class ChatManager:
    """
    [í•µì‹¬ í´ë˜ìŠ¤] ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ì
    - ì—­í• : ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹ , ì ì ˆí•œ ì²˜ë¦¬ê¸°(Agent, RAG) í˜¸ì¶œ, ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½, ì‘ë‹µ ìƒì„±
    """

    def __init__(self):
        self.rag_system = AdvancedRAGSystem()
        self.exaone_client = ExaoneClient(
            server_url=VLLM_SERVER_URL,
            api_key=VLLM_API_KEY,
            model_name = LLM_MODEL_NAME,
        )
        # ì¶”ë¡  ëª¨ë“œ ì—¬ë¶€ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        self.reasoning_temperature = 0.6 # ì°½ì˜ì  ì‚¬ê³  í•„ìš”
        self.non_reasoning_temperature = 0.4 # ì •í™•í•œ ì •ë³´ ì „ë‹¬ ì¤‘ìš”

    def initialize_system(self, pdf_paths: List[str]):
        """ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ RAG ì¸ë±ìŠ¤ ë¹Œë“œ"""
        logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        self.rag_system.build_index(pdf_paths)
        logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _prepare_messages(self, user_message: str, history: List[List[str]], rag_context: str = None, agent_context: str = None, uploaded_context: str = None):
        """
        [Prompt Engineering] LLMì— ë³´ë‚¼ ìµœì¢… ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½
        - System Prompt + ì‹œê°„ ì •ë³´ + ì—…ë¡œë“œ íŒŒì¼ ë‚´ìš© + Agent ê²°ê³¼ + RAG ê²€ìƒ‰ ê²°ê³¼ + ëŒ€í™” íˆìŠ¤í† ë¦¬ + í˜„ì¬ ì§ˆë¬¸
        """
        messages = []

        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ì‹œê°„ ì •ë³´ ì£¼ì…
        from datetime import datetime
        now_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %A %Hì‹œ %Më¶„")
        system_content = f"í˜„ì¬ ì‹œê°: {now_str}\n\n{SYSTEM_MESSAGE}"

        # 2. [íŒŒì¼ ë‚´ìš©] ì—…ë¡œë“œëœ ë¬¸ì„œ ë‚´ìš©ì„ ë³„ë„ ì„¹ì…˜ìœ¼ë¡œ ì£¼ì…
        if uploaded_context:
            truncated_context = uploaded_context[:15000] # í† í° ì œí•œ ê³ ë ¤í•˜ì—¬ ê¸¸ì´ ì œí•œ
            if len(uploaded_context) > 15000:
                truncated_context += "\n...(ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìƒëµë¨)..."
            system_content += f"\n\n=== [ì‚¬ìš©ì ì—…ë¡œë“œ íŒŒì¼ ë‚´ìš© (ì°¸ê³  ìë£Œ)] ===\n{truncated_context}\n\nìœ„ íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ê±°ë‚˜ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤."

        # 3. [Agent ê²°ê³¼] íˆ´ ì‹¤í–‰ ê²°ê³¼ ì£¼ì…
        if agent_context:
            system_content += f"\n\n === Agent ì‹¤í–‰ ê²°ê³¼ ===\n{agent_context}\n\nìœ„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        # 4. [RAG ê²°ê³¼] ê²€ìƒ‰ëœ ê·œì • ë¬¸ì„œ ì£¼ì…
        if rag_context:
            system_content += f"\n\n === ì°¸ê³  ë¬¸ì„œ ====\n{rag_context}\n\nìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        messages.append({"role": "system", "content": system_content})

        # 5. ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœê·¼ 10í„´ ìœ ì§€)
        max_history_turns = 10
        if len(history) > max_history_turns:
            history = history[-max_history_turns:]

        for human_msg, ai_msg in history:
            if human_msg: messages.append({"role": "user", "content": human_msg.strip()})
            if ai_msg: messages.append({"role": "assistant", "content": self._clean_ai_message(ai_msg.strip())})

        messages.append({"role": "user", "content": user_message})
        
        # [DEBUG] í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ ê²°ê³¼ íŒŒì¼ ì €ì¥ (PDF ë‚´ìš© í™•ì¸ìš©)
        try:
            debug_path = Path("hs_code/debug_prompt.log")
            with open(debug_path, "w", encoding="utf-8") as f:
                json.dump(messages, f, indent=2, ensure_ascii=False)
            print(f"ğŸ› [DEBUG] ì¡°ë¦½ëœ ì „ì²´ í”„ë¡¬í”„íŠ¸ê°€ '{debug_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ [DEBUG] í”„ë¡¬í”„íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

        return messages

    def _clean_ai_message(self, ai_msg: str) -> str:
        """íˆìŠ¤í† ë¦¬ ì˜¤ì—¼ ë°©ì§€: ì´ì „ ì‘ë‹µì—ì„œ ë¡œê·¸/ë©”íƒ€ë°ì´í„° ì œê±°"""
        # ... (ì •ê·œì‹ ì œê±° ë¡œì§ ìƒëµ) ...
        return ai_msg # ì‹¤ì œë¡œëŠ” ì •ì œ ë¡œì§ ì ìš©ë¨

    def process_message(self, user_message: str, history: List[List[str]], agent_mode: bool = False, 
                    reasoning_mode: bool = True, enable_reasoning: bool = True, enable_rag: bool = True,
                    selected_pdf_path: Optional[str] = None, reset_state: bool = False, uploaded_context: str = "") -> Generator[Tuple[List[List[str]], str], None, None]:
        """
        [Main Loop] ë©”ì‹œì§€ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸
        """
        
        # [TRACE] ì‹œì‘
        print(f"\n{'='*60}")
        print(f"ğŸš€ [TRACE] ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹ : \"{user_message}\"")
        print(f"   - ì„¤ì •: Agent={agent_mode}, RAG={enable_rag}, Reasoning={reasoning_mode}")
        print(f"{'='*60}")

        if reset_state: self._reset_processing_state()
        
        # 1. [Special Flow] ë³´ê³ ì„œ ìƒì„± ìš”ì²­ì¸ì§€ í™•ì¸
        if "ë³´ê³ ì„œ ìƒì„±" in user_message or "ë³´ê³ ì„œ ì‘ì„±" in user_message:
            print("ğŸš¦ [TRACE] ë¼ìš°íŒ… ê²°ì •: >> [ë³´ê³ ì„œ ìƒì„± íŠ¸ë™] (ReportGenerator)")
            logger.info("ğŸ“„ [PROCESS] ë³´ê³ ì„œ ìƒì„± ìš”ì²­ ê°ì§€")
            
            # ... (ê¸°ì¡´ ë³´ê³ ì„œ ìƒì„± ë¡œì§) ...
            # ì—¬ê¸°ì„œëŠ” yield ë¶€ë¶„ë§Œ ê°„ëµíˆ ìœ ì§€í•˜ê³  ì‹¤ì œ ë¡œì§ì€ generator í˜¸ì¶œë¶€ë¡œ ê°€ì •
            # (ì‹¤ì œ ì½”ë“œê°€ ê¸¸ì–´ì„œ ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬ëœ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜ ìƒëµ)
            new_history = history + [[user_message, "ğŸ“„ ë³´ê³ ì„œ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."]]
            yield (new_history, "")
            
            try:
                from core.generator import ReportGenerator
                generator = ReportGenerator()
                target_text = uploaded_context if uploaded_context else user_message
                
                print("   Step 1: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (parse_document)")
                new_history[-1][1] = "ğŸ“„ [1ë‹¨ê³„] ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì¤‘..."
                yield (new_history, "")
                structure = generator.parse_document(target_text)
                
                if structure.get("status") == "incomplete":
                    print(f"   âš ï¸ ì •ë³´ ë¶€ì¡±: {structure.get('missing_fields')}")
                    new_history[-1][1] = f"ğŸ¤” **í™•ì¸ í•„ìš”:** {structure.get('clarification_question')}"
                    yield (new_history, "")
                    return

                if not structure or not structure.get("sections"):
                    print("   âŒ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨")
                    new_history[-1][1] = "âŒ ë¬¸ì„œ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    yield (new_history, "")
                    return

                print("   Step 2: íˆ´ í”Œë˜ë‹ (plan_tools)")
                new_history[-1][1] = f"ğŸ› ï¸ [2ë‹¨ê³„] ë„êµ¬ ì‚¬ìš© ê³„íš ìˆ˜ë¦½ ì¤‘..."
                yield (new_history, "")
                structure = generator.plan_tools(structure)

                print("   Step 3: ì‹¤í–‰ ë° ì‘ì„± (fill_report)")
                new_history[-1][1] = f"âš¡ [3ë‹¨ê³„] ë°ì´í„° ìˆ˜ì§‘ ë° ë³´ê³ ì„œ ì‘ì„± ì¤‘..."
                yield (new_history, "")
                structure = generator.fill_report(structure)
                
                print("   Step 4: DOCX ë‚´ë³´ë‚´ê¸°")
                docx_path = generator.export_to_docx(structure)
                
                final_report = structure.get("full_report_md", "")
                if docx_path:
                    final_report += f"\n\n---\n### ğŸ’¾ [ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ]\nDOCX íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: `{Path(docx_path).name}`"
                
                print("ğŸ [TRACE] ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
                new_history[-1][1] = final_report
                yield (new_history, "")
                return

            except Exception as e:
                print(f"âŒ [TRACE] ë³´ê³ ì„œ ìƒì„± ì¤‘ ì—ëŸ¬: {e}")
                logger.error(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
                new_history[-1][1] = f"âŒ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                yield (new_history, "")
                return
        
        # ê¸°ë³¸ ë³€ìˆ˜ ì´ˆê¸°í™”
        rag_context = ""
        rag_evidence_text = ""  # [FIX] RAG ê·¼ê±° í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        agent_context = None
        new_history = history + [[user_message, ""]]
        yield (new_history, "")

        # 2. [Agent Mode] ì—ì´ì „íŠ¸ ì‹¤í–‰ (íˆ´ ì‚¬ìš©ì´ í•„ìš”í•œ ê²½ìš°)
        if agent_mode and exaone_agent.is_available():
            print("ğŸš¦ [TRACE] ë¼ìš°íŒ… ê²°ì •: >> [Agent íŠ¸ë™] (exaone_agent)")
            logger.info("ğŸ¤– [PROCESS] Agent ëª¨ë“œ í™œì„±í™”")
            new_history[-1][1] = "ğŸ¤– EXAONE Agent ì‹¤í–‰ ì‹œì‘...\n\n"
            yield (new_history, "")

            # Agent ìƒíƒœ ì½œë°± (UI ë¡œê·¸ í‘œì‹œìš©)
            def status_callback(api_name: str, message: str):
                pass 

            exaone_agent.set_status_callback(status_callback)
            agent_results = {"success": False}
            
            try:
                cleaned_history = []
                for h, a in (history or []):
                    if a: cleaned_history.append([h, self._clean_ai_message(a)])
                
                # Agent ì‹¤í–‰ ë° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                for chunk in exaone_agent.run_agent_stream(user_message, cleaned_history, True):
                    ctype = chunk.get("type")
                    # [TRACE] íˆ´ ì‹¤í–‰ ë¡œê·¸
                    if ctype == "tool_executing":
                        print(f"   ğŸ› ï¸ [TOOL] ì‹¤í–‰: {chunk.get('tool_name')}")
                    elif ctype == "tool_complete":
                        print(f"   âœ… [TOOL] ì™„ë£Œ: {chunk.get('tool_name')}")
                    
                    # UI ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ (Clean & Simple: ì±„íŒ…ì°½ì—ëŠ” ë‹µë³€ë§Œ í‘œì‹œ)
                    if ctype == "content":
                        new_history[-1][1] += chunk.get("content", "")
                        yield (new_history, "")
                    
                    elif ctype == "agent_error":
                        agent_results = {"success": False, "error": chunk.get("error", "Error")}
                        print(f"   âŒ [AGENT] ì—ëŸ¬: {agent_results['error']}")
                        new_history[-1][1] += f"\nâŒ ì˜¤ë¥˜: {agent_results['error']}\n"
                        break

                    elif ctype == "agent_complete":
                        agent_results = {"success": True, "content": chunk.get("content", ""), "tool_calls": chunk.get("tool_calls", [])}
                        print(f"   ğŸ¤– [AGENT] ì™„ë£Œ. (ë„êµ¬ í˜¸ì¶œ: {len(agent_results['tool_calls'])}ê°œ)")
                        break
                    
                    # ë‚˜ë¨¸ì§€ (thinking, tool logs)ëŠ” ì±„íŒ…ì°½ì— ì¶œë ¥í•˜ì§€ ì•ŠìŒ (pass)
                    else:
                        pass
            except Exception as e:
                logger.error(f"Agent stream error: {e}")
                agent_results = {"success": False, "error": str(e)}

            # Agent ê²°ê³¼ë¥¼ LLM ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê¸° ìœ„í•´ í¬ë§·íŒ…
            if agent_results.get("success"):
                agent_context = f"\n\n=== Agent ì‹¤í–‰ ê²°ê³¼ ===\n"
                for tool in agent_results.get("tool_calls", []):
                    agent_context += f"[{tool.get('name')}] {tool.get('result')}\n"
                if agent_results.get("content"):
                    agent_context += f"Agent ì‘ë‹µ: {agent_results.get('content')}\n"

        # 3. [RAG Mode] ê·œì • ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­ ë° ì˜ˆì™¸ ì²˜ë¦¬)
        # ë‹¨ìˆœ ë‚ ì”¨ ì§ˆë¬¸ì— ë§¤ë‰´ì–¼ì„ ê²€ìƒ‰í•˜ëŠ” ê³¼ì‰ ê°œì… ë°©ì§€
        rag_keywords = ["ê·œì •", "ì§€ì¹¨", "ë§¤ë‰´ì–¼", "ì ˆì°¨", "ê¸°ì¤€", "í–‰ë™ìš”ë ¹", "ìœ„ê¸°ê´€ë¦¬", "ëŒ€ì‘", "ì¡°ì¹˜"]
        weather_keywords = ["ë‚ ì”¨", "ë¯¸ì„¸ë¨¼ì§€", "ê¸°ì˜¨", "ì˜¨ë„", "ìŠµë„", "ê°•ìˆ˜", "ì˜ˆë³´"]
        
        has_rag_kw = any(k in user_message for k in rag_keywords)
        is_weather_query = any(k in user_message for k in weather_keywords)
        
        # ê·œì • í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜, ë‚ ì”¨ ì§ˆë¬¸ì´ ì•„ë‹ ë•Œë§Œ RAG ìˆ˜í–‰ (ë‹¨, ì‚¬ìš©ìê°€ enable_ragë¥¼ ì¼°ì„ ë•Œ)
        need_rag = has_rag_kw or (not is_weather_query and "íŠ¹ë³´" in user_message) or (not is_weather_query and "ì§€ì§„" in user_message)

        # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ RAGë¥¼ ì›í•˜ë©´(ê·œì • í‚¤ì›Œë“œ í¬í•¨) ë¬´ì¡°ê±´ ì‹¤í–‰
        if has_rag_kw: need_rag = True

        if enable_rag and need_rag:
            print(f"ğŸš¦ [TRACE] ë¼ìš°íŒ… ê²°ì •: >> [RAG íŠ¸ë™] (í‚¤ì›Œë“œ ë§¤ì¹­: {need_rag})")
            logger.info("ğŸ” [PROCESS] RAG ê²€ìƒ‰ ìˆ˜í–‰")
            s_res = self.rag_system.search(user_message, selected_pdf_path, top_k=TOP_K_RESULTS)
            if s_res:
                print(f"   ğŸ“š [RAG] ê²€ìƒ‰ ê²°ê³¼: {len(s_res)}ê±´")
                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜
                rag_context = "=== [í–‰ì •ì•ˆì „ë¶€/ê¸°ìƒì²­ ê³µì‹ ìœ„ê¸°ê´€ë¦¬ ë§¤ë‰´ì–¼ (SOP)] ===\n" + \
                              "\n".join([f"- {r['text']}" for r in s_res])
                
                # [FIX] ê·¼ê±° ìë£Œ í…ìŠ¤íŠ¸ ìƒì„±
                rag_evidence_text = "\n\n---\n**[ì°¸ê³  ìë£Œ (ê·œì •/ë§¤ë‰´ì–¼)]**\n"
                for r in s_res:
                    pdf_name = r.get('pdf_name', 'Unknown')
                    page_num = r.get('metadata', {}).get('page_num', '?')
                    score = r.get('similarity_score', 0.0)
                    rag_evidence_text += f"- ğŸ“„ **{pdf_name}** ({page_num}ìª½) (ê´€ë ¨ë„: {score:.2f})\n"
            else:
                print("   ğŸ“š [RAG] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        # 4. [Final Generation] ìµœì¢… ì‘ë‹µ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
        print("ğŸ“¨ [TRACE] ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ ë° LLM í˜¸ì¶œ")
        if uploaded_context: print(f"   ğŸ“ [CTX] ì—…ë¡œë“œ íŒŒì¼ í¬í•¨ ({len(uploaded_context)}ì)")
        if rag_context: print(f"   ğŸ“š [CTX] RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ({len(rag_context)}ì)")
        if agent_context: print(f"   ğŸ¤– [CTX] Agent ê²°ê³¼ í¬í•¨ ({len(agent_context)}ì)")
        
        messages = self._prepare_messages(user_message, history, rag_context, agent_context, uploaded_context)
        
        response = self.exaone_client.generate_response(
            messages=messages,
            stream=True,
            enable_thinking=reasoning_mode,
            temperature=self.reasoning_temperature if reasoning_mode else self.non_reasoning_temperature
        )
        
        # [Error Handling] API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë°©ì–´ ì½”ë“œ
        if response is None:
            print("âŒ [TRACE] LLM API í˜¸ì¶œ ì‹¤íŒ¨ (None)")
            logger.error("âŒ EXAONE API ì‘ë‹µì´ Noneì…ë‹ˆë‹¤. (ì„œë²„ ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ)")
            new_history[-1][1] = "âŒ ì„œë²„ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (API ì‘ë‹µ ì—†ìŒ)\nê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            yield (new_history, "")
            return

        # ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì²˜ë¦¬ (Thinking íŒŒíŠ¸ì™€ Content íŒŒíŠ¸ ë¶„ë¦¬)
        if reasoning_mode:
            for chunk in self._stream_response_with_reasoning(response):
                # UI ì—…ë°ì´íŠ¸ (Thinking -> Content ìˆœì„œ)
                if chunk['type'] == 'thinking_chunk':
                    new_history[-1][1] += chunk['content']
                elif chunk['type'] == 'content':
                    new_history[-1][1] += chunk['content']
                yield (new_history, "")
        else:
            # ì¼ë°˜ ìŠ¤íŠ¸ë¦¬ë°
            for chunk in self._stream_response_simple(response):
                new_history[-1][1] += chunk['content']
                yield (new_history, "")
        
        # [FIX] RAG ê·¼ê±° ìë£Œ ì²¨ë¶€
        if rag_evidence_text:
            new_history[-1][1] += rag_evidence_text
            yield (new_history, "")
        
        print("ğŸ [TRACE] ì²˜ë¦¬ ì™„ë£Œ\n")

    def _stream_response_with_reasoning(self, response) -> Generator[dict, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ë©° <think> íƒœê·¸ë¥¼ ê°ì§€í•˜ì—¬ ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ì„ êµ¬ë¶„í•´ yield í•©ë‹ˆë‹¤.
        UIì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ê³  ê³¼ì •ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
        """
        thinking_started = False
        content_started = False
        reasoning_content_complete = ""
        content_buffer = ""
        reasoning_seen = False
        
        try:
            for chunk in response:
                if hasattr(chunk, 'choices') and chunk.choices:
                    choice = chunk.choices[0]
                    delta = choice.delta if hasattr(choice, 'delta') else {}
                    
                    # VLLM API ìŠ¤í™ì— ë”°ë¥¸ reasoning_content í•„ë“œ í™•ì¸
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        reasoning_chunk = delta.reasoning_content
                        reasoning_content_complete += reasoning_chunk
                        reasoning_seen = True
                        if not thinking_started:
                            yield {'type': 'thinking_start'}
                            thinking_started = True
                        yield {'type': 'thinking_chunk', 'content': reasoning_chunk}
                    
                    # ì¼ë°˜ content í•„ë“œ í™•ì¸
                    if hasattr(delta, 'content') and delta.content:
                        content_chunk = delta.content
                        content_buffer += content_chunk
                        
                        # ì¶”ë¡ ì´ ìˆì—ˆê³  ì´ì œ ë§‰ ì»¨í…ì¸ ê°€ ì‹œì‘ë˜ëŠ” ê²½ìš°
                        if reasoning_seen and not content_started and thinking_started:
                            yield {'type': 'thinking_end'}
                            content_started = True
                            content_buffer = ""
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        elif reasoning_seen and content_started:
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        
                        if content_started:
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        
                        # <think> íƒœê·¸ë¥¼ ì§ì ‘ íŒŒì‹±í•˜ì—¬ ì²˜ë¦¬ (API í•„ë“œê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ ë‚´ í¬í•¨ëœ ê²½ìš°)
                        if '</think>' in content_buffer:
                            parts = content_buffer.split('</think>', 1)
                            thinking_part = parts[0]
                            answer_part = parts[1] if len(parts) > 1 else ""
                            
                            if not thinking_started:
                                yield {'type': 'thinking_start'}
                                thinking_started = True
                                if thinking_part: yield {'type': 'thinking_chunk', 'content': thinking_part}
                            
                            yield {'type': 'thinking_end'}
                            content_started = True
                            if answer_part: yield {'type': 'content', 'content': answer_part}
                            content_buffer = ""
                        else:
                            # </think>ê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ëŠ” ì¶”ë¡  ê³¼ì •ìœ¼ë¡œ ê°„ì£¼
                            if not thinking_started:
                                yield {'type': 'thinking_start'}
                                thinking_started = True
                            yield {'type': 'thinking_chunk', 'content': content_chunk}
            
            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ í›„ ì”ì—¬ ë²„í¼ ì²˜ë¦¬
            if thinking_started and not content_started:
                yield {'type': 'thinking_end'}
                if reasoning_content_complete.strip():
                    cleaned_content = reasoning_content_complete
                    if '</think>' in cleaned_content:
                        cleaned_content = cleaned_content.split('</think>', 1)[1]
                    if cleaned_content.strip():
                        yield {'type': 'content', 'content': cleaned_content.strip()}
                elif content_buffer.strip():
                    yield {'type': 'content', 'content': content_buffer}
                            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {'type': 'error', 'content': f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
    
    def _stream_response_simple(self, response) -> Generator[dict, None, None]:
        """
        ì¶”ë¡  ëª¨ë“œê°€ ì•„ë‹ ë•Œì˜ ë‹¨ìˆœ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì…ë‹ˆë‹¤.
        [ê°œì„ ] ë¶ˆí•„ìš”í•œ ë²„í¼ë§ì„ ìµœì†Œí™”í•˜ì—¬ ì‘ë‹µ ì†ë„ í–¥ìƒ
        """
        think_tag_found = False
        content_buffer = ""
        try:
            for chunk in response:
                if hasattr(chunk, 'choices') and chunk.choices:
                    delta = chunk.choices[0].delta
                    
                    # 1. reasoning_contentê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì²˜ë¦¬ (ë“œë¬¸ ê²½ìš°)
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        # ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” ì¶”ë¡  ë‚´ìš©ì„ ë³´ì—¬ì¤„ì§€ ë§ì§€ ì •ì±…ì— ë”°ë¼ ë‹¤ë¦„.
                        # ì—¬ê¸°ì„œëŠ” content ì·¨ê¸‰í•˜ì—¬ ì¶œë ¥
                        yield {'type': 'content', 'content': delta.reasoning_content}
                    
                    # 2. content ì²˜ë¦¬
                    elif hasattr(delta, 'content') and delta.content:
                        if think_tag_found: 
                            # ì´ë¯¸ <think> íƒœê·¸ ì²˜ë¦¬ê°€ ëë‚¬ë‹¤ë©´ ë¬´ì¡°ê±´ ì¦‰ì‹œ ì¶œë ¥
                            yield {'type': 'content', 'content': delta.content}
                        else:
                            content_buffer += delta.content
                            
                            # <think> íƒœê·¸ ê°ì§€ ë¡œì§
                            if '</think>' in content_buffer:
                                think_tag_found = True
                                parts = content_buffer.split('</think>', 1)
                                if len(parts) > 1 and parts[1]: 
                                    yield {'type': 'content', 'content': parts[1]}
                                content_buffer = ""
                            elif '<think>' in content_buffer:
                                # íƒœê·¸ ì‹œì‘ë¨, ë‹«í ë•Œê¹Œì§€ ëŒ€ê¸° (ë²„í¼ë§ ìœ ì§€)
                                pass
                            elif '<' in content_buffer:
                                # íƒœê·¸ì˜ ì‹œì‘ì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì ì‹œ ëŒ€ê¸° (ë‹¨, ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ë°©ì¶œ)
                                if len(content_buffer) > 50: 
                                    yield {'type': 'content', 'content': content_buffer}
                                    content_buffer = ""
                            else:
                                # íƒœê·¸ì™€ ë¬´ê´€í•œ ë‚´ìš©ì´ë©´ ì¦‰ì‹œ ë°©ì¶œ! (ì†ë„ ê°œì„  í•µì‹¬)
                                yield {'type': 'content', 'content': content_buffer}
                                content_buffer = ""
                                
            if content_buffer and not think_tag_found:
                yield {'type': 'content', 'content': content_buffer}
        except Exception as e:
            logger.error(f"Non-reasoning ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {'type': 'error', 'content': f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

    def get_pdf_list(self) -> List[Tuple[str, str]]:
        """RAG ì‹œìŠ¤í…œì— ë¡œë“œëœ PDF ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        # rag_systemì— í•´ë‹¹ ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
        if hasattr(self.rag_system, 'get_pdf_list'):
            return self.rag_system.get_pdf_list()
        return []

