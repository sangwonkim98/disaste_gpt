"""
Query Pipeline ë…¸ë“œ êµ¬í˜„

1. analyzer_node: ì‚¬ìš©ì ì…ë ¥ ë¶„ì„ â†’ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
2. tool_executor_node: API ë„êµ¬ ì‹¤í–‰
3. rag_executor_node: RAG ê²€ìƒ‰ ì‹¤í–‰
4. pdf_executor_node: PDF ë¶„ì„ ì‹¤í–‰
5. synthesizer_node: ê²°ê³¼ ì¢…í•© â†’ ìµœì¢… ì‘ë‹µ ìƒì„±
"""

import logging
import json
import re
from typing import Dict, Any, List, Optional
from langchain_core.messages import HumanMessage, AIMessage

from config import VLLM_SERVER_URL, VLLM_API_KEY, LLM_MODEL_NAME, SYSTEM_MESSAGE
from graph.query_state import (
    QueryState, ExecutionPlan, ExecutionResult,
    TOOL_METADATA, TOOL_DEPENDENCIES, KEYWORD_TOOL_MAP, RAG_KEYWORDS
)

logger = logging.getLogger(__name__)

# ===== ì§€ì—° ë¡œë”© ì‹±ê¸€í†¤ =====
_llm_client = None
_agent_tools = None
_rag_system = None


def _get_llm_client():
    global _llm_client
    if _llm_client is None:
        from core.llm_client import ExaoneClient
        _llm_client = ExaoneClient(
            server_url=VLLM_SERVER_URL,
            api_key=VLLM_API_KEY,
            model_name=LLM_MODEL_NAME
        )
    return _llm_client


def _get_agent_tools():
    global _agent_tools
    if _agent_tools is None:
        from services.agent_tools import exaone_agent_tools
        _agent_tools = exaone_agent_tools
    return _agent_tools


def _get_rag_system():
    """RAG ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ë°˜í™˜ (PDF ì¸ë±ìŠ¤ ìë™ ë¹Œë“œ)"""
    global _rag_system
    if _rag_system is None:
        from services.rag_engine import AdvancedRAGSystem
        from config import PDF_FILES
        from pathlib import Path

        _rag_system = AdvancedRAGSystem()

        # PDF íŒŒì¼ì´ ìˆìœ¼ë©´ ì¸ë±ìŠ¤ ë¹Œë“œ
        existing_pdfs = [pdf for pdf in PDF_FILES if Path(pdf).exists()]
        if existing_pdfs:
            logger.info(f"ğŸ“š RAG ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘: {len(existing_pdfs)}ê°œ PDF")
            _rag_system.build_index(existing_pdfs)
            logger.info(f"âœ… RAG ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ PDF íŒŒì¼ ì—†ìŒ - RAG ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤")

    return _rag_system


def _debug_log(state: QueryState, message: str, data: Any = None):
    """ë””ë²„ê·¸ ëª¨ë“œì¼ ë•Œë§Œ ìƒì„¸ ë¡œê¹…"""
    if state.get("debug_mode", False):
        if data:
            logger.info(f"ğŸ” [DEBUG] {message}: {json.dumps(data, ensure_ascii=False, default=str)[:500]}")
        else:
            logger.info(f"ğŸ” [DEBUG] {message}")


# =============================================================================
# 1. ANALYZER NODE
# =============================================================================

ANALYZER_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì¬ë‚œëŒ€ì‘ AI ì‹œìŠ¤í…œì˜ 'Query Analyzer'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ì •ë³´ ì†ŒìŠ¤ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

## ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ ì†ŒìŠ¤

### 1. ì™¸ë¶€ API ë„êµ¬ (tools)
ì‹¤ì‹œê°„ ë°ì´í„°ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©:
- serpapi_web_search: ìµœì‹  ë‰´ìŠ¤, ì‚¬ê±´ ê²€ìƒ‰
- kma_get_ultra_srt_ncst: í˜„ì¬ ì‹¤ì‹œê°„ ë‚ ì”¨
- kma_get_vilage_fcst: ë‹¨ê¸°ì˜ˆë³´ (3ì¼)
- kma_get_mid_land_fcst: ì¤‘ê¸°ì˜ˆë³´ (3~10ì¼)
- kma_get_mid_ta: ì¤‘ê¸°ê¸°ì˜¨ì˜ˆë³´
- kma_get_wthr_wrn_msg: ê¸°ìƒíŠ¹ë³´ (íƒœí’, í˜¸ìš° ë“±)
- kma_get_pwn_status: ì˜ˆë¹„íŠ¹ë³´
- kma_get_eqk_msg_list: ì§€ì§„ ëª©ë¡
- kma_get_eqk_msg: ì§€ì§„ ìƒì„¸

### 2. ë§¤ë‰´ì–¼ ê²€ìƒ‰ (rag)
ê³µë¬´ì› ëŒ€ì‘ ì ˆì°¨, í–‰ë™ìš”ë ¹ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©:
- í’ìˆ˜í•´ ëŒ€ì‘ ë§¤ë‰´ì–¼
- ì¬ë‚œ í˜„ì¥ì¡°ì¹˜ í–‰ë™ë§¤ë‰´ì–¼
- ìœ„ê¸°ê´€ë¦¬ í‘œì¤€ë§¤ë‰´ì–¼

### 3. ì‚¬ìš©ì PDF (pdf)
ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë¬¸ì„œ ë¶„ì„ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©

## ì¶œë ¥ í˜•ì‹ (JSON)
```json
{
  "need_tools": true/false,
  "need_rag": true/false,
  "need_pdf": true/false,
  "tool_list": ["tool_name1", "tool_name2"],
  "tool_params": {
    "tool_name1": {"location": "ì„œìš¸"},
    "tool_name2": {}
  },
  "tool_reasoning": "ë„êµ¬ ì„ íƒ ì´ìœ ",
  "rag_query": "ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì¿¼ë¦¬ (ì¬êµ¬ì„±)",
  "rag_reasoning": "RAG í•„ìš” ì´ìœ ",
  "pdf_task": "extract_info | use_as_template | null",
  "confidence": 0.9
}
```

## ì¤‘ìš” ê·œì¹™
1. ì‹¤ì‹œê°„ ì •ë³´(ë‚ ì”¨, ë‰´ìŠ¤, ì§€ì§„)ê°€ í•„ìš”í•˜ë©´ ë°˜ë“œì‹œ tools ì‚¬ìš©
2. ëŒ€ì‘ ì ˆì°¨, í–‰ë™ìš”ë ¹ ì§ˆë¬¸ì€ rag ì‚¬ìš©
3. ë³µí•© ì§ˆë¬¸ì€ tools + rag ëª¨ë‘ true
4. ë‹¨ìˆœ ì¸ì‚¬ë‚˜ ì¼ë°˜ ëŒ€í™”ëŠ” ëª¨ë‘ false

ì˜¤ì§ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""


def _extract_location(user_input: str) -> Optional[str]:
    """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì§€ì—­ëª… ì¶”ì¶œ"""
    # ìì£¼ ì“°ì´ëŠ” ì§€ì—­ëª… ëª©ë¡
    locations = [
        "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…",
        "ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼",
        "ìˆ˜ì›", "ì„±ë‚¨", "ìš©ì¸", "ê³ ì–‘", "ì°½ì›", "ì²­ì£¼", "ì²œì•ˆ", "ì „ì£¼",
        "í¬í•­", "ê¹€í•´", "ì•ˆì‚°", "ì•ˆì–‘", "ë‚¨ì–‘ì£¼", "í™”ì„±", "í‰íƒ"
    ]
    for loc in locations:
        if loc in user_input:
            return loc
    return None


def _rule_based_analysis(user_input: str, has_pdf: bool) -> ExecutionPlan:
    """ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (LLM ì‹¤íŒ¨ ì‹œ fallback)"""
    user_lower = user_input.lower()

    need_tools = False
    need_rag = False
    need_pdf = has_pdf
    tool_list = []
    tool_params = {}

    location = _extract_location(user_input)

    # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë„êµ¬ ì„ íƒ
    for keyword, tools in KEYWORD_TOOL_MAP.items():
        if keyword in user_lower:
            need_tools = True
            for tool in tools:
                if tool not in tool_list:
                    tool_list.append(tool)
                    if TOOL_METADATA.get(tool, {}).get("requires_location") and location:
                        tool_params[tool] = {"location": location}
                    else:
                        tool_params[tool] = {}

    # RAG í‚¤ì›Œë“œ ë§¤ì¹­
    for keyword in RAG_KEYWORDS:
        if keyword in user_lower:
            need_rag = True
            break

    # ë„êµ¬ ì˜ì¡´ì„± ì²´ì´ë‹
    for tool in list(tool_list):
        if tool in TOOL_DEPENDENCIES:
            dep = TOOL_DEPENDENCIES[tool]
            if dep.get("auto_chain"):
                for suggested in dep.get("suggests", []):
                    if suggested not in tool_list:
                        tool_list.append(suggested)
                        if TOOL_METADATA.get(suggested, {}).get("requires_location") and location:
                            tool_params[suggested] = {"location": location}
                        else:
                            tool_params[suggested] = {}

    return ExecutionPlan(
        need_tools=need_tools,
        need_rag=need_rag,
        need_pdf=need_pdf,
        tool_list=tool_list,
        tool_params=tool_params,
        tool_reasoning="ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (í‚¤ì›Œë“œ ë§¤ì¹­)",
        rag_query=user_input if need_rag else None,
        rag_reasoning="ëŒ€ì‘ ì ˆì°¨/ë§¤ë‰´ì–¼ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€" if need_rag else None,
        pdf_task="extract_info" if need_pdf else None,
        confidence=0.7
    )


def analyzer_node(state: QueryState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.

    1. LLMìœ¼ë¡œ ì˜ë„ ë¶„ì„ ì‹œë„
    2. ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ fallback
    3. ë„êµ¬ ì˜ì¡´ì„± ì²´ì´ë‹ ì ìš©
    """
    logger.info("ğŸ” [ANALYZER] ì‹¤í–‰ ì‹œì‘...")

    user_input = state.get("user_input", "")
    has_pdf = bool(state.get("uploaded_pdf_content"))
    debug_mode = state.get("debug_mode", False)

    _debug_log(state, "ì…ë ¥", {"user_input": user_input, "has_pdf": has_pdf})

    # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
    if not user_input.strip():
        return {
            "execution_plan": None,
            "next_node": "end",
            "final_response": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        }

    # LLM ê¸°ë°˜ ë¶„ì„ ì‹œë„
    try:
        client = _get_llm_client()

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = f"ì‚¬ìš©ì ì§ˆë¬¸: {user_input}"
        if has_pdf:
            context += "\n[ì‚¬ìš©ìê°€ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•¨]"

        messages = [
            {"role": "system", "content": ANALYZER_SYSTEM_PROMPT},
            {"role": "user", "content": context}
        ]

        response = client.generate_response(
            messages=messages,
            temperature=0.0,
            enable_thinking=False,
            stream=False
        )

        if response and response.choices:
            content = response.choices[0].message.content.strip()

            # JSON ì¶”ì¶œ
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content)

            # ExecutionPlan êµ¬ì„±
            plan = ExecutionPlan(
                need_tools=result.get("need_tools", False),
                need_rag=result.get("need_rag", False),
                need_pdf=result.get("need_pdf", has_pdf),
                tool_list=result.get("tool_list", []),
                tool_params=result.get("tool_params", {}),
                tool_reasoning=result.get("tool_reasoning", ""),
                rag_query=result.get("rag_query"),
                rag_reasoning=result.get("rag_reasoning"),
                pdf_task=result.get("pdf_task"),
                confidence=result.get("confidence", 0.8)
            )

            # ì§€ì—­ íŒŒë¼ë¯¸í„° ë³´ì™„
            location = _extract_location(user_input)
            if location:
                for tool in plan["tool_list"]:
                    if TOOL_METADATA.get(tool, {}).get("requires_location"):
                        if tool not in plan["tool_params"]:
                            plan["tool_params"][tool] = {}
                        if "location" not in plan["tool_params"][tool]:
                            plan["tool_params"][tool]["location"] = location

            # ë„êµ¬ ì˜ì¡´ì„± ì²´ì´ë‹
            for tool in list(plan["tool_list"]):
                if tool in TOOL_DEPENDENCIES:
                    dep = TOOL_DEPENDENCIES[tool]
                    if dep.get("auto_chain"):
                        for suggested in dep.get("suggests", []):
                            if suggested not in plan["tool_list"]:
                                plan["tool_list"].append(suggested)
                                if TOOL_METADATA.get(suggested, {}).get("requires_location") and location:
                                    plan["tool_params"][suggested] = {"location": location}

            logger.info(f"âœ… [ANALYZER] LLM ë¶„ì„ ì™„ë£Œ: tools={plan['need_tools']}, rag={plan['need_rag']}, pdf={plan['need_pdf']}")
            _debug_log(state, "ì‹¤í–‰ ê³„íš", plan)

            # ë‹¤ìŒ ë…¸ë“œ ê²°ì •
            if plan["need_tools"] or plan["need_rag"] or plan["need_pdf"]:
                next_node = "executor"
            else:
                next_node = "direct_response"

            return {
                "execution_plan": plan,
                "next_node": next_node
            }

    except Exception as e:
        logger.warning(f"âš ï¸ [ANALYZER] LLM ë¶„ì„ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜: {e}")

    # Fallback: ê·œì¹™ ê¸°ë°˜ ë¶„ì„
    plan = _rule_based_analysis(user_input, has_pdf)
    logger.info(f"ğŸ“‹ [ANALYZER] ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ: tools={plan['need_tools']}, rag={plan['need_rag']}")

    if plan["need_tools"] or plan["need_rag"] or plan["need_pdf"]:
        next_node = "executor"
    else:
        next_node = "direct_response"

    return {
        "execution_plan": plan,
        "next_node": next_node
    }


# =============================================================================
# 2. TOOL EXECUTOR NODE
# =============================================================================

def tool_executor_node(state: QueryState) -> Dict[str, Any]:
    """
    ì‹¤í–‰ ê³„íšì— ë”°ë¼ API ë„êµ¬ë“¤ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ”§ [TOOL_EXECUTOR] ì‹¤í–‰ ì‹œì‘...")

    plan = state.get("execution_plan")
    if not plan or not plan.get("need_tools"):
        return {}

    tool_list = plan.get("tool_list", [])
    tool_params = plan.get("tool_params", {})

    _debug_log(state, "ì‹¤í–‰í•  ë„êµ¬", {"tools": tool_list, "params": tool_params})

    agent_tools = _get_agent_tools()
    results = {}

    for tool_name in tool_list:
        try:
            params = tool_params.get(tool_name, {})
            logger.info(f"  ğŸ› ï¸ ì‹¤í–‰: {tool_name}({params})")

            result = agent_tools.execute_tool(tool_name, params)
            results[tool_name] = json.loads(result) if isinstance(result, str) else result

            logger.info(f"  âœ… ì™„ë£Œ: {tool_name}")

        except Exception as e:
            logger.error(f"  âŒ ì‹¤íŒ¨: {tool_name} - {e}")
            results[tool_name] = {"error": str(e)}

    # í˜„ì¬ execution_result ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì´ˆê¸°í™”)
    current_result = state.get("execution_result") or {}
    current_result["tool_results"] = results

    logger.info(f"âœ… [TOOL_EXECUTOR] {len(results)}ê°œ ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ")

    return {
        "execution_result": current_result
    }


# =============================================================================
# 3. RAG EXECUTOR NODE
# =============================================================================

def rag_executor_node(state: QueryState) -> Dict[str, Any]:
    """
    ë§¤ë‰´ì–¼ RAG ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ“š [RAG_EXECUTOR] ì‹¤í–‰ ì‹œì‘...")

    plan = state.get("execution_plan")
    if not plan or not plan.get("need_rag"):
        return {}

    user_input = state.get("user_input", "")
    rag_query = plan.get("rag_query") or user_input
    selected_manual = state.get("selected_manual")

    _debug_log(state, "RAG ê²€ìƒ‰", {"query": rag_query, "manual": selected_manual})

    try:
        rag = _get_rag_system()
        results = rag.search(rag_query, selected_manual, top_k=5)

        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        if results:
            for i, doc in enumerate(results):
                formatted_results.append({
                    "rank": i + 1,
                    "content": doc.page_content[:500] if hasattr(doc, 'page_content') else str(doc)[:500],
                    "source": doc.metadata.get("source", "unknown") if hasattr(doc, 'metadata') else "unknown"
                })

        logger.info(f"âœ… [RAG_EXECUTOR] {len(formatted_results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")

        # í˜„ì¬ execution_result ê°€ì ¸ì˜¤ê¸°
        current_result = state.get("execution_result") or {}
        current_result["rag_results"] = formatted_results

        return {
            "execution_result": current_result
        }

    except Exception as e:
        logger.error(f"âŒ [RAG_EXECUTOR] ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        current_result = state.get("execution_result") or {}
        current_result["rag_results"] = [{"error": str(e)}]
        return {
            "execution_result": current_result
        }


# =============================================================================
# 4. PDF EXECUTOR NODE
# =============================================================================

def pdf_executor_node(state: QueryState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì—…ë¡œë“œ PDFë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ“„ [PDF_EXECUTOR] ì‹¤í–‰ ì‹œì‘...")

    plan = state.get("execution_plan")
    if not plan or not plan.get("need_pdf"):
        return {}

    pdf_content = state.get("uploaded_pdf_content")
    if not pdf_content:
        return {}

    pdf_task = plan.get("pdf_task", "extract_info")

    _debug_log(state, "PDF ë¶„ì„", {"task": pdf_task, "content_len": len(pdf_content)})

    # í˜„ì¬ëŠ” ë‹¨ìˆœíˆ ë‚´ìš© ì „ë‹¬ (ì¶”í›„ ê³ ë„í™”)
    current_result = state.get("execution_result") or {}
    current_result["pdf_results"] = {
        "task": pdf_task,
        "content_preview": pdf_content[:1000] if pdf_content else "",
        "full_content": pdf_content
    }

    logger.info(f"âœ… [PDF_EXECUTOR] PDF ë¶„ì„ ì™„ë£Œ (task: {pdf_task})")

    return {
        "execution_result": current_result
    }


# =============================================================================
# 5. SYNTHESIZER NODE
# =============================================================================

SYNTHESIZER_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì¬ë‚œëŒ€ì‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ìœ ìš©í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

## ë‹µë³€ ì›ì¹™
1. ì •í™•ì„±: ì œê³µëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì‹¤ë§Œ ì „ë‹¬
2. ëª…í™•ì„±: í•µì‹¬ ì •ë³´ë¥¼ ë¨¼ì €, ë¶€ê°€ ì •ë³´ëŠ” ë’¤ì—
3. ì‹¤ìš©ì„±: ê³µë¬´ì›ì´ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ
4. ì¶œì²˜ ëª…ì‹œ: ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ (API ê²°ê³¼, ë§¤ë‰´ì–¼ ë“±)

## í¬ë§·
- ë‚ ì”¨ ì •ë³´: ê¸°ì˜¨, ê°•ìˆ˜, ë°”ëŒ ë“± í•µì‹¬ ìˆ˜ì¹˜ í¬í•¨
- íŠ¹ë³´ ì •ë³´: ë°œíš¨ ì¤‘ì¸ íŠ¹ë³´ì™€ ì£¼ì˜ì‚¬í•­
- ë§¤ë‰´ì–¼ ì •ë³´: ë‹¨ê³„ë³„ ì¡°ì¹˜ì‚¬í•­, ì¶œì²˜ í˜ì´ì§€
- ë³µí•© ì •ë³´: í˜„í™© â†’ ëŒ€ì‘ë°©ì•ˆ ìˆœì„œë¡œ ì •ë¦¬"""


def synthesizer_node(state: QueryState) -> Dict[str, Any]:
    """
    ìˆ˜ì§‘ëœ ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ“ [SYNTHESIZER] ì‹¤í–‰ ì‹œì‘...")

    user_input = state.get("user_input", "")
    plan = state.get("execution_plan")
    result = state.get("execution_result") or {}
    messages = state.get("messages", [])
    reasoning_mode = state.get("reasoning_mode", True)

    tool_results = result.get("tool_results", {})
    rag_results = result.get("rag_results", [])
    pdf_results = result.get("pdf_results")

    _debug_log(state, "ì¢…í•©í•  ê²°ê³¼", {
        "tool_count": len(tool_results),
        "rag_count": len(rag_results),
        "has_pdf": bool(pdf_results)
    })

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []

    # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
    if tool_results:
        context_parts.append("## ğŸ“Š ì‹¤ì‹œê°„ ë°ì´í„° (API ì¡°íšŒ ê²°ê³¼)")
        for tool_name, data in tool_results.items():
            tool_desc = TOOL_METADATA.get(tool_name, {}).get("description", tool_name)
            context_parts.append(f"\n### {tool_desc}")
            context_parts.append(f"```json\n{json.dumps(data, ensure_ascii=False, indent=2)[:2000]}\n```")

    # RAG ê²€ìƒ‰ ê²°ê³¼
    if rag_results:
        context_parts.append("\n## ğŸ“š ë§¤ë‰´ì–¼ ê²€ìƒ‰ ê²°ê³¼")
        for doc in rag_results[:3]:  # ìƒìœ„ 3ê°œë§Œ
            if "error" not in doc:
                context_parts.append(f"\n**ì¶œì²˜:** {doc.get('source', 'unknown')}")
                context_parts.append(f"{doc.get('content', '')}")

    # PDF ë¶„ì„ ê²°ê³¼
    if pdf_results:
        context_parts.append("\n## ğŸ“„ ì—…ë¡œë“œëœ PDF ë¶„ì„")
        context_parts.append(f"**ë¶„ì„ ìœ í˜•:** {pdf_results.get('task', 'extract_info')}")
        context_parts.append(f"{pdf_results.get('content_preview', '')[:500]}")

    context = "\n".join(context_parts)

    # LLMìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
    try:
        client = _get_llm_client()

        llm_messages = [
            {"role": "system", "content": SYNTHESIZER_SYSTEM_PROMPT},
            {"role": "user", "content": f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:** {user_input}

**ìˆ˜ì§‘ëœ ì •ë³´:**
{context}

ë‹µë³€:"""}
        ]

        response = client.generate_response(
            messages=llm_messages,
            temperature=0.4,
            enable_thinking=reasoning_mode,
            stream=False
        )

        if response and response.choices:
            content = response.choices[0].message.content or ""
            # thinking íƒœê·¸ ì œê±°
            final_response = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
        else:
            final_response = f"ì •ë³´ ì¡°íšŒ ê²°ê³¼:\n\n{context}"

        logger.info("âœ… [SYNTHESIZER] ì‘ë‹µ ìƒì„± ì™„ë£Œ")

        return {
            "final_response": final_response,
            "next_node": "end",
            "messages": messages + [
                HumanMessage(content=user_input),
                AIMessage(content=final_response)
            ]
        }

    except Exception as e:
        logger.error(f"âŒ [SYNTHESIZER] ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        # Fallback: ì›ë³¸ ë°ì´í„° ë°˜í™˜
        return {
            "final_response": f"ì •ë³´ ì¡°íšŒ ê²°ê³¼:\n\n{context}\n\n(ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})",
            "next_node": "end",
            "messages": messages + [HumanMessage(content=user_input)]
        }


# =============================================================================
# 6. DIRECT RESPONSE NODE (ë„êµ¬ ì—†ì´ ì§ì ‘ ì‘ë‹µ)
# =============================================================================

def direct_response_node(state: QueryState) -> Dict[str, Any]:
    """
    ë„êµ¬ í˜¸ì¶œ ì—†ì´ LLMìœ¼ë¡œ ì§ì ‘ ì‘ë‹µí•©ë‹ˆë‹¤.
    (ì¼ë°˜ ëŒ€í™”, ì¸ì‚¬ ë“±)
    """
    logger.info("ğŸ’¬ [DIRECT_RESPONSE] ì‹¤í–‰ ì‹œì‘...")

    user_input = state.get("user_input", "")
    messages = state.get("messages", [])
    reasoning_mode = state.get("reasoning_mode", True)

    try:
        client = _get_llm_client()

        llm_messages = [
            {"role": "system", "content": SYSTEM_MESSAGE}
        ]

        # ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
        for msg in messages[-6:]:
            if isinstance(msg, HumanMessage):
                llm_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                llm_messages.append({"role": "assistant", "content": msg.content})

        llm_messages.append({"role": "user", "content": user_input})

        response = client.generate_response(
            messages=llm_messages,
            temperature=0.6,
            enable_thinking=reasoning_mode,
            stream=False
        )

        if response and response.choices:
            content = response.choices[0].message.content or ""
            final_response = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
        else:
            final_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        logger.info("âœ… [DIRECT_RESPONSE] ì‘ë‹µ ìƒì„± ì™„ë£Œ")

        return {
            "final_response": final_response,
            "next_node": "end",
            "messages": messages + [
                HumanMessage(content=user_input),
                AIMessage(content=final_response)
            ]
        }

    except Exception as e:
        logger.error(f"âŒ [DIRECT_RESPONSE] ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "final_response": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "next_node": "end",
            "messages": messages + [HumanMessage(content=user_input)]
        }
