"""
LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜

ê° ë…¸ë“œëŠ” ê¸°ì¡´ ì„œë¹„ìŠ¤ ëª¨ë“ˆì„ ë˜í•‘í•˜ì—¬ LangGraph ìƒíƒœ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
- agent_node: LLM í˜¸ì¶œ ë° ë‹¤ìŒ í–‰ë™ íŒë‹¨
- tools_node: ì™¸ë¶€ API ë„êµ¬ ì‹¤í–‰ (ë‚ ì”¨, ê²€ìƒ‰ ë“±)
- retrieve_node: RAG ê²€ìƒ‰ ì‹¤í–‰
- report_node: ë³´ê³ ì„œ ìƒì„± íŒŒì´í”„ë¼ì¸
"""

import logging
import json
import re
from typing import Dict, Any, List
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from config import VLLM_SERVER_URL, VLLM_API_KEY, LLM_MODEL_NAME
from graph.state import GraphState

logger = logging.getLogger(__name__)

# ì§€ì—° ë¡œë”©ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
_agent_manager = None
_agent_tools = None
_rag_system = None
_report_generator = None
_llm_client = None


def _get_agent_manager():
    """ExaoneAgentManager ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _agent_manager
    if _agent_manager is None:
        from core.agent_manager import ExaoneAgentManager
        _agent_manager = ExaoneAgentManager()
    return _agent_manager


def _get_agent_tools():
    """ExaoneAgentTools ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _agent_tools
    if _agent_tools is None:
        from services.agent_tools import exaone_agent_tools
        _agent_tools = exaone_agent_tools
    return _agent_tools


def _get_rag_system():
    """AdvancedRAGSystem ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _rag_system
    if _rag_system is None:
        from services.rag_engine import AdvancedRAGSystem
        _rag_system = AdvancedRAGSystem()
    return _rag_system


def _get_report_generator():
    """ReportGenerator ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _report_generator
    if _report_generator is None:
        from core.generator import ReportGenerator
        _report_generator = ReportGenerator()
    return _report_generator


def _get_llm_client():
    """LLM í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _llm_client
    if _llm_client is None:
        from core.llm_client import ExaoneClient
        _llm_client = ExaoneClient(
            server_url=VLLM_SERVER_URL,
            api_key=VLLM_API_KEY,
            model_name=LLM_MODEL_NAME
        )
    return _llm_client


def _convert_messages_to_history(messages: List) -> List[List[str]]:
    """LangGraph ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¡´ history í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    history = []
    current_user = None

    def _ensure_string(content):
        """contentê°€ listì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if isinstance(content, list):
            return " ".join(str(c) for c in content)
        return str(content) if content else ""

    for msg in messages:
        if isinstance(msg, HumanMessage):
            current_user = _ensure_string(msg.content)
        elif isinstance(msg, AIMessage):
            if current_user is not None:
                history.append([current_user, _ensure_string(msg.content)])
                current_user = None

    return history


def _detect_intent_with_llm(user_input: str) -> str:
    """
    [LLM ê¸°ë°˜ ì˜ë„ íŒŒì•…]
    LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ì— ë§ëŠ” ì •í™•í•œ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    ROUTER_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” 'Intent Classifier'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ 4ê°€ì§€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

1. "report": ë³´ê³ ì„œ ì‘ì„±, ë¬¸ì„œ ìƒì„± ìš”ì²­
2. "tools": ë‚ ì”¨, ì‹¤ì‹œê°„ ì •ë³´, ë‰´ìŠ¤ ê²€ìƒ‰ ë“± ì™¸ë¶€ ë°ì´í„° í•„ìš”
3. "retrieve": ë§¤ë‰´ì–¼, ê·œì •, ê°€ì´ë“œ, ì‚¬ë‚´ ì§€ì¹¨ ë“± ë‚´ë¶€ ì§€ì‹ ê²€ìƒ‰
4. "end": ì¼ë°˜ì ì¸ ëŒ€í™”, ì¸ì‚¬, ì‘ë³„ ì¸ì‚¬

**ì¶œë ¥ í˜•ì‹:**
{"intent": "ì„ íƒí•œ_ì¹´í…Œê³ ë¦¬", "reason": "ì„ íƒ ì´ìœ "}

**ì£¼ì˜ì‚¬í•­:**
- ì˜¤ì§ JSON ë°ì´í„°ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ì œì™¸í•˜ì„¸ìš”.
"""

    try:
        client = _get_llm_client()

        messages = [
            {"role": "system", "content": ROUTER_SYSTEM_PROMPT},
            {"role": "user", "content": user_input}
        ]

        response = client.generate_response(
            messages=messages,
            temperature=0.0, 
            enable_thinking=False, 
            stream=False
        )

        content = response.choices[0].message.content.strip()
        
        if "```" in content:
            if "json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            else:
                content = content.split("```")[1].strip()

        result = json.loads(content)
        intent = result.get("intent", "end")

        logger.info(f"ğŸ§  [LLM ROUTER] íŒë‹¨: {intent} (ì´ìœ : {result.get('reason')})")
        
        valid_intents = ["report", "tools", "retrieve", "end"]
        if intent not in valid_intents:
            return "end"
            
        return intent

    except Exception as e:
        logger.error(f"âŒ [LLM ROUTER] ë¼ìš°íŒ… ì‹¤íŒ¨ (Rule-basedë¡œ ë„˜ì–´ê°): {e}")
        return _detect_intent_legacy(user_input)


def _detect_intent_legacy(user_input: str) -> str:
    """
    (Legacy) ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ë‹¤ìŒ í–‰ë™ ê²°ì •
    """
    user_input_lower = user_input.lower()

    # ë³´ê³ ì„œ ìƒì„± ê´€ë ¨ í‚¤ì›Œë“œ
    report_keywords = [
        "ë³´ê³ ì„œ", "report", "ì¼ì¼ë³´ê³ ", "ìƒí™©ë³´ê³ ",
        "ë¬¸ì„œ ìƒì„±", "ì‘ì„±í•´ì¤˜", "ìƒì„±í•´ì¤˜"
    ]
    for kw in report_keywords:
        if kw in user_input_lower:
            return "report"

    # ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•œ í‚¤ì›Œë“œ (ë‚ ì”¨, ì‹¤ì‹œê°„ ì •ë³´)
    tool_keywords = [
        "ë‚ ì”¨", "ê¸°ì˜¨", "ì˜¨ë„", "ë¹„", "ëˆˆ", "ë°”ëŒ", "ë¯¸ì„¸ë¨¼ì§€",
        "weather", "temperature", "ì§€ì§„", "earthquake",
        "íŠ¹ë³´", "ì£¼ì˜ë³´", "ê²½ë³´", "íƒœí’", "í­ì—¼", "í•œíŒŒ",
        "ê²€ìƒ‰", "ë‰´ìŠ¤", "ìµœê·¼", "í˜„ì¬", "ì§€ê¸ˆ", "ì˜¤ëŠ˜"
    ]
    for kw in tool_keywords:
        if kw in user_input_lower:
            return "tools"

    # RAG ê²€ìƒ‰ì´ í•„ìš”í•œ í‚¤ì›Œë“œ (ë§¤ë‰´ì–¼, ê·œì •, ì ˆì°¨)
    rag_keywords = [
        "ë§¤ë‰´ì–¼", "ê·œì •", "ì§€ì¹¨", "ì ˆì°¨", "ê°€ì´ë“œ", "ë°©ë²•",
        "í–‰ë™ìš”ë ¹", "ëŒ€ì‘", "ì¡°ì¹˜", "ê¸°ì¤€", "ì •ì˜",
        "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ë­ì•¼", "ì•Œë ¤ì¤˜"
    ]
    for kw in rag_keywords:
        if kw in user_input_lower:
            return "retrieve"

    # ê¸°ë³¸: ì¼ë°˜ ëŒ€í™”ë¡œ ì¢…ë£Œ
    return "end"


def agent_node(state: GraphState) -> Dict[str, Any]:
    """
    Agent ë…¸ë“œ: ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ê²°ì •

    1. ì‚¬ìš©ì ì…ë ¥ ë¶„ì„
    2. ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì—…ë¡œë“œëœ íŒŒì¼, íˆìŠ¤í† ë¦¬ ë“±)
    3. LLM í˜¸ì¶œí•˜ì—¬ ì˜ë„ íŒŒì•… ë˜ëŠ” ì§ì ‘ ì‘ë‹µ
    4. next_action ê²°ì •
    """
    logger.info("ğŸ¤– [AGENT NODE] ì‹¤í–‰ ì‹œì‘...")

    user_input = state.get("user_input", "")
    uploaded_context = state.get("uploaded_context", "")
    messages = state.get("messages", [])
    agent_mode = state.get("agent_mode", True)
    reasoning_mode = state.get("reasoning_mode", True)

    # ì˜ë„ íŒŒì•…
    detected_action = _detect_intent_with_llm(user_input)
    logger.info(f"ğŸ¯ [AGENT NODE] ê°ì§€ëœ ì˜ë„: {detected_action}")

    # ë³´ê³ ì„œ ìš”ì²­ì´ê³  ì—…ë¡œë“œëœ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ reportë¡œ
    if detected_action == "report" and uploaded_context:
        logger.info("ğŸ“ [AGENT NODE] ë³´ê³ ì„œ ìƒì„± ê²½ë¡œë¡œ ë¼ìš°íŒ…")
        return {
            "next_action": "report",
            "messages": messages + [HumanMessage(content=user_input)]
        }

    # Agent ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ ìˆê³  toolsê°€ í•„ìš”í•˜ë©´
    if agent_mode and detected_action == "tools":
        logger.info("ğŸ”§ [AGENT NODE] ë„êµ¬ ì‚¬ìš© ê²½ë¡œë¡œ ë¼ìš°íŒ…")
        return {
            "next_action": "tools",
            "messages": messages + [HumanMessage(content=user_input)]
        }

    # RAG ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´
    if state.get("enable_rag", True) and detected_action == "retrieve":
        logger.info("ğŸ“š [AGENT NODE] RAG ê²€ìƒ‰ ê²½ë¡œë¡œ ë¼ìš°íŒ…")
        return {
            "next_action": "retrieve",
            "messages": messages + [HumanMessage(content=user_input)]
        }

    # ì¼ë°˜ ëŒ€í™”: LLMìœ¼ë¡œ ì§ì ‘ ì‘ë‹µ ìƒì„±
    logger.info("ğŸ’¬ [AGENT NODE] ì¼ë°˜ ëŒ€í™” ì‘ë‹µ ìƒì„±")

    try:
        client = _get_llm_client()

        # ë©”ì‹œì§€ ì¤€ë¹„
        history = _convert_messages_to_history(messages)

        llm_messages = [
            {"role": "system", "content": _get_system_prompt()}
        ]

        # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 3ê°œë§Œ)
        for human_msg, ai_msg in history[-3:]:
            llm_messages.append({"role": "user", "content": human_msg})
            if ai_msg:
                llm_messages.append({"role": "assistant", "content": ai_msg})

        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        llm_messages.append({"role": "user", "content": user_input})

        # LLM í˜¸ì¶œ
        response = client.generate_response(
            messages=llm_messages,
            enable_thinking=reasoning_mode,
            temperature=0.6 if reasoning_mode else 0.4,
            stream=False
        )

        if response and response.choices:
            content = response.choices[0].message.content or ""
            # thinking íƒœê·¸ ì œê±°
            final_response = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
        else:
            final_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return {
            "next_action": "end",
            "final_response": final_response,
            "messages": messages + [
                HumanMessage(content=user_input),
                AIMessage(content=final_response)
            ]
        }

    except Exception as e:
        logger.error(f"âŒ [AGENT NODE] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return {
            "next_action": "end",
            "final_response": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "messages": messages + [HumanMessage(content=user_input)]
        }


def tools_node(state: GraphState) -> Dict[str, Any]:
    """
    Tools ë…¸ë“œ: ì™¸ë¶€ API ë„êµ¬ ì‹¤í–‰

    ExaoneAgentManagerì˜ run_agentë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ í˜¸ì¶œ
    ê²°ê³¼ë¥¼ tool_resultsì— ì €ì¥í•˜ê³  Agentë¡œ ëŒì•„ê°
    """
    logger.info("ğŸ”§ [TOOLS NODE] ì‹¤í–‰ ì‹œì‘...")

    user_input = state.get("user_input", "")
    messages = state.get("messages", [])
    reasoning_mode = state.get("reasoning_mode", True)

    try:
        agent = _get_agent_manager()
        history = _convert_messages_to_history(messages[:-1])  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸

        # Agent ì‹¤í–‰ (ë„êµ¬ í˜¸ì¶œ í¬í•¨)
        result = agent.run_agent(user_input, history, reasoning_mode)

        if result.get("success"):
            tool_content = result.get("content", "")
            tool_calls = result.get("tool_calls", [])

            # ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ…
            tool_results_str = ""
            if tool_calls:
                tool_results_str = "\n\n**[ë„êµ¬ ì‹¤í–‰ ê²°ê³¼]**\n"
                for tc in tool_calls:
                    tool_results_str += f"- {tc.get('name', 'unknown')}: ì‹¤í–‰ ì™„ë£Œ\n"

            final_response = tool_content

            logger.info(f"âœ… [TOOLS NODE] ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ: {len(tool_calls)}ê°œ ë„êµ¬ í˜¸ì¶œ")

            return {
                "tool_results": tool_results_str,
                "final_response": final_response,
                "next_action": "end",  # ë„êµ¬ ì‹¤í–‰ í›„ ì¢…ë£Œ
                "messages": messages + [AIMessage(content=final_response)]
            }
        else:
            error_msg = result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
            logger.error(f"âŒ [TOOLS NODE] ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")

            return {
                "tool_results": f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}",
                "next_action": "end",
                "final_response": f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}",
                "messages": messages
            }

    except Exception as e:
        logger.error(f"âŒ [TOOLS NODE] ì˜ˆì™¸ ë°œìƒ: {e}")
        return {
            "tool_results": f"ì˜¤ë¥˜: {str(e)}",
            "next_action": "end",
            "final_response": f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "messages": messages
        }


def summarize_node(state: GraphState) -> Dict[str, Any]:
    """
    Summarize ë…¸ë“œ: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš° LLMì„ í†µí•´ ìš”ì•½
    """
    logger.info("ğŸ” [SUMMARIZE NODE] ì‹¤í–‰ ì‹œì‘...")
    
    tool_results = state.get("tool_results", "")
    
    # ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì§§ìœ¼ë©´ íŒ¨ìŠ¤ (ì˜ˆ: 3000ì ë¯¸ë§Œ)
    if not tool_results or len(tool_results) < 3000:
        logger.info("âœ… [SUMMARIZE NODE] ìš”ì•½ ë¶ˆí•„ìš” (ê¸¸ì´ ì ì •)")
        return {} # ìƒíƒœ ë³€ê²½ ì—†ìŒ
        
    try:
        client = _get_llm_client()
        
        # ë„ˆë¬´ ê¸´ ì…ë ¥ì€ ì•ë¶€ë¶„ 20kë§Œ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ì‹œë„
        input_text = tool_results[:20000]
        
        prompt = f"""
ë‹¹ì‹ ì€ 'Data Summarizer'ì…ë‹ˆë‹¤. ì•„ë˜ ë„êµ¬(API) ì‹¤í–‰ ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ì–´ ì‹œìŠ¤í…œì´ ì²˜ë¦¬í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° í•„ìš”í•œ **í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ìš”ì•½**í•˜ì„¸ìš”.
ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ë°ì´í„°, ID, ë©”íƒ€ë°ì´í„° ë“±ì€ ì œê±°í•˜ê³ , ì¤‘ìš”í•œ ìˆ˜ì¹˜ë‚˜ í…ìŠ¤íŠ¸ ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.

[ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ (ì¼ë¶€)]
{input_text}

[ìš”ì•½ ê²°ê³¼]
"""
        messages = [{"role": "user", "content": prompt}]
        
        response = client.generate_response(
            messages=messages,
            enable_thinking=False, # ìš”ì•½ì€ ë¹ ë¥¸ ì²˜ë¦¬ ìš°ì„ 
            temperature=0.2,
            max_tokens=2048
        )
        
        if response and response.choices:
            content = response.choices[0].message.content.strip()
            # thinking íƒœê·¸ ì œê±°
            summary = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
            
            logger.info(f"âœ… [SUMMARIZE NODE] ìš”ì•½ ì™„ë£Œ ({len(tool_results)}ì -> {len(summary)}ì)")
            return {
                "tool_results": f"**[AI ìš”ì•½ëœ ë„êµ¬ ê²°ê³¼]**\n{summary}"
            }
            
    except Exception as e:
        logger.error(f"âŒ [SUMMARIZE NODE] ìš”ì•½ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ì•ë¶€ë¶„ë§Œ ìë¦„
        return {
            "tool_results": tool_results[:3000] + "\n...(ìš”ì•½ ì‹¤íŒ¨ë¡œ ì ˆì‚­ë¨)"
        }
    
    return {}


def retrieve_node(state: GraphState) -> Dict[str, Any]:
    """
    Retrieve ë…¸ë“œ: RAG ê²€ìƒ‰ ì‹¤í–‰

    AdvancedRAGSystemì˜ searchë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰
    ê²°ê³¼ë¥¼ rag_resultsì— ì €ì¥í•˜ê³  Agentë¡œ ëŒì•„ê°€ì„œ ìµœì¢… ì‘ë‹µ ìƒì„±
    """
    logger.info("ğŸ“š [RETRIEVE NODE] ì‹¤í–‰ ì‹œì‘...")

    user_input = state.get("user_input", "")
    messages = state.get("messages", [])
    selected_pdf = state.get("selected_pdf", None)
    reasoning_mode = state.get("reasoning_mode", True)

    try:
        rag = _get_rag_system()
        client = _get_llm_client()

        # RAG ê²€ìƒ‰ ì‹¤í–‰
        results = rag.search(user_input, selected_pdf, top_k=5)

        if results:
            # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
            rag_context = rag.format_search_results(results, selected_pdf)

            logger.info(f"âœ… [RETRIEVE NODE] ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬")

            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            llm_messages = [
                {"role": "system", "content": _get_system_prompt()},
                {"role": "user", "content": f"""ë‹¤ìŒ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

**ê²€ìƒ‰ëœ ë¬¸ì„œ:**
{rag_context}

**ì‚¬ìš©ì ì§ˆë¬¸:**
{user_input}

ë‹µë³€ ì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”."""}
            ]

            response = client.generate_response(
                messages=llm_messages,
                enable_thinking=reasoning_mode,
                temperature=0.4,
                stream=False
            )

            if response and response.choices:
                content = response.choices[0].message.content or ""
                final_response = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
            else:
                final_response = f"ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìœ¼ë‚˜ ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n{rag_context}"

            return {
                "rag_results": rag_context,
                "reference_docs": rag_context,
                "final_response": final_response,
                "next_action": "end",
                "messages": messages + [AIMessage(content=final_response)]
            }
        else:
            logger.warning("âš ï¸ [RETRIEVE NODE] ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨")
            return {
                "rag_results": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "next_action": "end",
                "final_response": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì‹œê² ì–´ìš”?",
                "messages": messages
            }

    except Exception as e:
        logger.error(f"âŒ [RETRIEVE NODE] ì˜ˆì™¸ ë°œìƒ: {e}")
        return {
            "rag_results": f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}",
            "next_action": "end",
            "final_response": f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "messages": messages
        }


def report_node(state: GraphState) -> Dict[str, Any]:
    """
    Report ë…¸ë“œ: ë³´ê³ ì„œ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    ReportGeneratorì˜ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰:
    1. parse_document: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
    2. plan_tools: ë„êµ¬ ì‚¬ìš© ê³„íš
    3. fill_report: ë‚´ìš© ì±„ìš°ê¸°
    4. export_to_docx: ë¬¸ì„œ ë‚´ë³´ë‚´ê¸° (ì„ íƒì )

    ê²°ê³¼ë¥¼ report_outputì— ì €ì¥í•˜ê³  ENDë¡œ ì§ì ‘ ì´ë™
    """
    logger.info("ğŸ“ [REPORT NODE] ì‹¤í–‰ ì‹œì‘...")

    user_input = state.get("user_input", "")
    uploaded_context = state.get("uploaded_context", "")
    messages = state.get("messages", [])

    try:
        generator = _get_report_generator()

        # ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì—…ë¡œë“œëœ ì»¨í…ìŠ¤íŠ¸ ë˜ëŠ” ì‚¬ìš©ì ì…ë ¥)
        input_text = uploaded_context if uploaded_context else user_input

        if not input_text:
            return {
                "report_output": "ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ì…ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.",
                "final_response": "ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ë ¤ë©´ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                "messages": messages
            }

        # Phase 1: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        logger.info("ğŸ“„ [REPORT NODE] Phase 1: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„...")
        structure = generator.parse_document(input_text)

        if structure.get("status") == "incomplete":
            # ì¶”ê°€ ì •ë³´ í•„ìš”
            clarification = structure.get("clarification_question", "ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return {
                "report_output": None,
                "final_response": f"ë³´ê³ ì„œ ì‘ì„±ì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n{clarification}",
                "messages": messages + [AIMessage(content=clarification)]
            }

        if structure.get("status") == "error":
            return {
                "report_output": None,
                "final_response": "ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "messages": messages
            }

        # Phase 2: ë„êµ¬ ê³„íš
        logger.info("ğŸ§  [REPORT NODE] Phase 2: ë„êµ¬ ê³„íš ìˆ˜ë¦½...")
        structure = generator.plan_tools(structure)

        # Phase 3 & 4: ë‚´ìš© ì±„ìš°ê¸° ë° ìƒì„±
        logger.info("ğŸ› ï¸ [REPORT NODE] Phase 3: ë‚´ìš© ì‘ì„±...")
        structure = generator.fill_report(structure)

        # ìµœì¢… ë³´ê³ ì„œ ê°€ì ¸ì˜¤ê¸°
        report_md = structure.get("full_report_md", "ë³´ê³ ì„œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # DOCX ë‚´ë³´ë‚´ê¸° (ì„ íƒì )
        docx_path = ""
        try:
            docx_path = generator.export_to_docx(structure)
            if docx_path:
                report_md += f"\n\nğŸ“ **ë¬¸ì„œ ì €ì¥ë¨:** `{docx_path}`"
        except Exception as e:
            logger.warning(f"DOCX ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

        logger.info("âœ… [REPORT NODE] ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")

        return {
            "report_output": report_md,
            "final_response": report_md,
            "messages": messages + [AIMessage(content=report_md)]
        }

    except Exception as e:
        logger.error(f"âŒ [REPORT NODE] ì˜ˆì™¸ ë°œìƒ: {e}")
        return {
            "report_output": f"ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {str(e)}",
            "final_response": f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "messages": messages
        }


def _get_system_prompt() -> str:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
    from config import SYSTEM_MESSAGE
    return SYSTEM_MESSAGE
