"""
[Chat Manager]
ëŒ€í™” íë¦„ ì œì–´ ë° í†µí•© ê´€ë¦¬ (Broker)
ì‚¬ìš©ì ë©”ì‹œì§€ -> [Agent/RAG/Report] ë¶„ê¸° ì²˜ë¦¬ -> LLM í˜¸ì¶œ -> ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
"""

import logging
import json
import re
from typing import List, Dict, Generator, Tuple, Optional
from pathlib import Path
import requests
from openai import OpenAI

from config import (
    VLLM_SERVER_URL, VLLM_API_KEY, LLM_MODEL_NAME, ENABLE_REASONING,
    MAX_TOKENS, TEMPERATURE, TOP_P, TOP_K, MIN_P, SYSTEM_MESSAGE, TOP_K_RESULTS
)
from services.rag_engine import AdvancedRAGSystem
from core.agent_manager import exaone_agent

logger = logging.getLogger(__name__)

class ExaoneClient:
    """EXAONE ëª¨ë¸ (VLLM ì„œë²„) í†µì‹  í´ë¼ì´ì–¸íŠ¸"""
    def __init__(self, server_url, api_key, model_name="LGAI-EXAONE/EXAONE-4.0-32B-AWQ"):
        self.server_url = server_url
        self.api_key = api_key
        self.model_name = model_name
        
        # OpenAI SDK í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        self.client = OpenAI(api_key=api_key, base_url=server_url)

    def generate_response(self, messages: List[Dict], enable_thinking: bool = False,
                          temperature: float = TEMPERATURE, max_tokens: int = MAX_TOKENS,
                          top_p: float = TOP_P, stream: bool = True):
        """
        LLM ìƒì„± ìš”ì²­ ì „ì†¡
        - extra_bodyë¥¼ í†µí•´ 'enable_thinking' íŒŒë¼ë¯¸í„° ì „ë‹¬ (Reasoning ëª¨ë“œ ì œì–´)
        """
        try:
            response = self.client.chat.completions.create(
                model = self.model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                stream=stream,
                extra_body={
                    "chat_template_kwargs": {"enable_thinking": enable_thinking},
                },
            )
            return response
        except Exception as e:
            logger.error(f"EXAONE API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None


class ChatManager:
    """
    [í•µì‹¬ í´ë˜ìŠ¤] ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ì
    - ì—­í• : ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹ , ì ì ˆí•œ ì²˜ë¦¬ê¸°(Agent, RAG) í˜¸ì¶œ, ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½, ì‘ë‹µ ìƒì„±
    """

    def __init__(self):
        self.rag_system = AdvancedRAGSystem()
        self.exaone_client = ExaoneClient(
            server_url=VLLM_SERVER_URL,
            api_key=VLLM_API_KEY,
            model_name = LLM_MODEL_NAME,
        )
        # ì¶”ë¡  ëª¨ë“œ ì—¬ë¶€ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        self.reasoning_temperature = 0.6 # ì°½ì˜ì  ì‚¬ê³  í•„ìš”
        self.non_reasoning_temperature = 0.4 # ì •í™•í•œ ì •ë³´ ì „ë‹¬ ì¤‘ìš”

    def initialize_system(self, pdf_paths: List[str]):
        """ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ RAG ì¸ë±ìŠ¤ ë¹Œë“œ"""
        logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        self.rag_system.build_index(pdf_paths)
        logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _prepare_messages(self, user_message: str, history: List[List[str]], rag_context: str = None, agent_context: str = None, uploaded_context: str = None):
        """
        [Prompt Engineering] LLMì— ë³´ë‚¼ ìµœì¢… ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½
        - System Prompt + ì‹œê°„ ì •ë³´ + ì—…ë¡œë“œ íŒŒì¼ ë‚´ìš© + Agent ê²°ê³¼ + RAG ê²€ìƒ‰ ê²°ê³¼ + ëŒ€í™” íˆìŠ¤í† ë¦¬ + í˜„ì¬ ì§ˆë¬¸
        """
        messages = []

        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ì‹œê°„ ì •ë³´ ì£¼ì…
        from datetime import datetime
        now_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %A %Hì‹œ %Më¶„")
        system_content = f"í˜„ì¬ ì‹œê°: {now_str}\n\n{SYSTEM_MESSAGE}"

        # 2. [íŒŒì¼ ë‚´ìš©] ì—…ë¡œë“œëœ ë¬¸ì„œ ë‚´ìš©ì„ ë³„ë„ ì„¹ì…˜ìœ¼ë¡œ ì£¼ì…
        if uploaded_context:
            truncated_context = uploaded_context[:15000] # í† í° ì œí•œ ê³ ë ¤í•˜ì—¬ ê¸¸ì´ ì œí•œ
            if len(uploaded_context) > 15000:
                truncated_context += "\n...(ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìƒëµë¨)..."
            system_content += f"\n\n=== [ì‚¬ìš©ì ì—…ë¡œë“œ íŒŒì¼ ë‚´ìš© (ì°¸ê³  ìë£Œ)] ===\n{truncated_context}\n\nìœ„ íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ê±°ë‚˜ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤."

        # 3. [Agent ê²°ê³¼] íˆ´ ì‹¤í–‰ ê²°ê³¼ ì£¼ì…
        if agent_context:
            system_content += f"\n\n === Agent ì‹¤í–‰ ê²°ê³¼ ===\n{agent_context}\n\nìœ„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        # 4. [RAG ê²°ê³¼] ê²€ìƒ‰ëœ ê·œì • ë¬¸ì„œ ì£¼ì…
        if rag_context:
            system_content += f"\n\n === ì°¸ê³  ë¬¸ì„œ ====\n{rag_context}\n\nìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        messages.append({"role": "system", "content": system_content})

        # 5. ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœê·¼ 10í„´ ìœ ì§€)
        max_history_turns = 10
        if len(history) > max_history_turns:
            history = history[-max_history_turns:]

        for human_msg, ai_msg in history:
            if human_msg: messages.append({"role": "user", "content": human_msg.strip()})
            if ai_msg: messages.append({"role": "assistant", "content": self._clean_ai_message(ai_msg.strip())})

        messages.append({"role": "user", "content": user_message})
        return messages

    def _clean_ai_message(self, ai_msg: str) -> str:
        """íˆìŠ¤í† ë¦¬ ì˜¤ì—¼ ë°©ì§€: ì´ì „ ì‘ë‹µì—ì„œ ë¡œê·¸/ë©”íƒ€ë°ì´í„° ì œê±°"""
        # ... (ì •ê·œì‹ ì œê±° ë¡œì§ ìƒëµ) ...
        return ai_msg # ì‹¤ì œë¡œëŠ” ì •ì œ ë¡œì§ ì ìš©ë¨

    def process_message(self, user_message: str, history: List[List[str]], agent_mode: bool = False, 
                    reasoning_mode: bool = True, enable_reasoning: bool = True, enable_rag: bool = True,
                    selected_pdf_path: Optional[str] = None, reset_state: bool = False, uploaded_context: str = "") -> Generator[Tuple[List[List[str]], str], None, None]:
        """
        [Main Loop] ë©”ì‹œì§€ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸
        
        Returns: Generator yielding (history, doc_info)
        """
        
        # 1. [Special Flow] ë³´ê³ ì„œ ìƒì„± ìš”ì²­ì¸ì§€ í™•ì¸
        if "ë³´ê³ ì„œ ìƒì„±" in user_message or "ë³´ê³ ì„œ ì‘ì„±" in user_message:
            logger.info("ğŸ“„ [PROCESS] ë³´ê³ ì„œ ìƒì„± ìš”ì²­ ê°ì§€")
            # ReportGenerator í˜¸ì¶œ ë¡œì§ (ë³„ë„ ë¶„ê¸°)
            # ... (ìƒëµ: generator.py ì°¸ì¡°) ...
            yield (history + [[user_message, "ë³´ê³ ì„œ ìƒì„± ë¡œì§ ì‹¤í–‰ë¨"]], "")
            return
        
        # ê¸°ë³¸ ë³€ìˆ˜ ì´ˆê¸°í™”
        rag_context = ""
        agent_context = None
        new_history = history + [[user_message, ""]]
        yield (new_history, "")

        # 2. [Agent Mode] ì—ì´ì „íŠ¸ ì‹¤í–‰ (íˆ´ ì‚¬ìš©ì´ í•„ìš”í•œ ê²½ìš°)
        if agent_mode and exaone_agent.is_available():
            # ... (Agent ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘ ë¡œì§) ...
            pass

        # 3. [RAG Mode] ê·œì • ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œ)
        rag_keywords = ["ê·œì •", "ì§€ì¹¨", "ë§¤ë‰´ì–¼", "ì ˆì°¨", "ê¸°ì¤€", "íŠ¹ë³´", "ì§€ì§„"]
        need_rag = any(k in user_message for k in rag_keywords)

        if enable_rag and need_rag:
            logger.info("ğŸ” [PROCESS] RAG ê²€ìƒ‰ ìˆ˜í–‰")
            s_res = self.rag_system.search(user_message, selected_pdf_path, top_k=TOP_K_RESULTS)
            if s_res:
                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜
                rag_context = "=== [í–‰ì •ì•ˆì „ë¶€/ê¸°ìƒì²­ ê³µì‹ ìœ„ê¸°ê´€ë¦¬ ë§¤ë‰´ì–¼ (SOP)] ===\n" + \
                              "\n".join([f"- {r['text']}" for r in s_res])

        # 4. [Final Generation] ìµœì¢… ì‘ë‹µ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
        messages = self._prepare_messages(user_message, history, rag_context, agent_context, uploaded_context)
        
        response = self.exaone_client.generate_response(
            messages=messages,
            stream=True,
            enable_thinking=reasoning_mode,
            temperature=self.reasoning_temperature if reasoning_mode else self.non_reasoning_temperature
        )
        
        # [Error Handling] API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë°©ì–´ ì½”ë“œ
        if response is None:
            logger.error("âŒ EXAONE API ì‘ë‹µì´ Noneì…ë‹ˆë‹¤. (ì„œë²„ ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ)")
            new_history[-1][1] = "âŒ ì„œë²„ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (API ì‘ë‹µ ì—†ìŒ)\nê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            yield (new_history, "")
            return

        # ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì²˜ë¦¬ (Thinking íŒŒíŠ¸ì™€ Content íŒŒíŠ¸ ë¶„ë¦¬)
        if reasoning_mode:
            for chunk in self._stream_response_with_reasoning(response):
                # UI ì—…ë°ì´íŠ¸ (Thinking -> Content ìˆœì„œ)
                if chunk['type'] == 'thinking_chunk':
                    new_history[-1][1] += chunk['content']
                elif chunk['type'] == 'content':
                    new_history[-1][1] += chunk['content']
                yield (new_history, "")
        else:
            # ì¼ë°˜ ìŠ¤íŠ¸ë¦¬ë°
            for chunk in self._stream_response_simple(response):
                new_history[-1][1] += chunk['content']
                yield (new_history, "")

    def _stream_response_with_reasoning(self, response) -> Generator[dict, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ë©° <think> íƒœê·¸ë¥¼ ê°ì§€í•˜ì—¬ ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ì„ êµ¬ë¶„í•´ yield í•©ë‹ˆë‹¤.
        UIì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ê³  ê³¼ì •ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
        """
        thinking_started = False
        content_started = False
        reasoning_content_complete = ""
        content_buffer = ""
        reasoning_seen = False
        
        try:
            for chunk in response:
                if hasattr(chunk, 'choices') and chunk.choices:
                    choice = chunk.choices[0]
                    delta = choice.delta if hasattr(choice, 'delta') else {}
                    
                    # VLLM API ìŠ¤í™ì— ë”°ë¥¸ reasoning_content í•„ë“œ í™•ì¸
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        reasoning_chunk = delta.reasoning_content
                        reasoning_content_complete += reasoning_chunk
                        reasoning_seen = True
                        if not thinking_started:
                            yield {'type': 'thinking_start'}
                            thinking_started = True
                        yield {'type': 'thinking_chunk', 'content': reasoning_chunk}
                    
                    # ì¼ë°˜ content í•„ë“œ í™•ì¸
                    if hasattr(delta, 'content') and delta.content:
                        content_chunk = delta.content
                        content_buffer += content_chunk
                        
                        # ì¶”ë¡ ì´ ìˆì—ˆê³  ì´ì œ ë§‰ ì»¨í…ì¸ ê°€ ì‹œì‘ë˜ëŠ” ê²½ìš°
                        if reasoning_seen and not content_started and thinking_started:
                            yield {'type': 'thinking_end'}
                            content_started = True
                            content_buffer = ""
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        elif reasoning_seen and content_started:
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        
                        if content_started:
                            yield {'type': 'content', 'content': content_chunk}
                            continue
                        
                        # <think> íƒœê·¸ë¥¼ ì§ì ‘ íŒŒì‹±í•˜ì—¬ ì²˜ë¦¬ (API í•„ë“œê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ ë‚´ í¬í•¨ëœ ê²½ìš°)
                        if '</think>' in content_buffer:
                            parts = content_buffer.split('</think>', 1)
                            thinking_part = parts[0]
                            answer_part = parts[1] if len(parts) > 1 else ""
                            
                            if not thinking_started:
                                yield {'type': 'thinking_start'}
                                thinking_started = True
                                if thinking_part: yield {'type': 'thinking_chunk', 'content': thinking_part}
                            
                            yield {'type': 'thinking_end'}
                            content_started = True
                            if answer_part: yield {'type': 'content', 'content': answer_part}
                            content_buffer = ""
                        else:
                            # </think>ê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ëŠ” ì¶”ë¡  ê³¼ì •ìœ¼ë¡œ ê°„ì£¼
                            if not thinking_started:
                                yield {'type': 'thinking_start'}
                                thinking_started = True
                            yield {'type': 'thinking_chunk', 'content': content_chunk}
            
            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ í›„ ì”ì—¬ ë²„í¼ ì²˜ë¦¬
            if thinking_started and not content_started:
                yield {'type': 'thinking_end'}
                if reasoning_content_complete.strip():
                    cleaned_content = reasoning_content_complete
                    if '</think>' in cleaned_content:
                        cleaned_content = cleaned_content.split('</think>', 1)[1]
                    if cleaned_content.strip():
                        yield {'type': 'content', 'content': cleaned_content.strip()}
                elif content_buffer.strip():
                    yield {'type': 'content', 'content': content_buffer}
                            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {'type': 'error', 'content': f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
    
    def _stream_response_simple(self, response) -> Generator[dict, None, None]:
        """
        ì¶”ë¡  ëª¨ë“œê°€ ì•„ë‹ ë•Œì˜ ë‹¨ìˆœ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì…ë‹ˆë‹¤.
        """
        think_tag_found = False
        content_buffer = ""
        try:
            for chunk in response:
                if hasattr(chunk, 'choices') and chunk.choices:
                    delta = chunk.choices[0].delta
                    # reasoning_contentê°€ ìˆë‹¤ë©´ ì¼ë°˜ contentë¡œ ì·¨ê¸‰í•˜ì—¬ ì „ë‹¬
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        if think_tag_found: yield {'type': 'content', 'content': delta.reasoning_content}
                        else:
                            content_buffer += delta.reasoning_content
                            if '</think>' in content_buffer:
                                think_tag_found = True
                                parts = content_buffer.split('</think>', 1)
                                if len(parts)>1 and parts[1]: yield {'type': 'content', 'content': parts[1]}
                                content_buffer = ""
                    elif hasattr(delta, 'content') and delta.content:
                        if think_tag_found: yield {'type': 'content', 'content': delta.content}
                        else:
                            content_buffer += delta.content
                            if '</think>' in content_buffer:
                                think_tag_found = True
                                parts = content_buffer.split('</think>', 1)
                                if len(parts)>1 and parts[1]: yield {'type': 'content', 'content': parts[1]}
                                content_buffer = ""
            if content_buffer and not think_tag_found:
                yield {'type': 'content', 'content': content_buffer}
        except Exception as e:
            logger.error(f"Non-reasoning ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {'type': 'error', 'content': f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

    def get_pdf_list(self) -> List[Tuple[str, str]]:
        """RAG ì‹œìŠ¤í…œì— ë¡œë“œëœ PDF ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        # rag_systemì— í•´ë‹¹ ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
        if hasattr(self.rag_system, 'get_pdf_list'):
            return self.rag_system.get_pdf_list()
        return []

